var tipuesearch = {"pages":[{"title":"About me","text":"Hao serves as an R&D; Engineer at New H3C Technologies . In early 2017, He completed EIT Digital Dual Master Degree Programme with a major of Internet Technology and Architecture at KTH Royal Institute of Technology and a specialization of Internet of Things(IoT) at IMT Atlantique . He also minored in ICT Innovation and Entrepreneurship during his Master studies. Hao is a Cisco Certificated Internetwork Expert in Routing and Switching (CCIE R&S #40343). He is passionate about future networking technologies. His interests mainly focus on Wireless and Mobile Networking , Datacenter and Cloud Networking , and Network Intelligence(Big Data, AI, etc.) . He is highly motivated by the desire to see these technologies meet together to shape a more intelligent, efficient, and reliable networked society. His curriculum vitae is available here . You can also find him on Linkedin . 现就职于 H3C 担任研发工程师。2017年初完成欧盟 EIT Digital 双硕士学位联合培养项目，分别获得 KTH瑞典皇家理工学院 的互联网技术与架构硕士学位和 IMT Atlantique法国大西洋高等矿业电信学校 的物联网硕士学位，以及ICT创新创业商科辅修证书。2016年3月至9月在 思科 巴黎创新研究院进行硕士毕业论文实习。2014年本科毕业于 浙江大学 环境与资源学院并获得农学学士学位。 思科认证互联网络专家（CCIE R&S; #40343），也是未来网络技术的狂热爱好者。兴趣主要集中在无线和移动网络，数据中心和云计算网络，以及网络大数据/人工智能等领域。期待这些技术融合后构建一个更加智能、高效和可靠的互联社会。 详细简历请戳 这里 。您也可以在 领英 上找到我。","tags":"pages","url":"https://jiang-hao.com/pages/aboutme.html","loc":"https://jiang-hao.com/pages/aboutme.html"},{"title":"Self Management","text":"1. Skill Tree var myChartContainer = document.getElementById('skillTree'); var resizeContainer = function () { myChartContainer.style.height = window.innerHeight+'px'; }; resizeContainer(); var myChart = echarts.init(myChartContainer); myChart.showLoading(); $.get('../doc/skillTree.json', function (data) { myChart.hideLoading(); myChart.setOption(option = { tooltip: { trigger: 'item', triggerOn: 'mousemove' }, series: [ { type: 'tree', data: [data], top: '1%', left: '10%', bottom: '1%', right: '25%', symbolSize: 7, label: { normal: { position: 'left', verticalAlign: 'middle', align: 'right', fontSize: 12 } }, leaves: { label: { normal: { position: 'right', verticalAlign: 'middle', align: 'left', fontSize: 12 } } }, expandAndCollapse: true, animationDuration: 550, animationDurationUpdate: 750 } ] }); }); window.onresize = function () { resizeContainer(); myChart.resize(); }; 2. Weekly Schedule Slot Sunday Monday Tuesday Wednesday Thursday Friday Saturday 06:00 getup getup getup getup getup getup getup 06:30 Java Java Java Java Java Java Java 07:00 Java Java Java Java Java Java Java 07:30 Java Java Java Java Java Java Java 08:00 Algorithms OTR OTR OTR OTR OTR Algorithms 08:30 Algorithms OTR OTR OTR OTR OTR Algorithms 09:00 Algorithms Algorithms 09:30 OTR 10:00 Security 10:30 Security 11:00 Security 11:30 Security 12:00 OTR OTR OTR OTR OTR OTR OTR 12:30 AI AI AI AI AI AI 13:00 AI AI AI AI AI AI 13:30 14:00 14:30 15:00 15:30 16:00 16:30 17:00 17:30 18:00 OTR OTR OTR OTR OTR OTR 18:30 Cloud Cloud Cloud Cloud OTR 19:00 Cloud Cloud Cloud Cloud Security 19:30 Cloud Cloud Cloud Cloud Security 20:00 Cloud Cloud Cloud Cloud Security 20:30 OTR OTR OTR OTR Security 21:00 Big Data Big Data Big Data Big Data OTR 21:30 Big Data Big Data Big Data Big Data 22:00 Big Data Big Data Big Data Big Data 22:30 SHW SHW SHW SHW SHW SHW SHW","tags":"pages","url":"https://jiang-hao.com/pages/management.html","loc":"https://jiang-hao.com/pages/management.html"},{"title":"Publications","text":"论文 2019 \"A JSON-Based Fast and Expressive Access Control Policy Framework\". H. Jiang and A. Bouabdallah. Advanced Data Modeling and Processing With JSON. To be published by IGI Global in 2019. 2017 \"JACPoL: A Simple but Expressive JSON-based Access Control Policy Language\". H. Jiang and A. Bouabdallah. Wistp 2017: International Conference on Information Security Theory and Practice, Springer Verlag, 28-29 September 2017. \"Towards a JSON-Based Fast Policy Evaluation Framework\". H. Jiang and A. Bouabdallah. In OTM Confederated International Conferences \"On the Move to Meaningful Internet Systems\", pp. 22-30. Springer, Cham, 2017. \"Controlled Replication for Higher Reliability and Predictability in Industrial IoT Networks\". H. Jiang , Z. Brodard, T.F. Chang, A. Bouabdallah, N. Montavont, G. Texier, P. Thubert, T. Watteyne, G. Z. Papadopoulos. In Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks (EWSN), Dependability Competition, ACM, Uppsala, Sweden, February 20-22, 2017. 2016 \"Rover: Poor (but Elegant) Man's Testbed\". Z. Brodard, H. Jiang , T.F. Chang, T. Watteyne, X. Vilajosana, P. Thubert, G. Texier. In Proceedings of the 13th ACM Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, and Ubiquitous Networks (PE-WASUN 2016). pp. 61-65. ACM. November 17, 2016. \"A Secure Multi-Tenant Framework for SDN\". H. Jiang , A. Bouabdallah, A. Aflatoonian, J. M. Bonnin, K. Guillouard. In Proceedings of the 9th International Conference on Security of Information and Networks(SIN 2016). pp. 40-44. ACM, July 20, 2016. 专利 2016 \"Timeslot Shifting for Lower-Priority Packets in a Time Slotted Network\". P. Thubert, Z. Brodard, H. Jiang . U.S.Patent 15/207,621 filed 12-Jul-2016. 标准 2016 \"A 6loRH for Bit Strings\". P. Thubert, Z. Brodard, H. Jiang , & G. Texier. IETF Internet draft. Version: draft-thubert-6lo-bier-dispatch-01. IETF 6lo WG. June 29, 2016. \"BIER-TE-based OAM, Replication & Elimination\". P. Thubert, Z. Brodard, H. Jiang . IETF Internet draft. Version: draft-thubert-bier-replication-elimination-00. IETF BIER WG. September 14, 2016. ​ 其它 2016 \"Software-defined IoT: 6TiSCH Centralized Scheduling and Multipath Construction\". H. Jiang . Internship report (Master Thesis). Cisco Paris Innovation and Research Lab, September 21, 2016. \"A Secure Multi-tenant Framework on the Northbound Side of SDN\". H. Jiang . Innovation project report. Department of Network, Security and Multimedia, Telecom Bretagne, March 07, 2016. 2014 未来互联网试验床资源管控系统的设计与实现/ Design and Implementation of Resource Management and Control System for Future Internet Testbed (in Chinese). H. Jiang . Bachelor thesis. Next Generation Network Technology Laboratory (NGNT Lab), College of Computer Science and Technology, Zhejiang University, June 10, 2014. 2013 CCIE笔记-路由交换理论和实战/ CCIE Notes - Routing and Switching (in Chinese). H. Jiang . August 27, 2013.","tags":"pages","url":"https://jiang-hao.com/pages/publications.html","loc":"https://jiang-hao.com/pages/publications.html"},{"title":"Resume","text":"Last Updated: Feb 25, 2018 Hao JIANG Research & Development Engineer, New H3C Technologies Co., Limited Email: jiang DOT haoa AT h3c DOT com Address: 466 Changhe Road, Binjiang District, 310052 Hangzhou SUMMARY A 2-year-old master graduate with double degrees in Internet Technology and Architecture & Internet of Things(IoT) from EIT Digital Program, and a minor certificate in Innovation and Entrepreneurship . Possess extensive knowledge in IoT Data Communication and Management & Datacenter and Cloud Networking technologies. Understand deeply IP network architecture, protocols, and popular techniques. Master common system programming languages, e.g., Java, Python, C/C++. EDUCATION M.Sc. in Internet of Things, IMT Atlantique, GPA: 3.85/3.9, 2015/2016 M.Sc. in Internet Technology and Architecture, KTH Royal Institute of Technology, GPA: 4.61/5.0, 2014/2015 B.Sc. in Agricultural Resources and Environment, Zhejiang University, GPA: 3.66/4.0, 2010-2014 SELECTED HONORS & AWARDS Excellence Scholarship, 1000€/month+3000€ mobility allowance, sponsored by EIT Digital, 09/2014 - 08/2016 Outstanding Collegiate Volunteer, awarded by Chinese Society for Environmental Sciences (CSES), 03/2013 National Endeavor Scholarship, 5000¥, sponsored by Ministry of Education of China, 12/2012 Excellent Student Awards, awarded by Zhejiang University, 12/2012 Outstanding Student Leader Awards, Top 6%, awarded by Zhejiang University, 10/2012 Academic Excellence Scholarship, Top 15%, sponsored by Zhejiang University, 10/2012 Outstanding Student Scholarship, sponsored by Zhejiang University, 10/2011 Scholarship for Outstanding Students, Top 1‰, sponsored by Yifeng Middle School of Jiangxi, China, 09/2007 - 06/2010 R&D; EXPERIENCE R&D; Engineer at New H3C Technologies Co., Limited, from 11/2017 Topic: SDN Research and Development R&D; Engineer at Telecom Bretagne, 11/2016 - 08/2017 Topic: Trustful Hyper-linked Entities in Dynamic Networks Participated in the reThink project under the European Horizon 2020 Programme. Designed and developed JACPoL: a scalable, expressive but lightweight JSON-based access control policy language. Prototyped a performant policy evaluation engine associated with JACPoL adopting PEP/PDP architecture. Published 2 papers. Research Intern (Master Thesis) at Cisco Paris Innovation and Research Lab, 03/2016 - 09/2016 Topic: Software Defined Internet of Things (SDIoT) - 6TiSCH Centralized Scheduling and Multipath Construction Developed SDN-based routing and resource allocation schemes for deterministic IoT Low-power Lossy Networks (LLNs). Designed, implemented, and evaluated a multipath forwarding and control mechanism based on IPv6 over the TSCH mode of IEEE 802.15.4e (6TiSCH). Co-authored 3 papers, 2 Internet drafts at Internet Engineering Task Force (IETF), and 1 patent. Contributed actively to OpenWSN open-source projects. Independent Study at Telecom Bretagne, 11/2015 - 02/2016 Topic: A Secure Northbound Multi-tenant Framework for Software-defined Networks (SDN) Designed and implemented a novel multi-tenant framework with AAA, MySQL, and RESTful web services on top of OpenDaylight controller. Published the research and brought up a new concept of Software Defined Multi-tenant Networking (SDMTN). Research Intern (Bachelor Thesis) at Laboratory of Next Generation Network Technology (NGNT Lab), Zhejiang University, 02/2014 - 06/2014 Topic: Resource Management and Control for a Large-scale SDN Testbed Designed and implemented virtual-to-physical network mapping and resource allocation mechanisms for the data plane. Summer Intern at Institute of Agricultural Remote Sensing and Information Technology, Zhejiang University, 06/2013 - 08/2013 Topic: Greenhouse Environment Monitoring and Automatic Control Programmed on wireless sensor nodes with TinyOS/nesC. Conducted Matlab simulations for greenhouse energy-efficiency analysis. EXTRACURRICULAR ACTIVITIES Member of the Preparatory Committee of EU-China Youth Innovation & Entrepreneurship Forum 2017, 11/2016 - 06/2017 Participating actively in the design and preparation of the forum to promote cooperation and exchange between Chinese and European investors and start-up companies. Director of EIT CSSA (Chinese Students and Scholars Association) France, 10/2015 - 09/2016 Organized academic and recreational activities among EIT Digital students over European countries. Teaching Assistant at OpenLab IT Education, Inc., Hangzhou, 08/2013 - 01/2014 Offered technical courses on routing, switching, and troubleshooting technologies. Performed periodical maintenance of teaching materials and translation of technical documents. Director of General Affairs Department of Students' Union, Zhejiang University, 09/2011 - 09/2012 Organized a variety of campus and volunteer activities. Received Outstanding Student Leader Awards from Zhejiang University. English Tutor for a junior middle school student, 09/2011 - 02/2012 Student Technician of IT Support Center, Zhejiang University, 09/2010 - 06/2011 Supported students and faculty in solving computer and network problems. Figured out and solved the problem of DHCP failure once in a campus wireless network paralysis. TECHNICAL COURSE PROJECTS Multitasking and Event Scheduling on Wireless Sensor Networks , 01/2016-02/2016 UVDECX101-Distributed Architectures and Embedded Systems, Télécom Bretagne Developed smart LED lighting control using networked Contiki motes with distributed sensing and actuation. IoT Data Stream Processing and Analytics , 11/2015-12/2015 UVDECX501-Internet of Things and Intelligent Transport, Télécom Bretagne Analysed real-time data stream from campus wireless sensor network using Apache Storm. Performance Analysis of Backoff Algorithms in WiFi Networks , 01/2015-03/2015 IK2217-Advanced Internetworking II, KTH Royal Institute of Technology Evaluated and improved the backoff mechanism in CSMA wireless networks to optimize throughput performance through NS3 simulation. 5G Wireless System with Massive MIMO , 11/2014-01/2015 EP2950-Wireless Networks, KTH Royal Institute of Technology Investigated novel techniques of massive MIMO as a key enabler for 5G based on extensive literature research. Advanced ISP (Internet Service Provider) Networking , 09/2014-11/2014 IK2215-Advanced Internetworking, KTH Royal Institute of Technology Designed and implemented high availability inter/intra-domain routing functions, IP multicast, along with a suite of DNS, DHCP, Web, FTP, Mail, VPN, Firewall and IPS/IDS services. Traffic Light Control Based on PLC (Programmable Logic Controller) , 11/2012-01/2013 101C0020-Electrical and Electronic Experiment, Zhejiang University Designed and implemented a traffic light PLC program through ladder logic using LogixPro Simulator. BUSINESS COURSE PROJECTS An Assessment of the Business Opportunity in Telehealth Industry , 09/2016 Minor Thesis, EIT Digital Master Programme Investigated extensively key technologies and market environment for health monitoring wearable products. Proposed a novel business model based on open and user innovation paradigm. Intellitest - Loved by Everyone Except Cheaters , 07/2015-08/2015 ME2078-Summer School: Security & Privacy in Digital Life, University of Trento Designed a smart anti-cheating system integrating machine learning capabilities to make online exams more trustful. Received the top 3 best entrepreneurial team prize. Locate - Locate Things Easily , 04/2015-05/2015 ME2073-Business Development Lab of Entrepreneurship Engineers, KTH Designed, validated and developed a complete business plan with a focus on the value proposition and customer validation of personal Internet of Things products. GreenCoating , 10/2012-09/2013 National Student Research Training Program (SRTP), Zhejiang University Led the development of a plant-derived antibacterial coating additive based on mentor's patents, in cooperation with Hangzhou Risun Chemical Technology Co., LTD. Received sponsorship also from National College Student Entrepreneurship Training Program(SIETP). CCIE CERTIFICATION TRACKS CCIE (Cisco Certificated Internetwork Expert) Data Center Written Exam, Score: 984/1000, Stockholm 05/2015 CCIE Routing and Switching Lab Exam, PASS, License #40343, Beijing 08/2013 CCIE Routing and Switching Written Exam, Score: 972/1000, Hangzhou 03/2013 SKILLS Programming: Python, Java, C/C++, JavaScript, HTML, CSS, Shell, SQL Networking: TCP/IP, Software-defined Networking, Wireless and Mobile Networking, Multimedia Networking Tools: JetBrains IDEs, Microsoft Office, Adobe Creative Suite, MySQL Platforms: Linux, Mac OS, Windows Languages: Chinese (native), English (professional), French (elementary) VOLUNTEER EXPERIENCE Environmental Volunteer of Hangzhou scenic areas, 03/2014 Kept the scenic environment clean, and the gardens out of damage. Inspired the environmental awareness of tourists. Lead Volunteer of Public Environmental Education, Anji, Zhejiang Province, China, 07/2012 - 08/2012 Organized a set of public environmental education activities with topics on plant protection, waste classification, and energy conservation with 30 volunteers. Presented as a good example and received Outstanding Collegiate Volunteer awards in the annual conference of Chinese Society for Environmental Sciences. Teaching Volunteer in Shilian Primary School of Suichang, Zhejiang Province, China, 07/2011 – 08/2011 Served as a Chinese teacher for more than 20 pupils during the summer holiday. PUBLICATION \"JACPoL: A Simple but Expressive JSON-based Access Control Policy Language\". H. Jiang and A. Bouabdallah. Wistp 2017: International Conference on Information Security Theory and Practice, Springer Verlag, 28-29 September 2017. \"Towards a JSON-Based Fast Policy Evaluation Framework\". H. Jiang and A. Bouabdallah. In OTM Confederated International Conferences \"On the Move to Meaningful Internet Systems\", pp. 22-30. Springer, Cham, 2017. \"Controlled Replication for Higher Reliability and Predictability in Industrial IoT Networks\". H. Jiang , Z. Brodard, T.F. Chang, A. Bouabdallah, N. Montavont, G. Texier, P. Thubert, T. Watteyne, G. Z. Papadopoulos. In Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks (EWSN), Dependability Competition, ACM, Uppsala, Sweden​, February 20-22, 2017. \"Rover: Poor (but Elegant) Man's Testbed\". Z. Brodard, H. Jiang , T.F. Chang, T. Watteyne, X. Vilajosana, P. Thubert, G. Texier. In Proceedings of the 13th ACM Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, and Ubiquitous Networks (PE-WASUN 2016). pp. 61-65. ACM. November 17, 2016. \"A Secure Multi-Tenant Framework for SDN\". H. Jiang , A. Bouabdallah, A. Aflatoonian, J. M. Bonnin, K. Guillouard. In Proceedings of the 9th International Conference on Security of Information and Networks(SIN 2016). pp. 40-44. ACM, July 20, 2016. \"Timeslot Shifting for Lower-Priority Packets in a Time Slotted Network\". P. Thubert, Z. Brodard, H. Jiang . U.S.Patent 15/207,621 filed 12-Jul-2016. \"A 6loRH for Bit Strings\". P. Thubert, Z. Brodard, H. Jiang , & G. Texier. IETF Internet draft. Version: draft-thubert-6lo-bier-dispatch-01. IETF 6lo WG. June 29, 2016. \"BIER-TE-based OAM, Replication & Elimination\". P. Thubert, Z. Brodard, H. Jiang . IETF Internet draft. Version: draft-thubert-bier-replication-elimination-00. IETF BIER WG. September 14, 2016.","tags":"pages","url":"https://jiang-hao.com/pages/resume.html","loc":"https://jiang-hao.com/pages/resume.html"},{"title":"Search Results","text":"$(document).ready(function() { $('#tipue_search_input').tipuesearch(); });","tags":"pages","url":"https://jiang-hao.com/pages/search.html","loc":"https://jiang-hao.com/pages/search.html"},{"title":"深入理解大数据之——事务及其ACID特性","text":"事务简介 事物的定义 事务（Transaction）是由一系列对系统中数据进行访问或更新的操作所组成的一个程序执行逻辑单元（Unit）。在计算机术语中，事务通常就是指数据库事务 。 在数据库管理系统（DBMS）中，事务是数据库恢复和并发控制的基本单位。它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 例如，银行转帐工作：从源帐号扣款并使目标帐号增款，这两个操作必须要么全部执行，要么都不执行，否则就会出现该笔金额平白消失或出现的情况。所以，应该把他们看成一个事务。 在现代数据库中，事务还可以实现其他一些事情，例如，确保你不能访问别人写了一半的数据；但是基本思想是相同的——事务是用来确保 无论发生什么情况，你使用的数据都将处于一个合理的状态 ： transactions are there to ensure, that no matter what happens, the data you work with will be in a sensible state. 它保证在任何情况下都不会出现在转账后从一个帐户中扣除了资金，而未将其存入另一个帐户的情况。 事务的目的 数据库事务通常包含了一个序列的对数据库的读/写操作。包含有以下两个目的： 为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。 当事务被提交给了DBMS，则DBMS需要确保该事务中的所有操作都成功完成且其结果被永久保存在数据库中，如果事务中有的操作没有成功完成，则事务中的所有操作都需要回滚，回到事务执行前的状态；同时，该事务对数据库或者其他事务的执行无影响，所有的事务都好像在独立的运行。 Martin Kleppmann在他的《Designing Data-Intensive Applications》一书中有提到： Transactions are not a law of nature; they were created with a purpose, namely to simplify the programming model for applications accessing a database. By using transactions, the application is free to ignore certain potential error scenarios and concurrency issues, because the database takes care of them instead (we call these safety guarantees). 在现实情况下，失败的风险很高。在一个数据库事务的执行过程中，有可能会遇上事务操作失败、数据库系统或操作系统出错，甚至是存储介质出错等情况。而上述Martin的话说明了事务的存在，就是为了能够简化我们的编程模型，不需要我们去考虑各种各样的潜在错误和并发问题 。我们在实际使用事务时，不需要考虑数据库宕机，网络异常，并发修改等问题，整个事务要么提交，要么回滚，非常方便。所以本质上来说， 事务的出现了是为了应用层服务的，而不是数据库系统本身的需要 。 事务的状态 因为事务具有原子性，所以从外部看的话，事务就是密不可分的一个整体，事务的状态也只有三种：Active、Commited 和 Failed，事务要不就在执行中，要不然就是成功或者失败的状态。 进一步放大看，事物内部还有部分提交这个中间状态，其对外是不可见的。 所以，具体来说，事务有以下几种可能的状态： Active：事务的初始状态，表示事务正在执行； Partially Committed：在最后一条语句执行之后； Failed：发现事务无法正常执行之后； Aborted：事务被回滚并且数据库恢复到了事务进行之前的状态之后； Committed：成功执行整个事务。 我们也可以看到，事务在执行之后只会以Aborted或者Committed状态作为结束。 事务的ACID属性 ACID简介 为了保持数据库的一致性，在事务处理之前和之后，都遵循某些属性，也就是大家耳熟能详的ACID属性： 原子性（Atomicity）：即不可分割性，事务中的操作要么全不做，要么全做 一致性（Consistency）：一个事务在执行前后，数据库都必须处于正确的状态，满足 完整性约束 隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性（Durability）：事务处理完成后，对数据的修改就是永久的，即便系统故障也不会丢失 并非任意的对数据库的操作序列都是数据库事务。ACID属性是一系列操作组成事务的必要条件。总体而言，ACID属性提供了一种机制，使每个事务都\"作为一个单元，完成一组操作，产生一致结果，事务彼此隔离，更新永久生效\"，从而来确保数据库的正确性和一致性。 原子性（Atomicity） 原子性也被称为\"全有或全无规则\"。它非常好理解，即整个事务要么完整发生，要么根本不发生，不会部分发生。它涉及以下两个操作： 中止 ：如果事务中止，则看不到对数据库所做的更改。 提交 ：如果事务提交，则所做的更改可见。 拿之前转账的例子来说，用户A给用户B转账，至少要包含两个操作，用户A钱数减少，用户B钱数增加，增加和减少的操作要么全部成功，要么全部失败，是一个原子操作。如下图，如果事务在 T1 完成之后但在 T2 完成之前失败，将导致数据库状态不正确。 一致性（Consistency） 一致性是指，一个事务必须使数据库从一个一致性状态变换到另一个一致性状态（执行成功），或回滚到原始的一致性状态（执行失败）。这意味着必须维护完整性约束，以使在事务之前和之后数据库保持一致性和正确性。 参考上面的示例，假设用户A和用户B两者的钱加起来一共是700，那么不管A和B之间如何转账，转几次账，这一约束都得成立，即事务结束后两个用户的钱相加起来还得是700，这就是事务的一致性。 如果转账过程中，仅完成A扣款或B增款两个操作中的一个，即未保证原子性，那么结果数据如上述完整性约束也就无法得到维护，一致性也就被打破。可以看出，事务的一致性和原子性是密切相关的，原子性的破坏可能导致数据库的不一致。 但数据的一致性问题并不都和原子性有关。比如转账的过程中，用户A扣款了100，而用户B只收款了50，那么该过程可以符合原子性，但是数据的一致性就出现了问题。 一致性既是事务的属性，也是事务的目的 。也正如本文开篇所提到的，\"事务是用来确保无论发生什么情况，你使用的数据都将处于一个合理的状态\"，这里所说的合理/正确，也就是指满足完整性约束。 总的来说， 一致性是事务ACID四大特性中最重要的属性，而原子性、隔离性和持久性，都是作为保障一致性的手段。事务作为这些性质的载体，实现了这种由ACID保障C的机制。 ACID和CAP中C（一致性）的区别 请注意，我们一直在讨论的一致性，即ACID中的C，是指单一实体内部的正确状态在时间维度上的一致性，进一步说，是通过维护数据的完整性约束，来保持数据库在时间上（比如事务前后）保持一致的正确状态。因为是描述单一实体的内部状态，故又称\"内部一致性\"。 而CAP原则中的一致性是指在分布式系统中，空间维度上，某一特定时刻，多个实体中不同数据备份之间值的一致性，又称\"外部一致性\"。具体我们会在另文CAP原则相关内容中做详细介绍。 隔离性（Isolation） 隔离性是指，并发执行的各个事务之间不能互相干扰，即一个事务内部的操作及使用的数据，对并发的其他事务是隔离的。此属性确保并发执行一系列事务的效果等同于以某种顺序串行地执行它们，也就是要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。这要求两件事: 在一个事务执行过程中，数据的中间的（可能不一致）状态不应该被暴露给所有的其他事务。 两个并发的事务应该不能操作同一项数据。数据库管理系统通常使用锁来实现这个特征。 还是拿转账来说，在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。如果在A给B转账的同时，有另外一个事务执行了C给B转账的操作，那么当两个事务都结束的时候，B账户里面的钱必定是A转给B的钱加上C转给B的钱再加上自己原有的钱。 如此，隔离性防止了多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。以上4个级别的隔离性依次增强，分别解决不同的问题。 事务隔离级别越高，就越能保证数据的完整性和一致性，但同时对并发性能的影响也越大 。 持久性（Durability） 事务的持久性又称为永久性（Permanency），是指一个事务一旦提交，对数据库中对应数据的状态变更就应该是 永久性 的。即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够根据事务日志对未持久化的数据重新进行操作，将其 恢复 到事务成功结束的状态。持久性意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会因为系统故障而被回滚。（完成的事务是系统永久的部分，对系统的影响是永久性的） 许多数据库通过引入 预写式日志 （Write-ahead logging，缩写 WAL）机制，来保证事务持久性和数据完整性，同时又很大程度上避免了基于事务直接刷新数据的频繁IO对性能的影响。 在使用WAL的系统中，所有的修改都先被写入到日志中，然后再被应用到系统状态中。假设一个程序在执行某些操作的过程中机器掉电了。在重新启动时，程序可能需要知道当时执行的操作是成功了还是部分成功或者是失败了。如果使用了WAL，程序就可以检查log文件，并对突然掉电时计划执行的操作内容跟实际上执行的操作内容进行比较。在这个比较的基础上，程序就可以决定是撤销已做的操作还是继续完成已做的操作，或者是保持原样。 总结 事务（Transaction）是由一系列对系统中数据进行访问或更新的操作所组成的一个程序执行逻辑单元（Unit）。在事务的ACID特性中，C即一致性是事务的根本追求，而对数据一致性的破坏主要来自两个方面： 事务的并发执行 事务故障或系统故障 数据库系统是通过并发控制技术和日志恢复技术来避免这种情况发生的。 并发控制技术保证了事务的隔离性，使数据库的一致性状态不会因为并发执行的操作被破坏。 日志恢复技术保证了事务的原子性，使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失，保证了事务的持久性。 我们将另文对以上两种技术进行详细介绍。 参考文献 What is a database transaction? (2019). Retrieved November 5, 2019, from Stack Overflow website: https://stackoverflow.com/questions/974596/what-is-a-database-transaction Communcations and Information Processing: First International Conference, ICCIP 2012, Aveiro, Portugal, March 7-11, 2012, Proceedings, 第 2 部分 Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems, 第25节 ACID Properties in DBMS - GeeksforGeeks. (2016, August 7). Retrieved November 5, 2019, from GeeksforGeeks website: https://www.geeksforgeeks.org/acid-properties-in-dbms/ 维基百科. (2011, July 25). 预写式日志. Retrieved November 5, 2019, from Wikipedia.org website: https://zh.wikipedia.org/wiki/%E9%A2%84%E5%86%99%E5%BC%8F%E6%97%A5%E5%BF%97 数据库事务的概念及其实现原理 - takumiCX - 博客园. (2018). Retrieved November 5, 2019, from Cnblogs.com website: https://www.cnblogs.com/takumicx/p/9998844.html 浅入深出MySQL中事务的实现. (2017, August 20). Retrieved November 5, 2019, from 面向信仰编程 website: https://draveness.me/mysql-transaction ‌","tags":"Backend","url":"https://jiang-hao.com/articles/2019/backend-transactions-acid.html","loc":"https://jiang-hao.com/articles/2019/backend-transactions-acid.html"},{"title":"Flink 词汇表","text":"Flink应用程序集群 Flink应用程序集群是指仅执行一个 Flink作业 专用的 Flink集群 。该 Flink集群 的生命周期与对应Flink作业的生命周期相同。在 工作模式下， 以前的Flink应用程序集群也称为Flink集群。点击查看与 Flink Session Cluster 的比较。 Flink集群 一种分布式系统，通常由一个 Flink Master 和一个或多个 Flink TaskManager 进程组成。 事件 事件是有关由应用程序建模的域的状态更改的声明。事件可以是流或批处理应用程序的输入和/或输出。事件是特殊类型的 记录 。 执行图 见 物理图 函数 函数由用户实现，并封装Flink程序的应用程序逻辑。大多数函数由相应的 算子 包装 。 实例 术语 实例 用于描述在运行期间特定类型的特定实例（通常是 算子 或 函数 ）。由于Apache Flink主要是用Java编写的，因此它对应于Java中的 Instance 或 Object 的定义。在Apache Flink的上下文中，术语\" 并行实例\" 也经常用来强调相同 算子 或 函数 类型的多个实例正在并行运行。 Flink作业 Flink作业是Flink程序的运行时表示形式。Flink作业既可以提交到长期运行的 Flink会话集群 ，也可以作为独立的 Flink应用程序集群启动 。 作业图 请参阅 逻辑图 Flink JobManager JobManager是 Flink Master中 运行的组件之一。JobManager负责监督单个作业的 任务 执行。历史上，整个 Flink Master 都称为JobManager。 逻辑图 逻辑图是描述流处理程序的高级逻辑的有向图。节点是 算子 ，边指示输入/输出关系或数据流或数据集。 受管状态 受管状态描述了已在框架中注册的应用程序状态。对于受管状态，Apache Flink将特别关注持久性和可伸缩性。 Flink Master Flink Master是 Flink群集 的Master。它包含三个不同的组件：Flink Resource Manager（资源管理器），Flink Dispatcher（FLink 调度器）和每个运行的 Flink Job 的 Flink JobManager 。 算子 逻辑图的 节点。算子执行某种操作，通常由 Function 执行。Source和SInk是用于数据摄取和数据出口的特殊算子。 算子链 一个算子链由两个或多个连续的 算子 组成，中间没有任何重新分配（rebalance）。同一算子链中的算子无需经过序列化或Flink的网络堆栈即可直接将记录彼此转发。 分区 分区是整个数据流或数据集的独立子集。通过将每个 记录 分配给一个或多个分区，将数据流或数据集划分为多个分区。 任务 在运行时消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重新分区（repartitioning）。 物理图 物理图是转换 逻辑图 以在分布式运行时中执行的结果。节点是 任务 ，边指示数据流或数据集的输入/输出关系或 分区 。 记录 记录是数据集或数据流的组成元素。 算子 和 函数 接收记录作为输入，并发出记录作为输出。 Flink会话集群 长期运行的 Flink群集 ，它接受多个 Flink作业 来执行。此Flink群集的生存期未绑定到任何Flink作业的生存期。以前，Flink群集在 会话模式下 也称为Flink会话群集。与 Flink应用程序集群 进行比较 。 状态后端 对于流处理程序， Flink作业 的状态后端确定如何在每个TaskManager（TaskManager的Java堆或（嵌入式）RocksDB）上存储其 状态 ，以及在检查点上写入状态的位置（ Flink Master 或文件系统的Java堆） ）。 子任务 子任务是负责处理数据流 分区 的 任务 。术语\"子任务\"强调针对同一 算子 或 算子链 有多个并行任务 。 任务 物理图的 节点。任务是基本工作单元，由Flink的运行时执行。任务恰好封装了 算子 或 算子链 的一个并行实例 。 Flink任务管理器 TaskManager是 Flink群集 的工作进程。 Tasks 安排在TaskManager中执行。它们彼此通信以在连续的任务之间交换数据。 转换 将转换应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。转换可能会更改数据流或数据集的每个记录，但也可能仅更改其分区或执行聚合。虽然 算子 和 函数 是Flink API的\"物理\"部分，但转换只是API概念。具体来说，大多数（但不是全部）转换是由某些 算子 实现的。","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-glossary.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-glossary.html"},{"title":"大陆访问Vultr各节点VPS网速测试综合报告","text":"限时福利 Vultr新用户赠送$50活动入口 测试方法 采用以下方法： 本地Ping测试 腾讯云主机Ping测试 本地下载测试 腾讯云主机下载测试 站长工具综合测试 节点列表 Location Looking Glass Download Test File: IPv4 Frankfurt, DE fra-de-ping.vultr.com 100MB 1GB Amsterdam, NL ams-nl-ping.vultr.com 100MB 1GB Paris, France par-fr-ping.vultr.com 100MB 1GB London, UK lon-gb-ping.vultr.com 100MB 1GB Singapore sgp-ping.vultr.com 100MB 1GB Tokyo, Japan hnd-jp-ping.vultr.com 100MB 1GB New York (NJ) nj-us-ping.vultr.com 100MB 1GB Toronto, Canada tor-ca-ping.vultr.com 100MB 1GB Chicago, Illinois il-us-ping.vultr.com 100MB 1GB Atlanta, Georgia ga-us-ping.vultr.com 100MB 1GB Seattle, Washington wa-us-ping.vultr.com 100MB 1GB Miami, Florida fl-us-ping.vultr.com 100MB 1GB Dallas, Texas tx-us-ping.vultr.com 100MB 1GB Silicon Valley, California sjo-ca-us-ping.vultr.com 100MB 1GB Los Angeles, California lax-ca-us-ping.vultr.com 100MB 1GB Sydney, Australia syd-au-ping.vultr.com 100MB 1GB 测试结果 电信 优选节点： Toronto New York Chicago Los Angeles Atlanta Dallas Sillicon Valley 联通 优选节点： New York Chicago Atlanta Los Angeles Toronto Dallas 移动 优选节点： New York Atlanta Seattle Amsterdam Paris Sillicon Valley 多线 优选节点： New York Chicago Toronto Los Angeles Atlanta Dallas","tags":"Tools","url":"https://jiang-hao.com/articles/2019/tools-Vultr-Nodes-Speed-Tests.html","loc":"https://jiang-hao.com/articles/2019/tools-Vultr-Nodes-Speed-Tests.html"},{"title":"Flink DataStream API教程","text":"在本文中，我们将从头开始，介绍从设置Flink项目到在Flink集群上运行流分析程序。 Wikipedia提供了一个IRC频道，其中记录了对Wiki的所有编辑。我们将在Flink中读取此通道，并计算每个用户在给定时间窗口内编辑的字节数。这很容易使用Flink在几分钟内实现，但它将为您提供一个良好的基础，从而开始自己构建更复杂的分析程序。 设置Maven项目 我们将使用Flink Maven Archetype来创建我们的项目结构。有关此内容的更多详细信息，请参阅 Java API快速入门 。出于我们的目的，运行命令是这样的： $ mvn archetype:generate \\ -DarchetypeGroupId = org.apache.flink \\ -DarchetypeArtifactId = flink-quickstart-java \\ -DarchetypeVersion = 1 .8.0 \\ -DgroupId = wiki-edits \\ -DartifactId = wiki-edits \\ -Dversion = 0 .1 \\ -Dpackage = wikiedits \\ -DinteractiveMode = false 您可以编辑 groupId ， artifactId 而 package 如果你喜欢。使用上面的参数，Maven将创建一个如下所示的项目结构： $ tree wiki-edits wiki-edits/ ├── pom.xml └── src └── main ├── java │ └── wikiedits │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 我们的 pom.xml 文件已经在根目录中添加了Flink依赖项，并且有几个示例Flink程序 src/main/java 。我们可以删除示例程序，因为我们将从头开始： $ rm wiki-edits/src/main/java/wikiedits/*.java 作为最后一步，我们需要将Flink Wikipedia连接器添加为依赖关系，以便我们可以在我们的程序中使用它。编辑 dependencies 部分 pom.xml ，使其看起来像这样： <dependencies> <dependency> <groupId> org.apache.flink </groupId> <artifactId> flink-java </artifactId> <version> ${flink.version} </version> </dependency> <dependency> <groupId> org.apache.flink </groupId> <artifactId> flink-streaming-java_2.11 </artifactId> <version> ${flink.version} </version> </dependency> <dependency> <groupId> org.apache.flink </groupId> <artifactId> flink-clients_2.11 </artifactId> <version> ${flink.version} </version> </dependency> <dependency> <groupId> org.apache.flink </groupId> <artifactId> flink-connector-wikiedits_2.11 </artifactId> <version> ${flink.version} </version> </dependency> </dependencies> 注意添加的依赖项 flink-connector-wikiedits_2.11 。（此示例和Wikipedia连接器的灵感来自Apache Samza 的 Hello Samza 示例。） 写一个Flink程序 接下来是编码。启动你喜欢的IDE并导入Maven项目或打开文本编辑器并创建文件 src/main/java/wikiedits/WikipediaAnalysis.java ： package wikiedits ; public class WikipediaAnalysis { public static void main ( String [] args ) throws Exception { } } 该程序现在还非常基础，但我们会慢慢填充。请注意，我不会在此处提供import语句，因为IDE可以自动添加它们。在本文结尾将给出包括import语句的完整的代码。 Flink程序的第一步是创建一个 StreamExecutionEnvironment （或者 ExecutionEnvironment ，如果您正在编写批处理作业）。这可用于设置执行参数并创建从外部系统读取的源。所以让我们继续把它添加到main方法中： StreamExecutionEnvironment see = StreamExecutionEnvironment . getExecutionEnvironment (); 接下来，我们将创建一个从Wikipedia IRC日志中读取的源： DataStream < WikipediaEditEvent > edits = see . addSource ( new WikipediaEditsSource ()); 这创建了我们可以进一步处理的一个包含 WikipediaEditEvent 元素的 DataStream 。出于本示例的目的，我们感兴趣的是确定每个用户在特定时间窗口中添加或删除的字节数，比如说五秒。为此，我们首先要对流以用户名进行键值化，也就是说此流上的操作应考虑用户名。在我们的例子中，窗口中编辑的字节的总和应该是基于每个唯一的用户分别进行统计。对流进行键值化，我们必须提供一个 KeySelector ，如下所示： KeyedStream < WikipediaEditEvent , String > keyedEdits = edits . keyBy ( new KeySelector < WikipediaEditEvent , String >() { @Override public String getKey ( WikipediaEditEvent event ) { return event . getUser (); } }); 这为我们提供了一个具有 String 类型用户名键的 WikipediaEditEvent 数据流。我们现在可以指定我们希望在此流上加上窗口，并根据这些窗口中的元素计算结果。窗口定义了要在其上执行计算的数据流的一个切片。在无限的元素流上计算聚合时需要Windows。在我们的例子中，我们将说我们想要每五秒聚合一次编辑的字节总和： DataStream < Tuple2 < String , Long >> result = keyedEdits . timeWindow ( Time . seconds ( 5 )) . aggregate ( new AggregateFunction < WikipediaEditEvent , Tuple2 < String , Long >, Tuple2 < String , Long >>() { @Override public Tuple2 < String , Long > createAccumulator () { return new Tuple2 <>( \"\" , 0 L ); } @Override public Tuple2 < String , Long > add ( WikipediaEditEvent value , Tuple2 < String , Long > accumulator ) { accumulator . f0 = value . getUser (); accumulator . f1 += value . getByteDiff (); return accumulator ; } @Override public Tuple2 < String , Long > getResult ( Tuple2 < String , Long > accumulator ) { return accumulator ; } @Override public Tuple2 < String , Long > merge ( Tuple2 < String , Long > a , Tuple2 < String , Long > b ) { return new Tuple2 <>( a . f0 , a . f1 + b . f1 ); } }); 第一个调用 .timeWindow() 表示我们希望有五秒钟的翻滚（不重叠）窗口。第二个调用为每个Key在每个窗口切片上指定 聚合转换 。在我们的例子中，我们从一个初始值 (\"\", 0L) 开始，并在该时间窗口中为用户添加每次编辑的字节差。现在，输出流中将包含每个用户对应一个每五秒钟发出一次的 Tuple2<String, Long> 。 剩下要做的就是将流打印到控制台并开始执行： result . print (); see . execute (); 最后一个调用是启动实际Flink工作所必需的。所有操作（例如创建源Source，转换Transformation和接收器Sink）仅构建内部操作的图形。只有在 execute() 被调用时才会提交到集群上或在本地计算机上执行此操作图。 到目前为止完整的代码是这样的： package wikiedits ; import org.apache.flink.api.common.functions.AggregateFunction ; import org.apache.flink.api.java.functions.KeySelector ; import org.apache.flink.api.java.tuple.Tuple2 ; import org.apache.flink.streaming.api.datastream.DataStream ; import org.apache.flink.streaming.api.datastream.KeyedStream ; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment ; import org.apache.flink.streaming.api.windowing.time.Time ; import org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEvent ; import org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSource ; public class WikipediaAnalysis { public static void main ( String [] args ) throws Exception { StreamExecutionEnvironment see = StreamExecutionEnvironment . getExecutionEnvironment (); DataStream < WikipediaEditEvent > edits = see . addSource ( new WikipediaEditsSource ()); KeyedStream < WikipediaEditEvent , String > keyedEdits = edits . keyBy ( new KeySelector < WikipediaEditEvent , String >() { @Override public String getKey ( WikipediaEditEvent event ) { return event . getUser (); } }); DataStream < Tuple2 < String , Long >> result = keyedEdits . timeWindow ( Time . seconds ( 5 )) . aggregate ( new AggregateFunction < WikipediaEditEvent , Tuple2 < String , Long >, Tuple2 < String , Long >>() { @Override public Tuple2 < String , Long > createAccumulator () { return new Tuple2 <>( \"\" , 0 L ); } @Override public Tuple2 < String , Long > add ( WikipediaEditEvent value , Tuple2 < String , Long > accumulator ) { accumulator . f0 = value . getUser (); accumulator . f1 += value . getByteDiff (); return accumulator ; } @Override public Tuple2 < String , Long > getResult ( Tuple2 < String , Long > accumulator ) { return accumulator ; } @Override public Tuple2 < String , Long > merge ( Tuple2 < String , Long > a , Tuple2 < String , Long > b ) { return new Tuple2 <>( a . f0 , a . f1 + b . f1 ); } }); result . print (); see . execute (); } } 您可以使用Maven在IDE或命令行上运行此示例： $ mvn clean package $ mvn exec:java -Dexec.mainClass = wikiedits.WikipediaAnalysis 第一个命令构建我们的项目，第二个命令执行我们的主类。输出应该类似于： 1 > ( Fenix down,114 ) 6 > ( AnomieBOT,155 ) 8 > ( BD2412bot,-3690 ) 7 > ( IgnorantArmies,49 ) 3 > ( Ckh3111,69 ) 5 > ( Slade360,0 ) 7 > ( Narutolovehinata5,2195 ) 6 > ( Vuyisa2001,79 ) 4 > ( Ms Sarah Welch,269 ) 4 > ( KasparBot,-245 ) 每行前面的数字告诉你输出是由哪个打印Sink的并行实例产生的。 要了解更多信息，您可以查看我们的 基本概念 指南和 DataStream API 。如果您想了解如何在自己的机器上设置Flink群集并将结果写入 Kafka ，请坚持参加奖励练习。 额外练习：在群集上运行并写入Kafka 请按照我们的 本地安装教程 在您的计算机上设置Flink分发，并在继续之前参考 Kafka快速入门 以设置Kafka安装。 作为第一步，我们必须添加Flink Kafka连接器作为依赖，以便我们可以使用Kafka Sink。将其添加到 pom.xml dependency 部分： <dependency> <groupId> org.apache.flink </groupId> <artifactId> flink-connector-kafka-0.11_2.11 </artifactId> <version> ${ flink . version } </version> </dependency> 接下来，我们需要修改我们的程序。我们将移除 print() Sink，而是使用Kafka Sink。新代码如下所示： result . map ( new MapFunction < Tuple2 < String , Long >, String >() { @Override public String map(Tuple2<String, Long> tuple) { return tuple.toString() ; } } ) . addSink ( new FlinkKafkaProducer011 <>( \"localhost:9092\" , \"wiki-result\" , new SimpleStringSchema ())); 还需要导入相关的类： import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011 ; import org.apache.flink.api.common.serialization.SimpleStringSchema ; import org.apache.flink.api.common.functions.MapFunction ; 注意我们是如何在一开始的时候使用MapFunction转换 Tuple2<String, Long> 流为 String 流。我们这样做是因为将简单字符串写入Kafka更容易。然后，我们创建了一个Kafka Sink。您须先修改使主机名和端口对应您的设置。 \"wiki-result\" 是在我们运行程序之前我们将要创建的Kafka流的名称。使用Maven构建项目，因为我们需要jar文件在集群上运行： $ mvn clean package 生成的jar文件将位于 target 子文件夹中： target/wiki-edits-0.1.jar 。我们稍后会用到它。 现在我们准备启动Flink集群并运行写入Kafka的程序。转到安装Flink的位置并启动本地群集： $ cd my/flink/directory $ bin/start-cluster.sh 我们还必须创建Kafka主题，以便我们的程序可以写入它： $ cd my/kafka/directory $ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wiki-results 现在我们准备在本地Flink集群上运行我们的jar文件： $ cd my/flink/directory $ bin/flink run -c wikiedits.WikipediaAnalysis path/to/wikiedits-0.1.jar 如果一切按计划进行，那么该命令的输出应该与此类似： 03/08/2016 15:09:27 Job execution switched to status RUNNING. 03/08/2016 15:09:27 Source: Custom Source(1/1) switched to SCHEDULED 03/08/2016 15:09:27 Source: Custom Source(1/1) switched to DEPLOYING 03/08/2016 15:09:27 Window(TumblingProcessingTimeWindows(5000), ProcessingTimeTrigger, AggregateFunction$3, PassThroughWindowFunction) -> Sink: Print to Std. Out (1/1) switched from CREATED to SCHEDULED 03/08/2016 15:09:27 Window(TumblingProcessingTimeWindows(5000), ProcessingTimeTrigger, AggregateFunction$3, PassThroughWindowFunction) -> Sink: Print to Std. Out (1/1) switched from SCHEDULED to DEPLOYING 03/08/2016 15:09:27 Window(TumblingProcessingTimeWindows(5000), ProcessingTimeTrigger, AggregateFunction$3, PassThroughWindowFunction) -> Sink: Print to Std. Out (1/1) switched from DEPLOYING to RUNNING 03/08/2016 15:09:27 Source: Custom Source(1/1) switched to RUNNING 您可以看到各个算子如何开始运行。我们这里只有两个，因为 window 之后的算子由于性能原因而折叠成一个操作。在Flink，我们称之为 算子链 。 您可以通过使用 Kafka console consumer 检查Kafka Topic来观察程序的输出： bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic wiki-result 您还可以查看在 http：// localhost：8081上 运行的Flink仪表板。您将看到群集资源和正在运行的作业的概述： 如果单击正在运行的作业，您将获得一个视图，您可以在其中检查单个操作，例如，查看已处理元素的数量：","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-datastream-api.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-datastream-api.html"},{"title":"深入理解大数据之——Lambda架构","text":"传统系统的问题 \"我们正在从IT时代走向DT时代(数据时代)。IT和DT之间，不仅仅是技术的变革，更是思想意识的变革，IT主要是为自我服务，用来更好地自我控制和管理，DT则是激活生产力，让别人活得比你好\" ——阿里巴巴董事局主席马云。 数据量从M的级别到G的级别到现在T的级、P的级别。数据量的变化数据管理系统（DBMS）和数仓系统（DW）也在悄然的变化着。 传统应用的数据系统架构设计时，应用直接访问数据库系统。当用户访问量增加时，数据库无法支撑日益增长的用户请求的负载时，从而导致数据库服务器无法及时响应用户请求，出现超时的错误。出现这种情况以后，在系统架构上就采用下图的架构，在数据库和应用中间过一层缓冲隔离，缓解数据库的读写压力。 然而，当用户访问量持续增加时，就需要考虑读写分离技术（Master－Slave）架构则如下图，分库分表技术。现在，架构变得越来越复杂了，增加队列、分区、复制等处理逻辑。应用程序需要了解数据库的schema，才能访问到正确的数据。 商业现实已经发生了变化，所以现在更快做出的决定更有价值。除此之外，技术也在不断发展。Kafka，Storm，Trident，Samza，Spark，Flink，Parquet，Avro，Cloud providers等都是工程师和企业广泛采用的流行语。因此，现代基于Hadoop的M/R管道（使用Kafka，Avro和数据仓库等现代二进制格式，即Amazon Redshift，用于临时查询）可能采用以下方式： 这看起来相当不错，但它仍然是一种传统的批处理方式，具有所有已知的缺点，主要原因是客户端的数据在批处理花费大量时间完成之前的数据处理时，新的数据已经进入而导致数据过时。 Lambda架构简介 对低成本规模化的需求促使人们开始使用分布式文件系统，例如 HDFS和基于批量数据的计算系统（MapReduce 作业）。但是这种系统很难做到低延迟。用 Storm 开发的实时流处理技术可以帮助解决延迟性的问题，但并不完美。其中的一个原因是，Storm 不支持 exactly-once 语义，因此不能保证状态数据的正确性，另外它也不支持基于事件时间的处理。有以上需求的用户不得不在自己的应用程序代码中加入这些功能。后来出现了一种混合分析的方法，它将上述两个方案结合起来，既保证低延迟，又保障正确性。这个方法被称作 Lambda 架构，它通过批量 MapReduce作业提供了虽有些延迟但是结果准确的计算，同时通过Storm将最新数据的计算结果初步展示出来。 Lambda架构是由Storm的作者Nathan Marz提出的一个实时大数据处理框架。Marz在Twitter工作期间开发了著名的实时大数据处理框架Storm，Lambda架构是其根据多年进行分布式大数据系统的经验总结提炼而成。Lambda架构的目标是设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。Lambda架构整合离线计算和实时计算，融合不可变性（Immunability），读写分离和复杂性隔离等一系列架构原则，可集成Hadoop，Kafka，Storm，Spark，Hbase等各类大数据组件。 Lambda架构关键特性 Marz认为大数据系统应具有以下的关键特性： Robust and fault-tolerant（容错性和鲁棒性）：对大规模分布式系统来说，机器是不可靠的，可能会当机，但是系统需要是健壮、行为正确的，即使是遇到机器错误。除了机器错误，人更可能会犯错误。在软件开发中难免会有一些Bug，系统必须对有Bug的程序写入的错误数据有足够的适应能力，所以比机器容错性更加重要的容错性是人为操作容错性。对于大规模的分布式系统来说，人和机器的错误每天都可能会发生，如何应对人和机器的错误，让系统能够从错误中快速恢复尤其重要。 Low latency reads and updates（低延时）：很多应用对于读和写操作的延时要求非常高，要求对更新和查询的响应是低延时的。 Scalable（横向扩容）：当数据量/负载增大时，可扩展性的系统通过增加更多的机器资源来维持性能。也就是常说的系统需要线性可扩展，通常采用scale out（通过增加机器的个数）而不是scale up（通过增强机器的性能）。 General（通用性）：系统需要能够适应广泛的应用，包括金融领域、社交网络、电子商务数据分析等。 Extensible（可扩展）：需要增加新功能、新特性时，可扩展的系统能以最小的开发代价来增加新功能。 Allows ad hoc queries（方便查询）：数据中蕴含有价值，需要能够方便、快速的查询出所需要的数据。 Minimal maintenance（易于维护）：系统要想做到易于维护，其关键是控制其复杂性，越是复杂的系统越容易出错、越难维护。 Debuggable（易调试）：当出问题时，系统需要有足够的信息来调试错误，找到问题的根源。其关键是能够追根溯源到每个数据生成点。 数据系统的本质 为了设计出能满足前述的大数据关键特性的系统，我们需要对数据系统有本质性的理解。我们可将数据系统简化为： 数据系统 = 数据 + 查询 从而从数据和查询两方面来认识大数据系统的本质。 数据的特性： when & what 我们先从\"数据\"的特性谈起。数据是一个不可分割的单位，数据有两个关键的性质：When和What。 When 是指数据是与时间相关的，数据一定是在某个时间点产生的。比如Log日志就隐含着按照时间先后顺序产生的数据，Log前面的日志数据一定先于Log后面的日志数据产生；消息系统中消息的接受者一定是在消息的发送者发送消息后接收到的消息。相比于数据库，数据库中表的记录就丢失了时间先后顺序的信息，中间某条记录可能是在最后一条记录产生后发生更新的。对于分布式系统，数据的时间特性尤其重要。分布式系统中数据可能产生于不同的系统中，时间决定了数据发生的全局先后顺序。比如对一个值做算术运算，先+2，后 3，与先 3，后+2，得到的结果完全不同。数据的时间性质决定了数据的全局发生先后，也就决定了数据的结果。 What 是指数据的本身。由于数据跟某个时间点相关，所以数据的本身是不可变的(immutable)，过往的数据已经成为事实（Fact），你不可能回到过去的某个时间点去改变数据事实。这也就意味着对数据的操作其实只有两种：读取已存在的数据和添加更多的新数据。采用数据库的记法，CRUD就变成了CR，Update和Delete本质上其实是新产生的数据信息，用C来记录。 数据的存储：Store Everything Rawly and Immutably 根据上述对数据本质特性的分析，Lamba架构中对数据的存储采用的方式是：数据不可变，存储所有数据。 通过采用不可变方式存储所有的数据，可以有如下好处： 简单。采用不可变的数据模型，存储数据时只需要简单的往主数据集后追加数据即可。相比于采用可变的数据模型，为了Update操作，数据通常需要被索引，从而能快速找到要更新的数据去做更新操作。 应对人为和机器的错误。前述中提到人和机器每天都可能会出错，如何应对人和机器的错误，让系统能够从错误中快速恢复极其重要。不可变性（Immutability）和重新计算（Recomputation）则是应对人为和机器错误的常用方法。采用可变数据模型，引发错误的数据有可能被覆盖而丢失。相比于采用不可变的数据模型，因为所有的数据都在，引发错误的数据也在。修复的方法就可以简单的是遍历数据集上存储的所有的数据，丢弃错误的数据，重新计算得到Views。重新计算的关键点在于利用数据的时间特性决定的全局次序，依次顺序重新执行，必然能得到正确的结果。 当前业界有很多采用不可变数据模型来存储所有数据的例子。比如分布式数据库Datomic，基于不可变数据模型来存储数据，从而简化了设计。分布式消息中间件Kafka，基于Log日志，以追加append-only的方式来存储消息。 查询的本质 查询是个什么概念？Marz给查询如下一个简单的定义： Query = Function(All Data) 该等式的含义是：查询是应用于数据集上的函数。该定义看似简单，却几乎囊括了数据库和数据系统的所有领域：RDBMS、索引、OLAP、OLTP、MapReduce、EFL、分布式文件系统、NoSQL等都可以用这个等式来表示。 让我们进一步深入看一下函数的特性，从而挖掘函数自身的特点来执行查询。 有一类称为Monoid特性的函数应用非常广泛。Monoid的概念来源于范畴学（Category Theory），其一个重要特性是满足结合律。如整数的加法就满足Monoid特性： (a+b)+c=a+(b+c) 不满足Monoid特性的函数很多时候可以转化成多个满足Monoid特性的函数的运算。如多个数的平均值Avg函数，多个平均值没法直接通过结合来得到最终的平均值，但是可以拆成分母除以分子，分母和分子都是整数的加法，从而满足Monoid特性。 Monoid的结合律特性在分布式计算中极其重要，满足Monoid特性意味着我们可以将计算分解到多台机器并行运算，然后再结合各自的部分运算结果得到最终结果。同时也意味着部分运算结果可以储存下来被别的运算共享利用（如果该运算也包含相同的部分子运算），从而减少重复运算的工作量。 Lambda的三层架构 有了上面对数据系统本质的探讨，下面我们来讨论大数据系统的关键问题：如何实时地在任意大数据集上进行查询？大数据再加上实时计算，问题的难度比较大。 最简单的方法是，根据前述的查询等式 Query = Function(All Data) ，在全体数据集上在线运行查询函数得到结果。但如果数据量比较大，该方法的计算代价太大了，所以不现实。 Lambda架构通过分解的三层架构来解决该问题：Batch Layer，Speed Layer和Serving Layer。 Batch Layer 理想状态下，任何数据访问都可以从表达式Query= function(all data)开始，但是，若数据达到相当大的一个级别（例如PB），且还需要支持实时查询时，就需要耗费非常庞大的资源。一个解决方式是预运算查询函数（precomputed query function）。书中将这种预运算查询函数称之为Batch View（A），这样当需要执行查询时，可以从Batch View中读取结果。这样一个预先运算好的View是可以建立索引的，因而可以支持随机读取（B）。于是系统就变成： （A）batch view = function(all data) （B）query = function(batch view) 在Lambda架构中，实现（A）batch view =function(all data)的部分称之为Batch Layer。Batch Layer的功能主要有两点： 存储master dataset, 这是一个不变的持续增长的数据集 在master dataset上预先计算查询函数，构建查询所对应的View 存储数据集 根据前述对数据When&What特性的讨论，Batch Layer采用不可变模型存储所有的数据。因为数据量比较大，可以采用HDFS之类的大数据储存方案。如果需要按照数据产生的时间先后顺序存放数据，可以考虑如InfluxDB之类的时间序列数据库（TSDB）存储方案。 构建查询View 上面说到根据等式Query = Function(All Data)，在全体数据集上在线运行查询函数得到结果的代价太大。但如果我们预先在数据集上计算并保存查询函数的结果，查询的时候就可以直接返回结果（或通过简单的加工运算就可得到结果）而无需重新进行完整费时的计算了。这儿可以把Batch Layer看成是一个数据预处理的过程。我们把针对查询预先计算并保存的结果称为View，View是Lambda架构的一个核心概念，它是针对查询的优化，通过View即可以快速得到查询结果。 显然，batch view是一个批处理过程，如采用Hadoop或spark支持的map－reduce方式。采用这种方式计算得到的每个view都支持再次计算，且每次计算的结果都相同。Batch Layer的工作可以简单的用如下伪码表示： 该工作看似简单，实质非常强大。任何人为或机器发生的错误，都可以通过修正错误后重新计算来恢复得到正确结果。 对View的理解 View是一个和业务关联性比较大的概念，View的创建需要从业务自身的需求出发。一个通用的数据库查询系统，查询对应的函数千变万化，不可能穷举。但是如果从业务自身的需求出发，可以发现业务所需要的查询常常是有限的。Batch Layer需要做的一件重要的工作就是根据业务的需求，考察可能需要的各种查询，根据查询定义其在数据集上对应的Views。 Batch Layer的Immutable data模型和Views 如下图agent id＝50023的人，在10:00:06分的时候，状态是calling，在10:00:10的时候状态为waiting。在传统的数据库设计中，直接后面的纪录覆盖前面的纪录，而在Immutable数据模型中，不会对原有数据进行更改，而是采用插入修改纪录的形式更改历史纪录。 上文所提及的View是上图中预先计算得到的相关视图，例如： 2016-06-21 当天所有上线的agent数，每条热线、公司下上线的Agent数。根据业务需要，预先计算出结果。此过程相当于传统数仓建模的应用层，应用层也是根据业务场景，预先加工出的view。 Speed Layer Batch Layer可以很好的处理离线数据，但有很多场景数据不断实时生成，并且需要实时查询处理。Speed Layer正是用来处理增量的实时数据。 Speed Layer和Batch Layer比较类似，对数据进行计算并生成Realtime View，其主要区别在于： Speed Layer处理的数据是最近的增量数据流，Batch Layer处理的全体数据集 Speed Layer为了效率，接收到新数据时不断更新Realtime View，而Batch Layer根据全体离线数据集直接得到Batch View。Speed Layer是一种增量计算，而非重新计算（recomputation） Speed Layer因为采用增量计算，所以延迟小，而Batch Layer是全数据集的计算，耗时比较长 综上所诉，Speed Layer是Batch Layer在实时性上的一个补充。Speed Layer可总结为： （C）realtime view＝function(realtime view，new data) 注意，realtime view是基于新数据和已有的realtime view。 Lambda架构将数据处理分解为Batch Layer和Speed Layer有如下优点： 容错性。Speed Layer中处理的数据也不断写入Batch Layer，当Batch Layer中重新计算的数据集包含Speed Layer处理的数据集后，当前的Realtime View就可以丢弃，这也就意味着Speed Layer处理中引入的错误，在Batch Layer重新计算时都可以得到修正。这点也可以看成是CAP理论中的最终一致性（Eventual Consistency）的体现。 复杂性隔离。Batch Layer处理的是离线数据，可以很好的掌控。Speed Layer采用增量算法处理实时数据，复杂性比Batch Layer要高很多。通过分开Batch Layer和Speed Layer，把复杂性隔离到Speed Layer，可以很好的提高整个系统的鲁棒性和可靠性。 如前所述，任何传入查询都必须通过合并来自批量视图和实时视图的结果来得到答案，因此这些视图需要满足Monoid的结合律特性。需要注意的一点是，实时视图是以前的实时视图和新数据增量的函数，因此可以使用增量算法。批处理视图是所有数据的函数，因此应该在那里使用重算算法。 Serving Layer Lambda架构的Serving Layer用于响应用户的查询请求，合并Batch View和Realtime View中的结果数据集到最终的数据集。 这儿涉及到数据如何合并的问题。前面我们讨论了查询函数的Monoid性质，如果查询函数满足Monoid性质，即满足结合律，只需要简单的合并Batch View和Realtime View中的结果数据集即可。否则的话，可以把查询函数转换成多个满足Monoid性质的查询函数的运算，单独对每个满足Monoid性质的查询函数进行Batch View和Realtime View中的结果数据集合并，然后再计算得到最终的结果数据集。另外也可以根据业务自身的特性，运用业务自身的规则来对Batch View和Realtime View中的结果数据集合并。 综上所诉，Serving Layer采用如下等式表示： （D）query ＝ function(batch view, realtime view) Lambda架构组件选型 上面分别讨论了Lambda架构的三层：Batch Layer，Speed Layer和Serving Layer。总结下来，Lambda架构就是如下的三个等式： batch view = function(all data) realtime view = function(realtime view, new data) query = function(batch view, realtime view) 下图给出了Lambda架构的一个完整视图和流程。 数据流进入系统后，同时发往Batch Layer和Speed Layer处理。Batch Layer以不可变模型离线存储所有数据集，通过在全体数据集上不断重新计算构建查询所对应的Batch Views。Speed Layer处理增量的实时数据流，不断更新查询所对应的Realtime Views。Serving Layer响应用户的查询请求，合并Batch View和Realtime View中的结果数据集到最终的数据集。 组件选型 下图给出了Lambda架构中各组件在大数据生态系统中和阿里集团的常用组件。数据流存储选用不可变日志的分布式系统Kafka、TT、Metaq；BatchLayer数据集的存储选用Hadoop的HDFS或者阿里云的ODPS；BatchView的加工采用MapReduce；BatchView数据的存储采用Mysql（查询少量的最近结果数据）、Hbase（查询大量的历史结果数据）。SpeedLayer采用增量数据处理Storm、Flink；RealtimeView增量结果数据集采用内存数据库Redis。 另一个实现版本： 根据batch layer的特点，具备存储(HDFS)和计算(MapReduce)的Hadoop显然是第一人选，而batch view 可以是hadoop本身的hdfs 或者基于hdfs的所构建的类似hive那样的仓库，speed layer因为时效性的影响，采用实时流式处理系统，例如strom或者spark streaming, 而speed view 可以存在HBase 或者其他类似的Nosql数据库。server layer 提供用户查询的方法，采用facebook 开源的Impala，统一入口查询。或者自己实现hive和HBase统一查询。这是两年前的文章，当时spark 还没那么火，现在看来spark可以直接作为batch和speed层的替代者了。 选型原则 Lambda架构是个通用框架，各个层选型时不要局限时上面给出的组件，特别是对于View的选型。从我对Lambda架构的实践来看，因为View是个和业务关联性非常大的概念，View选择组件时关键是要根据业务的需求，来选择最适合查询的组件。不同的View组件的选择要深入挖掘数据和计算自身的特点，从而选择出最适合数据和计算自身特点的组件，同时不同的View可以选择不同的组件。 总结 在过去Lambda数据架构成为每一个公司大数据平台必备的架构，它解决了一个公司大数据批量离线处理和实时数据处理的需求。一个典型的Lambda架构如下： 数据从底层的数据源开始，经过各种各样的格式进入大数据平台，在大数据平台中经过Kafka、Flume等数据组件进行收集，然后分成两条线进行计算。一条线是进入流式计算平台（例如 Storm、Flink或者Spark Streaming），去计算实时的一些指标；另一条线进入批量数据处理离线计算平台（例如Mapreduce、Hive，Spark SQL），去计算T+1的相关业务指标，这些指标需要隔日才能看见。 Lambda架构经历多年的发展，其优点是稳定，对于实时计算部分的计算成本可控，批量处理可以用晚上的时间来整体批量计算，这样把实时计算和离线计算高峰分开，这种架构支撑了数据行业的早期发展，但是它也有一些致命缺点，并在大数据3.0时代越来越不适应数据分析业务的需求。缺点如下： 实时与批量计算结果不一致引起的数据口径问题 ：因为批量和实时计算走的是两个计算框架和计算程序，算出的结果往往不同，经常看到一个数字当天看是一个数据，第二天看昨天的数据反而发生了变化。 批量计算在计算窗口内无法完成 ：在IOT时代，数据量级越来越大，经常发现夜间只有4、5个小时的时间窗口，已经无法完成白天20多个小时累计的数据，保证早上上班前准时出数据已成为每个大数据团队头疼的问题。 开发和维护的复杂性问题 ：Lambda 架构需要在两个不同的 API（application programming interface，应用程序编程接口）中对同样的业务逻辑进行两次编程：一次为批量计算的ETL系统，一次为流式计算的Streaming系统。针对同一个业务问题产生了两个代码库，各有不同的漏洞。这种系统实际上非常难维护 服务器存储大 ：数据仓库的典型设计，会产生大量的中间结果表，造成数据急速膨胀，加大服务器存储压力。 也就是由于Lambda架构的以上局限性，Kappa应运而生，它比Lambda架构更加灵活和精简，具体将另文介绍。 Kappa架构：","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-lambda-architecture.html","loc":"https://jiang-hao.com/articles/2019/big-data-lambda-architecture.html"},{"title":"Flink 分布式运行时环境","text":"Tasks and Operator Chains 对于分布式执行，Flink将多个算子子任务链串联成任务。每个任务由一个线程执行。将算子链接到任务是一项有用的优化：它可以减少线程到线程切换和缓冲的开销，并在降低延迟的同时提高整体吞吐量。可以配置链接行为; 有关详细信息，请参阅 链接文档 。 下图中的示例数据流由五个子任务执行，因此具有五个并行线程。 Job Managers, Task Managers, Clients Flink运行时包含两种类型的进程： JobManagers （也称为 masters ）协调分布式执行。他们安排任务，协调检查点，协调故障恢复等。 Job Manager总是至少有一个。高可用性设置将具有多个JobManagers，其中一个始终是 领导者 ，其他处于 待机状态 。 TaskManagers （也叫 workers ）执行dataflow的 任务 （或者更具体地说应该是子任务），以及缓冲和交换data streams 。 必须始终至少有一个TaskManager。 JobManagers和TaskManagers可以通过多种方式启动：作为 独立集群 直接在计算机上，在容器中，或由 YARN 或 Mesos 等资源框架管理。TaskManagers连接到JobManagers，宣布自己可用，并被分配工作。 Client 不是运行时和程序执行的一部分，而是被用来准备和发送dataflow到JobManager。之后，客户端可以断开连接或保持连接以接收进度报告。客户端既可以作为触发执行的Java / Scala程序的一部分运行，也可以在命令行进程 ./bin/flink run ... 中运行。 Task Slots and Resources 每个worker（TaskManager）都是一个 JVM进程 ，可以在不同的线程中执行一个或多个子任务。为了控制一个worker接受的任务数量，每个worker都有所谓的 task slots （至少一个）。 每个 task slot 代表TaskManager的一个固定资源子集。例如，具有3个task slot的TaskManager会将其1/3的托管内存分配于每个task slot。资源切片意味着子任务不会与来自托管内存的其他作业的子任务竞争资源，相反其具有一定量的预留托管内存。请注意，这里没有CPU隔离; 当前task slots只分隔任务的托管内存。 通过调整task slot的数量，用户可以定义子任务如何相互隔离。每个TaskManager有一个slot意味着每个任务组在一个单独的JVM中运行（比如也就可以在一个单独的容器中启动）。拥有多个slots意味着更多子任务共享同一个JVM。同一JVM中的任务共享TCP连接（通过多路复用）和心跳消息。它们还可以共享数据集和数据结构，从而减少每任务开销。 默认情况下，Flink允许子任务共享slots，即使它们是不同任务的子任务，只要它们来自同一个job。结果是一个slot可以承载某个job的整个pipeline。允许这种 slot 共享有两个主要好处： Flink集群需要与job中使用的最高并行度一样多的task slot。无需计算程序总共包含多少任务（具有不同的并行性）。 更容易获得更好的资源利用率。没有slot共享，非密集 source/ map（） 子任务将占用与资源密集型 window 子任务一样多的资源。通过slot共享，将示例中的基本并行性从2增加到6可以充分利用切片后的资源，同时确保繁重的子任务在TaskManagers之间公平分配。 该API还包括可用于防止不期望的slot共享发生的 resource group 机制。 根据经验，一个很好的默认task slots数就是CPU核心数。使用超线程，每个slot则需要2个或更多硬件线程上下文。 State Backends 存储键/值索引的确切数据结构取决于所选的 状态后端 。一种状态后端将数据存储在内存中的hash map中，另一种状态后端使用 RocksDB 作为键/值存储。除了定义保存状态的数据结构之外，状态后端还实现逻辑以获取键/值状态的时间点快照，并将该快照存储为一个checkpoint的一部分。 Savepoints 用Data Stream API编写的程序可以从 savepoint 恢复执行。savepoint允许更新程序和Flink群集，而不会丢失任何状态。 savepoints 是 手动触发的checkpoints ，它 捕获 程序的快照并将其写入状态后端。他们依靠常规的checkpointing机制。在执行期间，程序会定期在工作节点上创建快照并生成检查点。对于恢复，仅需要最后完成的检查点；并且一旦完成新检查点就可以安全地丢弃旧检查点。 保存点与这些定期检查点类似，不同之处在于它们 由用户触发， 并且在较新的检查点完成时 不会自动过期 。可以 从命令行 创建保存点，也可以在通过 REST API 取消作业时创建。","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-distributed-runtime.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-distributed-runtime.html"},{"title":"Flink FAQ","text":"Apache Flink仅用于（近）实时处理用例吗？ Flink是一个非常通用的系统，用于数据处理和数据驱动的应用程序， 数据流 作为核心构建块。这些数据流可以是实时数据流或存储的历史数据流。例如，在Flink的视图中，文件是存储的字节流。因此，Flink支持实时数据处理和应用程序，以及批处理应用程序。 流可以是 无界的 （没有结束，事件不断发生）或受 限制 （流有开始和结束）。例如，来自消息队列的Twitter馈送或事件流通常是无界的流，而来自文件的字节流是有界流。 如果一切都是流，为什么Flink中有DataStream和DataSet API？ 有界流通常比无界流更高效。在（近）实时处理无限事件流需要系统能够立即对事件起作用并产生中间结果（通常具有低延迟）。处理有界流通常不需要产生低延迟结果，因为无论如何数据都是旧的（相对而言）。这允许Flink以简单且更高效的方式处理数据。 DataStream API 捕获无界和有界的流的连续处理，以支持低等待时间的结果以及对事件和时间（包括事件时间）灵活反应的模型。 DataSet API 具有加快有界的数据流的处理的技术。将来，社区计划将这些优化与DataStream API中的技术相结合。 Flink如何与Hadoop栈相关联？ Flink独立于 Apache Hadoop， 并且在没有任何Hadoop依赖性的情况下运行。 但是，Flink与许多Hadoop组件集成得非常好，例如 HDFS ， YARN 或 HBase 。与这些组件一起运行时，Flink可以使用HDFS读取数据，或写入结果和检查点/快照。Flink可以通过YARN轻松部署，并与YARN和HDFS Kerberos安全模块集成。 Flink可运行的其他栈是什么？ 用户在 Kubernetes ， Mesos ， Docker 上运行Flink ，甚至作为独立服务运行。 使用Flink有哪些先决条件？ 您需要 Java 8 来运行Flink作业/应用程序。 Scala API（可选）基于Scala 2.11。 Apache ZooKeeper 需要高度可用且没有单点故障的设置。 对于可以从故障中恢复的高可用流处理设置，Flink需要某种形式的分布式存储用于检查点（HDFS / S3 / NFS / SAN / GFS / Kosmos / Ceph / ...）。 Flink支持多大的规模？ 用户可在非常小的配置（少于5个节点）和1000个节点以及TB级的状态上运行Flink作业。 Flink是否仅限于内存数据集？ 对于DataStream API，Flink支持大于内存的状态来配置RocksDB状态后端。 对于DataSet API，所有操作（delta迭代除外）都可以扩展到主内存之外。","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-FAQ.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-FAQ.html"},{"title":"Flink Dataflow 编程模型","text":"Apache Flink是一个用于分布式流和批处理数据处理的开源平台。Flink的核心是流数据流引擎，为数据流上的分布式计算提供数据分发，通信和容错。Flink基于流引擎之上构建批处理，覆加本地迭代支持，内存托管和程序优化。 抽象层次 Flink提供不同级别的抽象来开发流/批处理应用程序。 最低级抽象只提供 stateful streaming 。它通过 Process Function 嵌入到 DataStream API中 。它允许用户自由处理来自一个或多个流的事件，并使用一致的容错 状态 。此外，用户可以注册事件时间和处理时间回调，允许程序实现复杂的计算。 在实践中，大多数应用程序不需要上述低级抽象，而是针对 Core API 编程， 如 DataStream API （有界/无界流）和 DataSet API （有界数据集）。这些流畅的API提供了用于数据处理的通用构建块，例如各种形式的用户指定的转换，连接，聚合，窗口，状态等（transformations, joins, aggregations, windows, state, etc.）。在这些API中处理的数据类型在相应的编程语言中表示为类。 低级 Process Function 与 DataStream API 集成在一起，因此只能对某些操作进行低级抽象。 DataSet API 提供有限数据集额外的原语支持，如循环/迭代。 Table API 是以 tables 为中心的声明性DSL ，此处的表是动态改变的表（当表示流时）。 Table API 遵循（扩展）关系模型：表有一个模式连接（类似于在关系数据库中的表）和API提供可比的操作，如select, project, join, group-by, aggregate等。Table API程序以声明方式定义 应该执行的逻辑操作， 而不是准确指定 操作代码的外观 。虽然Table API可以通过各种类型的用户定义函数进行扩展，但它的表现力不如 Core API ，但使用更简洁（编写的代码更少）。此外，Table API程序还会通过优化程序，在执行之前应用优化规则。 可以在 tables 和 DataStream / DataSet 之间无缝转换，允许程序混合 Table API 以及 DataStream 和 DataSet API。 Flink提供的最高级抽象是 SQL 。这种抽象在语义和表达方面类似于 Table API ，但是将程序表示为SQL查询表达式。 SQL 抽象与Table API紧密地相互作用，并且SQL语句可以作用在 Table API 定义的 tables 上。 程序和数据流 Flink程序的基本构建块是 streams 和 transformations 。（请注意，Flink的DataSet API中使用的DataSets其实内部也是streams - 稍后会详细介绍。）从概念上讲， stream 是（可能永无止境的）数据记录流，而 transformation 是将一个或多个流作为输入，并产生一个或多个输出流作为结果的操作。 执行时，Flink程序映射为 streaming dataflows ，由 流 和转换 算子 组成。每个 dataflow 都以一个或多个 sources 开头，并以一个或多个 sinks 结束。数据流类似于任意 有向无环图 （DAG） 。尽管通过 迭代 结构允许特殊形式的循环 ，但为了简单起见，我们将在大多数情况下不将其考虑其中。 通常，程序中的transformations与dataflow中的operators之间存在一对一的对应关系。但是，有时一个转换可能包含多个转换算子。 Sources和Sinks在 流连接器 和 批处理连接器 文档中有介绍。 DataStream算子 和 DataSet转换 中记录了 转换 有关介绍。 并行数据流 Flink中的程序本质上是并行和分布式的。在执行期间， stream 具有一个或多个 stream partitions ，并且每个 算子 具有一个或多个 算子子任务 。算子子任务彼此独立，并且可以在不同的线程中执行，并且可能在不同的机器或容器上执行。 算子子任务的数量是该特定算子的 并行 度。流的并行度始终是其生成算子的并行度。同一程序的不同算子可能具有不同的并行级别。 流可以 以一对一 （或 转发 ）模式或以 重新分发 模式在两个operator之间传输数据： 一对一 流（例如，在上图中的 Source 和 map（） 算子之间）保留元素的分区和排序。这意味着 map（） 算子的subtask [1] 将以 Source 算子的subtask [1]生成的相同顺序的相同元素作为输入。 重新分配 流（在上面的 map（） 和 keyBy / window 之间，以及 keyBy / window 和 Sink之间 ）重新分配流。每个 算子子任务 将数据发送到不同的目标子任务，具体取决于所选的transformation。实例是 keyBy（） （其通过散列密钥重新分区）， broadcast（） ，或 rebalance（） （随机地重新分区）。在 rebalance 交换中，元素之间的排序仅保留在每对发送和接收子任务中（例如， map（）的 子任务[1] 和子任务[2] keyBy / window ）。因此，在此示例中，保留了每个密钥内的排序，但并行性确实引入了关于不同密钥的聚合结果到达接收器的顺序的非确定性。 有关配置和控制并行性的详细信息，请参阅 并行执行 的文档。 窗口 聚合事件（例如，计数，求和）在流上的工作方式与批处理方式不同。例如，不可能计算流中的所有元素，因为流通常是无限的（无界）。相反，流上的聚合（计数，总和等）由 窗口 限定，例如 \"在最后5分钟内计数\" 或 \"最后100个元素的总和\" 。 Windows可以是 时间驱动的 （例如：每30秒）或 数据驱动 （例如：每100个元素）。人们通常区分不同类型的窗口，例如 翻滚窗口 （没有重叠）， 滑动窗口 （具有重叠）和 会话窗口 （由不活动间隙打断）。 可以在此 博客文章中 找到更多窗口示例。更多详细信息在 窗口文档中 。 时间 当在流程序中引用时间（例如定义窗口）时，可以参考不同的时间概念： 事件时间 是创建 事件的时间 。它通常由事件中的时间戳描述，例如由生产传感器或生产服务附加。Flink通过 时间戳分配器 访问事件时间戳。 摄取时间 是事件在Source算子处输入Flink数据流的时间。 处理时间 是执行基于时间的操作的每个算子的本地时间。 有关如何处理时间的更多详细信息，请参阅 事件时间文档 。 有状态的操作 虽然数据流中的许多操作只是一次查看一个单独的 事件 （例如事件解析器），但某些操作会记住多个事件（例如窗口算子）的信息。这些操作称为 有状态 。 状态操作的状态保持在可以被认为是嵌入式键/值存储的状态中。状态被分区并严格地与有状态算子读取的流一起分发。因此，只有在 keyBy（） 函数之后才能在 键控流 上访问键/值状态，并且限制为与当前事件的键相关联的值。对齐流和状态的密钥可确保所有状态更新都是本地操作，从而保证一致性而无需事务开销。此对齐还允许Flink重新分配状态并透明地调整流分区。 有关更多信息，请参阅有关 状态 的文档。 容错检查点 Flink使用 流重放 和 检查点 的组合实现容错。检查点与每个输入流中的特定点以及每个算子的对应状态相关。通过恢复算子的状态并从检查点重放事件，可以从检查点恢复流数据流，同时保持一致性 （恰好一次处理语义） 。 检查点间隔是在执行期间用恢复时间（需要重放的事件的数量）来折衷容错开销的手段。 容错内部 的描述提供了有关Flink如何管理检查点和相关主题的更多信息。有关启用和配置检查点的详细信息，请参阅 检查点API文档 。 批量流媒体 Flink以流程序的特殊情况执行 批处理程序 ，其中流是有界的（有限数量的元素）。 DataSet 在内部视为数据流。因此，上述概念以适用于流程序相同的方式应用于批处理程序，除了少数例外： 批处理程序的容错 不使用检查点(checkpointing)。而是通过完全重放流来进行恢复。这是可行的，因为输入的有限性。这会使成本更多地用于恢复，但使常规处理更轻松，因为它避免了检查点。 DataSet API中的有状态操作使用简化的内存/核外数据结构，而不是键/值索引。 DataSet API引入了特殊的同步（超级步骤）迭代，这些迭代只能在有界流上进行。有关详细信息，请查看 迭代文档 。","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-programming-model.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-programming-model.html"},{"title":"Flink用例","text":"Apache Flink因其丰富的功能集而成为开发和运行多种不同类型应用程序的绝佳选择。Flink的功能包括支持流和批处理，复杂的状态管理，事件时间处理语义以及状态的精确一次一致性保证。此外，Flink可以部署在各种资源提供者（如YARN，Apache Mesos和Kubernetes）上，也可以作为裸机硬件上的独立群集。配置为高可用性，Flink没有单点故障。Flink已被证明可扩展到数千个核心和TB级的应用程序状态，提供高吞吐量和低延迟，并为世界上最苛刻的流处理应用程序提供支持。 下面，我们将探讨由Flink提供支持的最常见类型的应用程序，并指出实际示例。 事件驱动的应用程序 数据分析应用 数据管道应用 事件驱动的应用程序 什么是事件驱动的应用程序？ 事件驱动的应用程序是一个有状态的应用程序，它从一个或多个事件流中提取事件，并通过触发计算，状态更新或外部操作对传入事件做出反应。 事件驱动的应用程序是传统应用程序设计的演变。传统应用一般具有独立的计算和数据存储层，在此体系结构中，应用程序从远程事务数据库读取数据并将数据持久化。 相反，事件驱动的应用程序基于有状态流处理应用程序。在这种设计中，数据和计算是共同定位的，即本地（内存或磁盘）数据访问。通过定期将检查点写入远程持久存储来实现容错。下图描绘了传统应用程序体系结构和事件驱动应用程序之间的差异。 事件驱动的应用程序有哪些优点？ 事件驱动的应用程序不是查询远程数据库，而是在本地访问其数据，从而在吞吐量和延迟方面产生更好的性能。远程持久存储的定期检查点可以异步和递增完成。因此，检查点对常规事件处理的影响非常小。但是，事件驱动的应用程序设计提供的不仅仅是本地数据访问。在分层体系结构中，多个应用程序共享同一数据库是很常见的。因此，需要协调数据库的任何更改，例如由于应用程序更新或扩展服务而更改数据布局。由于每个事件驱动的应用程序都负责自己的数据，因此更改数据表示或扩展应用程序只需要较少的协调。 Flink如何支持事件驱动的应用程序？ 事件驱动应用程序的限制由流处理器处理时间和状态的程度来定义。Flink的许多杰出功能都围绕着这些概念。Flink提供了一组丰富的状态原语，可以管理非常大的数据量（最多几TB），并且具有精确一次性的一致性保证。此外，Flink支持事件时间，高度可定制的窗口逻辑，以及通过 ProcessFunction 实现高级业务逻辑提供的细粒度时间控制。此外，Flink还提供了一个用于复杂事件处理（CEP）的库，用于检测数据流中的模式。 然而，Flink在事件驱动应用程序方面的出色功能是savepoint。保存点是一致的状态快照，可用作兼容应用程序的起点。给定保存点，可以更新应用程序或调整其规模，或者可以启动应用程序的多个版本以进行A / B测试。 什么是典型的事件驱动应用程序？ 欺诈识别 异常检测 基于规则的警报 业务流程监控 Web应用程序（社交网络） 数据分析应用 什么是数据分析应用程序？ 分析工作从原始数据中提取信息和洞察力。传统上，分析在记录事件的有界数据集上作为批量查询或应用程序执行。为了将最新数据合并到分析结果中，必须将其添加到分析的数据集中，并重新运行查询或应用程序。结果将写入存储系统或作为报告发出。 借助先进的流处理引擎，还可以实时地执行分析。流式查询或应用程序不是读取有限数据集，而是摄取实时事件流，并在消耗事件时不断生成和更新结果。结果要么写入外部数据库，要么保持为内部状态。仪表板应用程序可以从外部数据库读取最新结果或直接查询应用程序的内部状态。 Apache Flink支持流式和批量分析应用程序，如下图所示。 流式分析应用程序有哪些优势？ 与批量分析相比，连续流分析的优势不仅限于因消除定期导入和查询执行而从事件到洞察的低得多的延迟。与批量查询相比，流式查询不必处理输入数据中的人为边界，这些边界是由定期导入和输入的有界性质引起的。 另一方面是更简单的应用程序架构。批处理分析管道由若干独立组件组成，以定期调度数据提取和查询执行。可靠地操作这样的管道并非易事，因为一个组件的故障会影响管道的后续步骤。相比之下，在像Flink这样的复杂流处理器上运行的流分析应用程序包含从数据摄取到连续结果计算的所有步骤。因此，它可以依赖于引擎的故障恢复机制。 Flink如何支持数据分析应用程序？ Flink为连续流式传输和批量分析提供了非常好的支持。具体来说，它具有符合ANSI标准的SQL接口，具有用于批处理和流式查询的统一语义。无论是在记录事件的静态数据集上还是在实时事件流上运行，SQL查询都会计算相同的结果。对用户定义函数的丰富支持可确保在SQL查询中执行自定义代码。如果需要更多的自定义逻辑，Flink的DataStream API或DataSet API提供更多的低级控制。此外，Flink的Gelly库为批量数据集上的大规模和高性能图形分析提供算法和构建块。 什么是典型的数据分析应用程序？ 电信网络的质量监控 分析 移动应用程序中 的产品更新和实验评估 对消费者技术中 的实时数据 进行 特别分析 大规模图分析 数据管道应用 什么是数据管道？ 提取 - 转换 - 加载（ETL）是在存储系统之间转换和移动数据的常用方法。通常会定期触发ETL作业，以将数据从事务数据库系统复制到分析数据库或数据仓库。 数据管道与ETL作业具有相似的用途。它们可以转换和丰富数据，并可以将数据从一个存储系统移动到另一个。但是，它们以连续流模式运行，而不是定期触发。因此，他们能够从连续生成数据的源中读取记录，并以低延迟将其移动到目的地。例如，数据管道可能会监视文件系统目录中的新文件，并将其数据写入事件日志。另一个应用程序可能会将事件流实现到数据库，或者逐步构建和优化搜索索引。 下图描述了定期ETL作业和连续数据管道之间的差异。 数据管道有哪些优势？ 连续数据流水线优于周期性ETL作业的明显优势是减少了将数据移动到目的地的延迟。此外，数据管道更通用，可用于更多用例，因为它们能够连续消耗和发送数据。 Flink如何支持数据管道？ Flink的SQL接口（或表API）可以解决许多常见的数据转换或丰富任务，并支持用户定义的函数。通过使用更通用的DataStream API，可以实现具有更高级要求的数据管道。Flink为各种存储系统（如Kafka，Kinesis，Elasticsearch和JDBC数据库系统）提供了丰富的连接器。它还具有连续的文件系统源，用于监视以时间分区方式写入文件的目录和接收器。 什么是典型的数据管道应用？ 电子商务中 的实时搜索索引构建 电子商务中 持续的ETL","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-usecases.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-usecases.html"},{"title":"Flink入门","text":"一、架构 Apache Flink是一个分布式处理引擎和框架，用于对无界和有界数据流进行状态计算。Flink可在所有常见的集群环境中运行，可以内存级速度和任意规模执行计算。 接下来首先我们阐述Flink的架构。 处理无界和有界数据 任何类型的数据都是作为事件流产生的。信用卡交易，传感器测量，机器日志或网站或移动应用程序上的用户交互，所有这些数据都作为流生成。 数据可以作为 无界 或 有界 流处理。 无界流 有一个开始但没有明确定义的结束。它们不会终止，并且数据在生成时即提供。必须连续处理无界流，即必须在摄取事件后立即处理事件。无法等待所有输入数据到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果完整性。无界流的处理也称为流处理。 有界流 具有明确定义的开始和结束。可以通过在执行任何计算之前先摄取所有数据的方式来处理有界流。处理有界流不需要有序摄取，因为可以始终对有界数据集进行排序。有界流的处理也称为批处理。 Apache Flink擅长处理无界和有界数据集。 精确控制时间和状态使Flink的运行时能够在无界流上运行任何类型的应用程序。有界流由算法和数据结构内部处理，这些算法和数据结构专为固定大小的数据集而设计，从而产生出色的性能。 随处部署应用程序 Apache Flink是一个分布式系统，需要计算资源才能执行应用程序。Flink可与所有常见的集群资源管理器（Resource Manager, 如 Hadoop YARN ， Apache Mesos 和 Kubernetes）集成， 但也可以设置为作为独立集群运行。 Flink旨在很好地运作以上列出的每个资源管理器。这是通过与资源管理器对应的部署模式实现的，这些模式允许Flink以其惯用方式与每个资源管理器进行交互。 部署Flink应用程序时，Flink会根据应用程序配置的并行性自动识别所需资源，并从资源管理器请求它们。如果发生故障，Flink会通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都通过REST调用进行。这简化了Flink在许多环境中的集成。 以任何规模运行应用程序 Flink旨在以任何规模运行有状态流应用程序。应用程序并行化为数千个在集群中分布和同时执行的任务。因此，应用程序可以利用几乎无限量的CPU，主内存，磁盘和网络IO。而且，Flink很容易保持非常大的应用程序状态。其异步和增量检查点算法确保对处理延迟的影响最小，同时保证\"精确一次\"的状态一致性。 用户报告了 在其生产环境中运行的Flink应用程序 令人印象深刻的可扩展性的数字 ，例如： 应用程序 每天 处理 数万亿个事件 ， 应用程序维护 多个TB的状态 ， 应用程序 在数千个内核的运行 。 利用内存中性能 有状态Flink应用程序针对本地状态访问进行了优化。任务状态始终保留在内存中，如果状态大小超过可用内存，则保存在访问高效的磁盘上数据结构中。因此，任务通过访问本地（通常是内存中）状态来执行所有计算，从而产生非常低的处理延迟。Flink通过定期和异步地将本地状态checkpoint到持久存储来保证在出现故障时的\"精确一次\"的状态一致性。 二、应用 Apache Flink是一个用于对无界和有界数据流进行有状态计算的框架。Flink在不同的抽象级别提供多个API，并为常见用例提供专用库。 在这里，我们介绍Flink易于使用和富有表现力的API和库。 流处理应用的构建元素 可以由流处理框架构建和执行的应用程序的类型由框架控制 流 ， 状态 和 时间 的程度来定义。在下文中，我们描述了流处理应用程序的这些构建块，并解释了Flink处理它们的方法。 流 显然，流是流处理的一个基本元素。但是，流可以具有不同的特征，这些特征会影响流的处理方式。Flink是一个多功能的处理框架，可以处理任何类型的流。 有界 和 无界 流：流可以是无界的或有界的，即固定大小的数据集。Flink具有完美支持处理无界流的成熟功能，同时也有专门的算子来有效地处理有界流。 实时 和 记录 流：所有数据都作为流生成。有两种方法可以处理数据。在生成时实时处理它或将流持久保存到存储系统（例如，文件系统或对象存储库），并在以后处理它。Flink应用程序可以处理记录或实时流。 状态 每个有意义的流应用程序都是有状态的，只有对单个事件进行转换的应用才不需要状态。运行基本业务逻辑的任何应用程序都需要记住事件或中间结果，以便在以后的时间点访问它们，例如在收到下一个事件时或在特定持续时间之后。 应用状态是Flink中的\"一等公民\"。可以通过Flink在状态处理环境中提供的所有功能来确认这一点： 多状态基元 ：Flink为不同的数据结构提供状态基元，例如原子值，列表或映射。开发人员可以根据函数的访问模式选择最有效的状态原语。 可插拔状态后端 ：应用程序状态由可插拔状态后端管理和checkpoint。Flink具有不同的状态后端，可以在内存或 RocksDB 中存储状态， RocksDB 是一种高效的嵌入式磁盘数据存储。也可以插入自定义状态后端。 精确一次的状态一致性 ：Flink的checkpoint和恢复算法可确保在发生故障时应用程序状态的一致性。因此，故障是透明处理的，不会影响应用程序的正确性。 非常大的状态 ：由于其异步和增量检查点算法，Flink能够维持几兆兆字节的应用程序状态。 可扩展的应用程序 ：Flink通过将状态重新分配给更多或更少的workers来支持有状态应用程序的伸缩。 时间 时间是流应用程序的另一个重要组成部分，大多数事件流都具有固有的时间语义，因为每个事件都是在特定时间点生成的。此外，许多常见的流计算都基于时间，比如 windows aggregations, sessionization, pattern detection, 以及time-based joins。流处理的一个重要方面是应用程序如何测量时间，即事件时间和处理时间的差异。 Flink提供了一组丰富的与时间相关的功能。 事件时间模式 ：使用事件时间语义处理流的应用程序根据事件的时间戳计算结果。因此，无论是否处理记录的或实时的事件，事件时间处理都允许准确和一致的结果。 水印支持 ：Flink使用水印来推断事件时间应用中的时间。水印也是一种灵活的机制，可以权衡延迟和结果的完整性。 延迟数据处理 ：当使用水印在事件时间模式下处理流时，可能会在所有相关事件到达之前完成计算。这类事件被称为迟发事件。Flink具有多个选项来处理延迟事件，例如通过侧输出重新路由它们并更新以前完成的结果。 处理时间模式 ：除了事件时间模式之外，Flink还支持处理时间语义，该处理时间语义执行由处理机器的挂钟时间触发的计算。处理时间模式适用于具有严格的低延迟要求、可以容忍近似结果的某些应用。 分层API Flink提供三层API。每个API在简洁性和表达性之间提供不同的权衡，并针对不同的用例。 我们简要介绍每个API，讨论其应用，并显示代码示例。 ProcessFunctions ProcessFunctions 是Flink提供的最具表现力的功能接口。Flink提供ProcessFunction来处理来自一个或两个输入流的单个事件，或被分组在一个窗口中的事件。ProcessFunctions提供对时间和状态的细粒度控制。ProcessFunction可以任意修改其状态并注册将在未来触发回调函数的定时器。因此，ProcessFunctions可以根据许多 有状态事件驱动的应用的 需要实现复杂的每事件业务逻辑。 以下显示了一个 KeyedProcessFunction 对一个 KeyedStream 进行操作，匹配 START 以及 END 事件的示例。当一个 START 事件被接收，则该函数在状态中记住其时间戳并且注册一个4小时的计时器。如果在计时器触发之前收到 END 事件，则该函数计算事件 END 和 START 事件之间的持续时间，清除状态并返回值。否则，计时器只会超时并清除状态。 /** * Matches keyed START and END events and computes the difference between * both elements' timestamps. The first String field is the key attribute, * the second String attribute marks START and END events. */ public static class StartEndDuration extends KeyedProcessFunction < String , Tuple2 < String , String >, Tuple2 < String , Long >> { private ValueState < Long > startTime ; @Override public void open ( Configuration conf ) { // obtain state handle startTime = getRuntimeContext () . getState ( new ValueStateDescriptor < Long >( \"startTime\" , Long . class )); } /** Called for each processed event. */ @Override public void processElement ( Tuple2 < String , String > in , Context ctx , Collector < Tuple2 < String , Long >> out ) throws Exception { switch ( in . f1 ) { case \"START\" : // set the start time if we receive a start event. startTime . update ( ctx . timestamp ()); // register a timer in four hours from the start event. ctx . timerService () . registerEventTimeTimer ( ctx . timestamp () + 4 * 60 * 60 * 1000 ); break ; case \"END\" : // emit the duration between start and end event Long sTime = startTime . value (); if ( sTime != null ) { out . collect ( Tuple2 . of ( in . f0 , ctx . timestamp () - sTime )); // clear the state startTime . clear (); } default : // do nothing } } /** Called when a timer fires. */ @Override public void onTimer ( long timestamp , OnTimerContext ctx , Collector < Tuple2 < String , Long >> out ) { // Timeout interval exceeded. Cleaning up the state. startTime . clear (); } } 这个例子说明了 KeyedProcessFunction 的表现力，但也强调了它是一个相当冗长的接口。 The DataStream API DataStream API 为诸如窗口等许多常见的流处理操作提供原语。DataStream API可用于Java和Scala，基于如 map() ， reduce() 和 aggregate() 等方法。可以通过扩展接口，或像Java或Scala中lambda函数一样来定义函数。 以下示例显示如何对点击流进行会话化并计算每个会话的点击次数。 // a stream of website clicks DataStream < Click > clicks = ... DataStream < Tuple2 < String , Long >> result = clicks // project clicks to userId and add a 1 for counting . map ( // define function by implementing the MapFunction interface. new MapFunction < Click , Tuple2 < String , Long >>() { @Override public Tuple2 < String , Long > map ( Click click ) { return Tuple2 . of ( click . userId , 1L ); } }) // key by userId (field 0) . keyBy ( 0 ) // define session window with 30 minute gap . window ( EventTimeSessionWindows . withGap ( Time . minutes ( 30L ))) // count clicks per session. Define function as lambda function. . reduce (( a , b ) -> Tuple2 . of ( a . f0 , a . f1 + b . f1 )); SQL & Table API Flink具有两个关系型API， Table API和SQL 。这两个API都是用于批处理和流处理的统一API，即，在无界的实时流或有界的记录流上以相同的语义执行查询，并产生相同的结果。Table API和SQL利用 Apache Calcite 进行解析，验证和查询优化。它们可以与DataStream和DataSet API无缝集成，并支持用户定义的标量，聚合和表值函数。 Flink的关系型API旨在简化 数据分析 ， 数据pipelining和ETL应用 的定义。 以下示例显示用于会话化点击流并计算每个会话的点击次数的SQL查询。这与DataStream API示例中的用例相同。 SELECT userId , COUNT ( * ) FROM clicks GROUP BY SESSION ( clicktime , INTERVAL '30' MINUTE ), userId 库 Flink具有几个用于常见数据处理用例的库。这些库通常嵌入在API中，而不是完全独立的。因此，他们可以从API的所有功能中受益，并与其他库集成。 复杂事件处理（CEP） ：模式检测是事件流处理的一个非常常见的用例。Flink的CEP库提供了一个API来指定事件的模式（想想正则表达式或状态机）。CEP库与Flink的DataStream API集成，以便在DataStream上评估模式。CEP库的应用包括网络入侵检测，业务流程监控和欺诈检测。 DataSet API ：DataSet API是Flink用于批处理应用程序的核心API。DataSet API的原语包括 map ， reduce ， （外部）join ， co-group 和 iterate 。所有操作都由算法和数据结构支持，这些算法和数据结构对内存中的序列化数据进行操作，并在数据大小超过内存预算时溢出到磁盘。Flink的DataSet API的数据处理算法是受传统数据库算子的启发，例如混合散列连接或外部合并排序。 Gelly ：Gelly是一个可扩展的图形处理和分析库。Gelly在DataSet API之上实现并与之集成。因此，它受益于其可扩展且强大的算子。Gelly具有 内置算法 ，例如标签传播，三角形枚举和页面排名，但也提供了一种 Graph API 从而简化自定义图算法的实现。 三、运行 由于许多流应用程序旨在以最短的停机时间连续运行，因此流处理器必须提供出色的故障恢复，以及在应用程序运行时监视和维护应用程序的工具。 Apache Flink非常关注流处理的运维方面。在这里，我们将解释Flink的故障恢复机制，并介绍其管理和监督正在运行的应用程序的功能。 全天候运行您的应用程序 机器和过程故障在分布式系统中无处不在。像Flink这样的分布式流处理器必须从故障中恢复，以便能够24/7全天候运行流应用程序。显然，这不仅意味着在故障后重新启动应用程序，而且还要确保其内部状态保持一致，以便应用程序可以继续处理，就像从未发生过故障一样。 Flink提供了多种功能，以确保应用程序保持运行并保持一致： 一致的检查点 ：Flink的恢复机制基于应用程序状态的一致检查点。如果发生故障，将重新启动应用程序并从最新检查点加载其状态。结合可重置流源，此功能可以保证 精确一次的状态一致性 。 高效检查点 ：如果应用程序保持TB级状态，则checkpoint应用程序的状态可能非常昂贵。Flink可以执行异步和增量检查点，以便将检查点对应用程序的延迟SLA的影响保持在非常小的水平。 端到端精确一次 ：Flink为特定存储系统提供事务接收器，保证数据只写出一次，即使出现故障。 与集群管理器集成 ：Flink与集群管理器紧密集成，例如 Hadoop YARN ， Mesos 或 Kubernetes 。当进程失败时，将自动启动一个新进程来接管其工作。 高可用性设置 ：Flink具有高可用性模式，可消除所有单点故障。HA模式基于 Apache ZooKeeper ，这是一种经过验证的可靠分布式协调服务。 更新，迁移，暂停和恢复您的应用程序 需要维护为关键业务服务提供支持的流应用程序。需要修复错误，并且需要实现改进或新功能。但是，更新有状态流应用程序并非易事。通常，人们不能简单地停止应用程序并重新启动固定版本或改进版本，因为人们无法承受丢失应用程序的状态。 Flink的 Savepoints 是一个独特而强大的功能，可以解决更新有状态应用程序和许多其他相关挑战的问题。保存点是应用程序状态的一致快照，因此与检查点非常相似。但是，与检查点不同，保存点需要手动触发，并且在应用程序停止时不会自动删除保存点。保存点可用于启动状态兼容的应用程序并初始化其状态。保存点具有以下功能： 应用程序演变 ：保存点可用于演进应用程序。一个应用程序的固定或改进版本可以从先前版本的保存点重新启动。也可以从较早的时间点（假设存在这样的保存点）启动应用程序，以修复由有缺陷的版本产生的错误结果。 群集迁移 ：使用保存点，可以将应用程序迁移（或克隆）到不同的群集。 Flink版本更新 ：可以使用保存点迁移应用程序以在新的Flink版本上运行。 应用程序扩展 ：保存点可用于增加或减少应用程序的并行性。 A / B测试和假设情景 ：可以通过启动同一保存点的所有版本来比较两个（或更多）不同版本的应用程序的性能或质量。 暂停和恢复 ：可以通过获取保存点并停止应用程序来暂停应用程序。在以后的任何时间点，都可以从保存点恢复应用程序。 存档 ：可以存档，以便能够将应用程序的状态重置为较早的时间点。 监控您的应用程序 与任何其他服务一样，需要对连续运行的流应用程序进行监督，并将其集成到组织的运营基础架构（即监控和日志记录服务）中。监控有助于预测问题并提前做出反应。通过日志记录可以进行根因分析调查失败。最后，通过一个易于访问的接口来控制运行应用程序的是一个重要特性。 Flink与许多常见的日志记录和监视服务很好地集成，并提供REST API来控制应用程序和查询信息。 Web UI ：Flink具有Web UI，可以检查，监视和调试正在运行的应用程序。它还可用于提交执行或取消执行。 日志记录 ：Flink实现了流行的slf4j日志记录界面，并与日志框架 log4j 或 logback 集成。 指标 ：Flink具有复杂的指标系统，可收集和报告系统和用户定义的指标。指标可以导出到多个报告器，包括 JMX ，Ganglia， Graphite ， Prometheus ， StatsD ， Datadog 和 Slf4j 。 REST API ：Flink公开REST API以提交新应用程序、生成正在运行的应用程序的保存点或取消应用程序。REST API还公开元数据和收集运行或已完成应用程序的指标。","tags":"Big Data","url":"https://jiang-hao.com/articles/2019/big-data-flink-intro.html","loc":"https://jiang-hao.com/articles/2019/big-data-flink-intro.html"},{"title":"OpenShift 详细教程 - 基础知识入门","text":"简介 自由和开放源码的云计算平台使开发人员能够创建、测试和运行他们的应用程序，并且可以把它们部署到云中。 OpenShift是红帽的云开发平台即服务（PaaS）。是一个基于主流的容器技术Docker和K8s构建的开源容器云平台。底层以Docker作为容器引擎驱动，以K8s作为容器编排引擎组件，并提供了开发语言，中间件，DevOps自动化流程工具和web console用户界面等元素，提供了一套完整的基于容器的应用云平台。 Openshift提供比任何PaaS更多的灵活性，它支持用于Java、Python、PHP、Perl、node.js、go和Ruby的更多的开发框架，包括 Spring、Seam、Weld、CDI、Rails、Rack、Symfony、Zend Framework、Twisted、Django和Java E。数据库语言则支持MySQL、MongoDB和PostgreSQL。 另外它还提供了多种集成开发工具如Eclipse integration，JBoss Developer Studio和 Jenkins等。 OpenShift Online服务构建在Red Hat Enterprise Linux上。Red Hat Enterprise Linux提供集成应用程序，运行库和一个配置可伸缩的多用户单实例的操作系统，以满足企业级应用的各种需求。 建立在红帽开源领导地位基础上的OpenShift旨在终结PaaS的厂商锁定，使用户可以选择自 己应用运行在哪个云提供商的云中。OpenShift将作为在线服务来提供。 OpenShift使用的架构由单个节点组成，以容纳应用程序代码和服务，同时还有一系列的单独代理来管理节点和提供服务。除此之外，OpenShift的架构还包括一个消息系统将节点和代理绑定到一起，并且使用 RESTful 的API同外部工具整合。 Openshift包括社区版和企业版： 社区版： Openshift Origin 企业版： Openshift Online/Openshift Enterprise 重要概念 system:admin为默认的集群管理员，拥有最高的权限。该用户没有密码，登陆依赖于证书密钥。 Service Account 是 Openshift 中专门供程序和组件使用的账号。不同的用户或组关联不同的角色，同时关联不同的SCC（security context constriant）安全上下文。 总体架构 自底而上包括几个层次：基础架构层，容器引擎层，容器编排层，PaaS服务层，界面及工具层。 基础架构层：为Openshift平台的运行提供基础的运行环境。Openshift支持运行在物理机，虚拟机（kvm,vmware,virtual box等），公有云（阿里云，AWS等），私有云，混合云上。 容器引擎层：以当前主流的Docker作为容器引擎。 容器编排层：以Google的k8s进行容器编排。 PaaS服务层：容器云平台的最终目的是为上层应用服务提供支持，提高开发，测试，部署，运维的速度和效率。用户在Openshift云平台上可以快速的获取和部署一个数据库，缓存等。 界面及工具层：Openshift提供了多种用户的接入渠道：Web控制台，命令行，RestFul接口等。 核心组件 Master节点：主控节点 集群内的管理组件都运行在Master节点上。Master节点负责集群的配置管理，维护集群的状态。 Master节点运行的服务组件： API Server：负责提供Web console和RESTful API。集群内所有节点都会访问API Server，更新节点的状态及其上的容器状态。 数据源（Data store）：集群内所有状态信息都会存储在后端的一个etcd的分布式数据库中。 调度控制器（Scheduler）：负责按用户输入的要求寻找合适的计算节点。 复制控制器（Replication Controller）：负责监控当前容器实例的数量和用户部署指定的数量是否匹配，若有容器异常退出，复制控制器发现实际数少于部署定义数，从而触发部署新的实例。 Node节点：计算节点 接收Master节点的指令，运行和维护Docker容器。Master节点也可以是Node节点，只是在一般环境中，其运行容器的功能是关闭的。 Project 在k8s中使用命名空间来分隔资源。同一个命名空间中，某一个对象的名称在其分类中必须唯一，但在不同命名空间中的对象则可以同名。Openshift集成了k8s命名空间的概念，而且在其上定义了Project对象的概念， 每一个Project会和一个namespace相关联 。 Pod 在Openshift中的容器都会Pod包裹，即容器都运行在Pod内部， 一个Pod可以运行一个或多个容器，绝大多少情况下，一个Pod内部运行一个容器 。 Service 由于容器是一个非持久化的对象，所有对容器的修改在容器销毁后都会丢失，而且每个容器的IP地址会不断变化。k8s提供了Service组件，当部署某个应用时，会创建一个Service对象，该对象与一个或多个Pod关联，同时每个Service分配一个相对恒定的IP，通过访问该IP及相应的端口，请求就会转发到对应Pod端口。除了可通过IP，也可以通过域名访问Service，格式为：..svc.cluster.local Router和Route Service提供了一个通往后端Pod集群的稳定入口，但是Service的IP地址只是集群内部的节点和容器可见。外部需通过Router（路由器）来转发。Router组件是Openshift集群中一个重要的组件，它是外界访问集群内容器应用的入口。用户可以创建Route（路由规则）对象，一个Route会与一个Service关联，并绑定一个域名。Route规则被Router加载。当集群外部的请求通过指定域名访问应用时，域名被解析并指向Router所在的计算机节点上，Router获取该请求，然后根据Route规则定义转发给与这个域名对应的Service后端所关联的Pod容器实例。上述转发流程类似于nginx。Router负责将集群外的请求转发到集群的容器，Service则负责把来自集群内部的请求转发到指定的容器中。 Persistent Storage 容器默认是非持久化的，所有的修改在容器销毁时都会丢失。Docker提供了持久化卷挂载的能力，Openshift除了提供持久化卷挂载的能力，还提供了一种持久化供给模型即PV（Persistent Volume）和PVC（Persistent Volume Claim）。在PV和PVC模型中，集群管理员会创建大量不同大小和不同特性的PV。用户在部署应用时显式的声明对持久化的需求，创建PVC，在PVC中定义所需要的存储大小，访问方式。Openshift集群会自动寻找符合要求的PV与PVC自动对接。 Registry Openshift内部的镜像仓库，主要用于存放内置的S2I构建流程所产生的镜像。 S2I Source to Image，负责将应用源码构建成镜像。步骤： 1）用户输入源代码仓库的地址 2）选择S2I构建的基础镜像 3）触发构建 4）S2I构建执行器从指定的源码仓库地址下载代码 5）S2I构建执行器实例化Builder镜像，并将代码注入到Builder镜像 6）S2I构建执行器按照预定义的逻辑执行源代码的编译，构建 7）生成新的镜像 8）S2I构建执行器将新镜像Push到Registry 9）更新相关的Image Stream信息 核心流程 1）创建应用：用户通过web控制台或oc命令创建应用，Openshift平台根据用户输入的源码地址和Builder镜像，生成构建配置Builder config和部署配置Deployment config，Service，Route等。 2）触发构建 3）实例化构建：平台根据Builder config实例化Builder对象，下载代码，并将代码注入到Builder对象，执行编译，构建 4）生成新镜像并Push到Registry 5）更新相关的Image Stream信息 6）触发部署：当Image Stream更新后，触发平台部署镜像 7）实例化镜像部署：平台根据Deployment config实例化部署，生成Deploy对象 8）生成Replication Controller 9）部署容器：通过Replication Controller，平台将pod及容器部署到各个节点上 10）用户访问：用户通过浏览器访问Route对象中定义的应用域名 11）请求处理并返回：请求到达Router组件后，通过Route转发给相关联的Service，最终到对应的容器实例。 优点 支持快速部署，实现敏捷开发。 提供动态伸缩功能，将过程简化至只需更改一个值。 管理资源，为容器分配合适的资源，提高资源利用率。 有对应的平台自动化运维工具，大大减少运维负担。 在大规模集群时提供方便高效的管理方法。 有完善的结构，部署以后能快速地测试应用。 丰富的接口，提供给各种插件与二次开发使用 上手难度：是基于docker和k8s的开源项目，有丰富的社区技术支持。还有关于openshift中文参考书。","tags":"Cloud","url":"https://jiang-hao.com/articles/2019/cloud-openshift-basics.html","loc":"https://jiang-hao.com/articles/2019/cloud-openshift-basics.html"},{"title":"详解Java中的final关键字","text":"final 简介 1 final 关键字可用于多个场景，且在不同场景具有不同的作用。首先， final 是一个 非访问修饰符 ， 仅 适用 于变量，方法或类 。下面是使用final的不同场景： 上面这张图可以概括成： 当 final 修饰 变量 时，被修饰的变量必须被初始化(赋值)，且后续不能修改其值，实质上是常量； 当 final 修饰 方法 时，被修饰的方法无法被所在类的子类重写（覆写）； 当 final 修饰 类 时，被修饰的类不能被继承，并且 final 类中的所有成员方法都会被隐式地指定为 final 方法，但成员变量则不会变。 final 修饰变量 当使用 final 关键字声明类成员变量或局部变量后，其值不能被再次修改；也经常和 static 关键字一起，作为 类常量 使用。很多时候会容易把 static 和 final 关键字混淆， static 作用于成员变量用来表示只保存一份副本，而 final 的作用是用来保证变量不可变 。如果 final 变量是引用，这意味着该变量不能重新绑定到引用另一个对象，但是可以更改该引用变量指向的对象的内部状态，即可以从 final 数组 或 final 集合中添加或删除元素。最好用全部大写来表示 final 变量，使用下划线来分隔单词。 例子 ： //一个final成员常量 final int THRESHOLD = 5 ; //一个空的final成员常量 final int THRESHOLD ; //一个静态final类常量 static final double PI = 3.141592653589793 ; //一个空的静态final类常量 static final double PI ; 初始化final变量 ： 我们必须初始化一个 final 变量，否则编译器将抛出编译时错误。 final 变量只能通过 初始化器 或赋值语句初始化一次。初始化 final 变量有三种方法： 可以在声明它时初始化 final 变量。这种方法是最常见的。如果在声明时 未 初始化，则该变量称为 空 final 变量 。下面是初始化空 final 变量的两种方法。 可以在 instance-initializer块 或内部构造函数中 初始化 空的 final 变量。如果您的类中有多个构造函数，则必须在所有构造函数中初始化它，否则将抛出编译时错误。 可以在 静态块 内初始化空的 final 静态变量。 这里注意有一个很普遍的误区。 很多人会认为static修饰的final常量必须在声明时就进行初始化，否则会报错。但其实则不然，我们可以先使用 static final 关键字声明一个类常量，然后再在 静态块 内初始化空的 final 静态变量。 让我们通过一个例子看上面初始化 final 变量的不同方法。 // Java program to demonstrate different // ways of initializing a final variable class Gfg { // a final variable direct initialize // 直接赋值 final int THRESHOLD = 5 ; // a blank final variable // 空final变量 final int CAPACITY ; // another blank final variable final int MINIMUM ; // a final static variable PI direct initialize // 直接赋值的静态final变量 static final double PI = 3.141592653589793 ; // a blank final static variable // 空的静态final变量，此处并不会报错，因为在下方的静态代码块内对其进行了初始化 static final double EULERCONSTANT ; // instance initializer block for initializing CAPACITY // 用来赋值空final变量的实例初始化块 { CAPACITY = 25 ; } // static initializer block for initializing EULERCONSTANT // 用来赋值空final变量的静态初始化块 static { EULERCONSTANT = 2.3 ; } // constructor for initializing MINIMUM // Note that if there are more than one // constructor, you must initialize MINIMUM // in them also // 构造函数内初始化空final变量；注意如果有多个 // 构造函数时，必须在每个中都初始化该final变量 public GFG () { MINIMUM = - 1 ; } } 何时使用 final 变量： 普通变量和 final 变量之间的唯一区别是我们可以将值重新赋值给普通变量；但是对于 final 变量，一旦赋值，我们就不能改变 final 变量的值。因此， final 变量必须仅用于我们希望在整个程序执行期间保持不变的值。 final 引用变量： 当 final 变量是对象的引用时，则此变量称为 final 引用变量。例如， final 的 StringBuffer 变量： final StringBuffer sb ; final 变量无法重新赋值。但是对于 final 的引用变量，可以更改该引用变量指向的对象的内部状态。请注意，这不是重新赋值。 final的 这个属性称为 非传递性 。要了解对象内部状态的含义，请参阅下面的示例： // Java program to demonstrate // reference final variable class Gfg { public static void main ( String [] args ) { // a final reference variable sb final StringBuilder sb = new StringBuilder ( \"Geeks\" ); System . out . println ( sb ); // changing internal state of object // reference by final reference variable sb // 更改final变量sb引用的对象的内部状态 sb . append ( \"ForGeeks\" ); System . out . println ( sb ); } } 输出： Geeks GeeksForGeeks 非传递 属性也适用于数组，因为在Java中 数组也是对象 。带有 final 关键字的数组也称为 final 数组 。 注意 ： 如上所述， final 变量不能重新赋值，这样做会抛出编译时错误。 // Java program to demonstrate re-assigning // final variable will throw compile-time error class Gfg { static final int CAPACITY = 4 ; public static void main ( String args []) { // re-assigning final variable // will throw compile-time error CAPACITY = 5 ; } } 输出： Compiler Error: cannot assign a value to final variable CAPACITY 当在方法/构造函数/块中创建 final 变量时，它被称为局部 final 变量，并且必须在创建它的位置初始化一次。参见下面的局部 final 变量程序： // Java program to demonstrate // local final variable // The following program compiles and runs fine class Gfg { public static void main ( String args []) { // local final variable final int i ; i = 20 ; System . out . println ( i ); } } 输出： 20 注意C ++ const 变量和Java final 变量之间的区别。声明时，必须为C ++中的const变量赋值。对于Java中的 final 变量，正如我们在上面的示例中所看到的那样，可以稍后赋值，但只能赋值一次。 final 在 foreach循环 中：在foreach语句中使用 final 声明存储循环元素的变量是合法的。 // Java program to demonstrate final // with for-each statement class Gfg { public static void main ( String [] args ) { int arr [] = { 1 , 2 , 3 }; // final with for-each statement // legal statement for ( final int i : arr ) System . out . print ( i + \" \" ); } } 输出： 1 2 3 说明： 由于i变量在循环的每次迭代时超出范围，因此实际上每次迭代都重新声明，允许使用相同的标记（即i）来表示多个变量。 final 修饰类 当使用 final 关键字声明一个类时，它被称为 final 类。被声明为 final 的类不能被扩展（继承）。 final 类有两种用途： 一个是彻底防止被 继承 ，因为 final 类不能被扩展。例如，所有 包装类 如 Integer ， Float 等都是 final 类。我们无法扩展它们。 final 类的另一个用途是 创建一个 类似于 String 类的不可变类。只有将一个类定义成为 final 类，才能使其不可变。 final class A { // methods and fields } // 下面的这个类B想要扩展类A是非法的 class B extends A { // COMPILE-ERROR! Can't subclass A } Java支持把class定义成 final ，似乎违背了面向对象编程的基本原则，但在另一方面，封闭的类也保证了该类的所有方法都是固定不变的，不会有子类的覆盖方法需要去动态加载。这给编译器做优化时提供了更多的可能，最好的例子是String，它就是 final 类，Java编译器就可以把字符串常量（那些包含在双引号中的内容）直接变成String对象，同时对运算符\"+\"的操作直接优化成新的常量，因为final修饰保证了不会有子类对拼接操作返回不同的值。 对于所有不同的类定义一顶层类(全局或包可见)、嵌套类(内部类或静态嵌套类)都可以用final来修饰。但是一般来说final多用来修饰在被定义成全局(public)的类上，因为对于非全局类，访问修饰符已经将他们限制了它们的也可见性，想要继承这些类已经很困难，就不用再加一层final限制。 final 与匿名内部类 匿名类(Anonymous Class)虽然说同样不能被继承，但它们并没有被编译器限制成final。另外要提到的是，网上有许多地方都说因为使用内部类，会有两个地方必须需要使用 final 修饰符： 在内部类的方法使用到方法中定义的局部变量，则该局部变量需要添加 final 修饰符 在内部类的方法形参使用到外部传过来的变量，则形参需要添加 final 修饰符 原因大多是说当我们创建匿名内部类的那个方法调用运行完毕之后，因为局部变量的生命周期和方法的生命周期是一样的，当方法弹栈， 这个局部变量就会消亡了，但内部类对象可能还存在。 此时就会出现一种情况，就是我们调用这个内部类对象去访问一个不存在的局部变量，就可能会出现空指针异常。而此时需要使用 final 在类加载的时候进入常量池，即使方法弹栈，常量池的常量还在，也可以继续使用，JVM 会持续维护这个引用在回调方法中的生命周期。 但是 JDK 1.8 取消了对匿名内部类引用的局部变量 final 修饰的检查 对此， theonlin 专门通过实验做出了总结：其实局部内部类并不是直接调用方法传进来的参数，而是内部类将传进来的参数通过自己的构造器备份到了自己的内部，自己内部的方法调用的实际是自己的属性而不是外部类方法的参数。外部类中的方法中的变量或参数只是方法的局部变量，这些变量或参数的作用域只在这个方法内部有效，所以方法中被 final 的变量的仅仅作用是表明这个变量将作为内部类构造器参数， 其实 final 不加也可以，加了可能还会占用内存空间，影响 GC 。最后结论就是，需要使用 final 去持续维护这个引用在回调方法中的生命周期这种说法应该是错误的，也没必要。 final 修饰方法 下面这段话摘自《Java编程思想》第四版第143页： 使用 final 方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。 当使用 final 关键字声明方法时，它被称为 final 方法。 final 方法无法被 覆盖 （重写）。比如 Object类 ，它的一些方法就被声明成为了 final 。如果你认为一个方法的功能已经足够完整了，子类中不需要改变的话，你可以声明此方法为 final 。以下代码片段说明了用 final 关键字修饰方法： class A { // 父类的ml方法被使用了final关键字修饰 final void m1 () { System . out . println ( \"This is a final method.\" ); } } class B extends A { // 此处会报错，子类B尝试重写父类A的被final修饰的ml方法 @override void m1 () { // COMPILE-ERROR! Can't override. System . out . println ( \"Illegal!\" ); } } 而关于高效，是因为在java早期实现中，如果将一个方法指明为final，就是同意编译器将针对该方法的调用都转化为内嵌调用（内联）。大概就是，如果是内嵌调用，虚拟机不再执行正常的方法调用（参数压栈，跳转到方法处执行，再调回，处理栈参数，处理返回值），而是直接将方法展开，以方法体中的实际代码替代原来的方法调用。这样减少了方法调用的开销。所以有一些程序员认为： 除非有足够的理由使用多态性，否则应该将所有的方法都用 final 修饰。这样的认识未免有些偏激 ，因为在最近的java设计中，虚拟机（特别是hotspot技术）可以自己去根据具体情况自动优化选择是否进行内联，只不过使用了 final 关键字的话可以显示地影响编译器对被修饰的代码进行内联优化。所以请切记，对于Java虚拟机来说编译器在编译期间会自动进行内联优化，这是由编译器决定的，对于开发人员来说，一定要设计好时空复杂度的平衡，不要滥用final。 注1：类的 private 方法会隐式地被指定为 final 方法，也就同样无法被重写。可以对private方法添加final修饰符，但并没有添加任何额外意义。 注2：在java中，你永远不会看到同时使用 final 和 abstract 关键字声明的类或方法。对于类， final 用于防止 继承 ，而抽象类反而需要依赖于它们的子类来完成实现。在修饰方法时， final 用于防止被 覆盖 ，而抽象方法反而需要在子类中被重写。 有关 final 方法和 final 类的更多示例和行为 ，请参阅 使用final继承 。 final 优化编码的艺术 final 关键字在效率上的作用主要可以总结为以下三点： 缓存： final 配合 static 关键字提高了代码性能，JVM和Java应用都会缓存 final 变量。 同步： final 变量或对象是只读的，可以安全的在多线程环境下进行共享，而不需要额外的同步开销。 内联：使用 final 关键字，JVM会 显式地 主动对方法、变量及类进行内联优化。 更多关于 final 关键字对代码的优化总结以及注意点可以参考IBM的 《Is that your final answer?》 这篇文章。 本文由笔者参考多篇博文汇总作成，因数量众多不一一列出，主体部分从GeeksforGeeks网站翻译，实际由 Gaurav Miglani 撰写。如果您发现任何不正确的内容，或者您想要分享有关上述主题的更多信息，请撰写评论。 ↩","tags":"Backend","url":"https://jiang-hao.com/articles/2019/backend-java-final-keyword.html","loc":"https://jiang-hao.com/articles/2019/backend-java-final-keyword.html"},{"title":"一种通过网盘实现多终端桌面同步和云化存储的方法","text":"简介 方法其实很简单，而且很多人可能已经想到或者在用了。一言以蔽之，就是把桌面文件夹设置为网盘的同步文件夹。之所以要这么做，因为很多人习惯使用一些程序搜索工具来启动程序（比如wox, everything, 甚至系统自带的搜索功能等等），不在系统桌面保留桌面图标，而且通常为了平时工作方便，会直接把桌面当做文档库或者workshop来用，也就是把平时常用的一些文档直接保存在桌面。这个时候如果直接用网盘把桌面设置为同步文件夹，就可以直接实时备份桌面上的文件了。而许多人可能平时会用到诸多不同设备（比如工作场所的电脑，私人电脑，手机，ipad等等），这种方法就可以实现多个电脑之间桌面的实时文件共享，同时其他移动端设备也能实时进行同步了。平时工作或者收藏的文件或者个人材料，在外临时要用的时候随时随处都可以拿到，而不用刻意事先进行上传或者分享。 效果 以下是大致效果: 工具 程序启动 如果您不喜欢应用程序图标，并且只希望在桌面上放置文件和文件夹（如图所示），但仍然希望能够以一种方便的方式启动应用程序，则可以删除所有图标，然后安装 WOX ，用于快速访问应用程序。 网盘同步 这里的网盘可以使用MEGASync或者坚果云，可以在多个设备之间同步桌面，并可以通过网页或者客户端随时随地访问文档。 MEGASync 提供以下功能： 具有50GB免费存储空间的安全云网络磁盘 通过端到端加密进行快速传输 多个设备之间的文件夹同步 如我们所见，MEGASync的一个非常有用的功能是在多个设备之间同步文件夹。 但是，当我们使用它将桌面放在云中时，它将变得更加有趣。 配置 配置非常简单，安装软件后设置MEGASync同步文件夹时只需将桌面目录设置为同步文件夹即可。","tags":"Tools","url":"https://jiang-hao.com/articles/2019/tools-Cloud-Desktop-Based-On-MegaSync.html","loc":"https://jiang-hao.com/articles/2019/tools-Cloud-Desktop-Based-On-MegaSync.html"},{"title":"A Java TFTP Server","text":"简介 一个完全多线程的tftp服务器。可以同时处理多个客户端。实现 RFC 1350 并包装块号以获得大文件支持。 通过判断监听到的请求tftpPacket_的类型是TFTPReadRequestPacket（读）或者TFTPWriteRequestPacket（写），将其对应交由handleRead()或者handleWrite()方法处理。 要启动，只需创建该类的实例。如果服务器由于正在使用的端口，端口被拒绝等原因而无法启动，则将抛出IOException。 要停止，请使用shutdown方法。 要检查服务器是否仍在运行（或者由于错误而停止），请调用isRunning方法。 默认情况下，事件不会记录到stdout/stderr。可以使用setLog和setLogError方法更改此设置。 示例用法如下： public static void main ( String [] args ) throws Exception { if ( args . length != 1 ) { System . out . println ( \"You must provide 1 argument - the base path for the server to serve from.\" ); System . exit ( 1 ); } TFTPServer ts = new TFTPServer ( new File ( args [ 0 ]), new File ( args [ 0 ]), GET_AND_PUT ); ts . setSocketTimeout ( 2000 ); System . out . println ( \"TFTP Server running. Press enter to stop.\" ); new InputStreamReader ( System . in ). read (); ts . shutdown (); System . out . println ( \"Server shut down.\" ); System . exit ( 0 ); } 代码 /* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the \"License\"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.commons.net.tftp ; import java.io.BufferedInputStream ; import java.io.BufferedOutputStream ; import java.io.File ; import java.io.FileInputStream ; import java.io.FileNotFoundException ; import java.io.FileOutputStream ; import java.io.IOException ; import java.io.InputStream ; import java.io.OutputStream ; import java.io.PrintStream ; import java.net.InetAddress ; import java.net.NetworkInterface ; import java.net.SocketTimeoutException ; import java.util.HashSet ; import java.util.Enumeration ; import java.util.Iterator ; import org.apache.commons.net.io.FromNetASCIIOutputStream ; import org.apache.commons.net.io.ToNetASCIIInputStream ; /** * A fully multi-threaded tftp server. Can handle multiple clients at the same time. Implements RFC * 1350 and wrapping block numbers for large file support. * * To launch, just create an instance of the class. An IOException will be thrown if the server * fails to start for reasons such as port in use, port denied, etc. * * To stop, use the shutdown method. * * To check to see if the server is still running (or if it stopped because of an error), call the * isRunning() method. * * By default, events are not logged to stdout/stderr. This can be changed with the * setLog and setLogError methods. * * <p> * Example usage is below: * * <code> * public static void main(String[] args) throws Exception * { * if (args.length != 1) * { * System.out * .println(\"You must provide 1 argument - the base path for the server to serve from.\"); * System.exit(1); * } * * TFTPServer ts = new TFTPServer(new File(args[0]), new File(args[0]), GET_AND_PUT); * ts.setSocketTimeout(2000); * * System.out.println(\"TFTP Server running. Press enter to stop.\"); * new InputStreamReader(System.in).read(); * * ts.shutdown(); * System.out.println(\"Server shut down.\"); * System.exit(0); * } * * </code> * * @since 2.0 */ public class TFTPServer implements Runnable { private static final int DEFAULT_TFTP_PORT = 69 ; public static enum ServerMode { GET_ONLY , PUT_ONLY , GET_AND_PUT ; } private final HashSet < TFTPTransfer > transfers_ = new HashSet < TFTPTransfer >(); private volatile boolean shutdownServer = false ; private TFTP serverTftp_ ; private File serverReadDirectory_ ; private File serverWriteDirectory_ ; private final int port_ ; private final InetAddress laddr_ ; private Exception serverException = null ; private final ServerMode mode_ ; /* /dev/null output stream (default) */ private static final PrintStream nullStream = new PrintStream ( new OutputStream () { @Override public void write ( int b ){} @Override public void write ( byte [] b ) throws IOException {} } ); // don't have access to a logger api, so we will log to these streams, which // by default are set to a no-op logger private PrintStream log_ ; private PrintStream logError_ ; private int maxTimeoutRetries_ = 3 ; private int socketTimeout_ ; private Thread serverThread ; /** * Start a TFTP Server on the default port (69). Gets and Puts occur in the specified * directories. * * The server will start in another thread, allowing this constructor to return immediately. * * If a get or a put comes in with a relative path that tries to get outside of the * serverDirectory, then the get or put will be denied. * * GET_ONLY mode only allows gets, PUT_ONLY mode only allows puts, and GET_AND_PUT allows both. * Modes are defined as int constants in this class. * * @param serverReadDirectory directory for GET requests * @param serverWriteDirectory directory for PUT requests * @param mode A value as specified above. * @throws IOException if the server directory is invalid or does not exist. */ public TFTPServer ( File serverReadDirectory , File serverWriteDirectory , ServerMode mode ) throws IOException { this ( serverReadDirectory , serverWriteDirectory , DEFAULT_TFTP_PORT , mode , null , null ); } /** * Start a TFTP Server on the specified port. Gets and Puts occur in the specified directory. * * The server will start in another thread, allowing this constructor to return immediately. * * If a get or a put comes in with a relative path that tries to get outside of the * serverDirectory, then the get or put will be denied. * * GET_ONLY mode only allows gets, PUT_ONLY mode only allows puts, and GET_AND_PUT allows both. * Modes are defined as int constants in this class. * * @param serverReadDirectory directory for GET requests * @param serverWriteDirectory directory for PUT requests * @param port the port to use * @param mode A value as specified above. * @param log Stream to write log message to. If not provided, uses System.out * @param errorLog Stream to write error messages to. If not provided, uses System.err. * @throws IOException if the server directory is invalid or does not exist. */ public TFTPServer ( File serverReadDirectory , File serverWriteDirectory , int port , ServerMode mode , PrintStream log , PrintStream errorLog ) throws IOException { port_ = port ; mode_ = mode ; log_ = ( log == null ? nullStream : log ); logError_ = ( errorLog == null ? nullStream : errorLog ); laddr_ = null ; launch ( serverReadDirectory , serverWriteDirectory ); } /** * Start a TFTP Server on the specified port. Gets and Puts occur in the specified directory. * * The server will start in another thread, allowing this constructor to return immediately. * * If a get or a put comes in with a relative path that tries to get outside of the * serverDirectory, then the get or put will be denied. * * GET_ONLY mode only allows gets, PUT_ONLY mode only allows puts, and GET_AND_PUT allows both. * Modes are defined as int constants in this class. * * @param serverReadDirectory directory for GET requests * @param serverWriteDirectory directory for PUT requests * @param port The local port to bind to. * @param localaddr The local address to bind to. * @param mode A value as specified above. * @param log Stream to write log message to. If not provided, uses System.out * @param errorLog Stream to write error messages to. If not provided, uses System.err. * @throws IOException if the server directory is invalid or does not exist. */ public TFTPServer ( File serverReadDirectory , File serverWriteDirectory , int port , InetAddress localaddr , ServerMode mode , PrintStream log , PrintStream errorLog ) throws IOException { port_ = port ; mode_ = mode ; laddr_ = localaddr ; log_ = ( log == null ? nullStream : log ); logError_ = ( errorLog == null ? nullStream : errorLog ); launch ( serverReadDirectory , serverWriteDirectory ); } /** * Start a TFTP Server on the specified port. Gets and Puts occur in the specified directory. * * The server will start in another thread, allowing this constructor to return immediately. * * If a get or a put comes in with a relative path that tries to get outside of the * serverDirectory, then the get or put will be denied. * * GET_ONLY mode only allows gets, PUT_ONLY mode only allows puts, and GET_AND_PUT allows both. * Modes are defined as int constants in this class. * * @param serverReadDirectory directory for GET requests * @param serverWriteDirectory directory for PUT requests * @param port the port to use * @param localiface The local network interface to bind to. * The interface's first address wil be used. * @param mode A value as specified above. * @param log Stream to write log message to. If not provided, uses System.out * @param errorLog Stream to write error messages to. If not provided, uses System.err. * @throws IOException if the server directory is invalid or does not exist. */ public TFTPServer ( File serverReadDirectory , File serverWriteDirectory , int port , NetworkInterface localiface , ServerMode mode , PrintStream log , PrintStream errorLog ) throws IOException { mode_ = mode ; port_ = port ; InetAddress iaddr = null ; if ( localiface != null ) { Enumeration < InetAddress > ifaddrs = localiface . getInetAddresses (); if ( ifaddrs != null ) { if ( ifaddrs . hasMoreElements ()) iaddr = ifaddrs . nextElement (); } } log_ = ( log == null ? nullStream : log ); logError_ = ( errorLog == null ? nullStream : errorLog ); laddr_ = iaddr ; launch ( serverReadDirectory , serverWriteDirectory ); } /** * Set the max number of retries in response to a timeout. Default 3. Min 0. * * @param retries number of retries, must be &gt; 0 */ public void setMaxTimeoutRetries ( int retries ) { if ( retries < 0 ) { throw new RuntimeException ( \"Invalid Value\" ); } maxTimeoutRetries_ = retries ; } /** * Get the current value for maxTimeoutRetries * @return the max allowed number of retries */ public int getMaxTimeoutRetries () { return maxTimeoutRetries_ ; } /** * Set the socket timeout in milliseconds used in transfers. Defaults to the value here: * http://commons.apache.org/net/apidocs/org/apache/commons/net/tftp/TFTP.html#DEFAULT_TIMEOUT * (5000 at the time I write this) Min value of 10. * @param timeout the timeout; must be larger than 10 */ public void setSocketTimeout ( int timeout ) { if ( timeout < 10 ) { throw new RuntimeException ( \"Invalid Value\" ); } socketTimeout_ = timeout ; } /** * The current socket timeout used during transfers in milliseconds. * @return the timeout value */ public int getSocketTimeout () { return socketTimeout_ ; } /* * start the server, throw an error if it can't start. */ private void launch ( File serverReadDirectory , File serverWriteDirectory ) throws IOException { log_ . println ( \"Starting TFTP Server on port \" + port_ + \". Read directory: \" + serverReadDirectory + \" Write directory: \" + serverWriteDirectory + \" Server Mode is \" + mode_ ); serverReadDirectory_ = serverReadDirectory . getCanonicalFile (); if (! serverReadDirectory_ . exists () || ! serverReadDirectory . isDirectory ()) { throw new IOException ( \"The server read directory \" + serverReadDirectory_ + \" does not exist\" ); } serverWriteDirectory_ = serverWriteDirectory . getCanonicalFile (); if (! serverWriteDirectory_ . exists () || ! serverWriteDirectory . isDirectory ()) { throw new IOException ( \"The server write directory \" + serverWriteDirectory_ + \" does not exist\" ); } serverTftp_ = new TFTP (); // This is the value used in response to each client. socketTimeout_ = serverTftp_ . getDefaultTimeout (); // we want the server thread to listen forever. serverTftp_ . setDefaultTimeout ( 0 ); if ( laddr_ != null ) { serverTftp_ . open ( port_ , laddr_ ); } else { serverTftp_ . open ( port_ ); } serverThread = new Thread ( this ); serverThread . setDaemon ( true ); serverThread . start (); } @Override protected void finalize () throws Throwable { shutdown (); } /** * check if the server thread is still running. * * @return true if running, false if stopped. * @throws Exception throws the exception that stopped the server if the server is stopped from * an exception. */ public boolean isRunning () throws Exception { if ( shutdownServer && serverException != null ) { throw serverException ; } return ! shutdownServer ; } @Override public void run () { try { while (! shutdownServer ) { TFTPPacket tftpPacket ; tftpPacket = serverTftp_ . receive (); TFTPTransfer tt = new TFTPTransfer ( tftpPacket ); synchronized ( transfers_ ) { transfers_ . add ( tt ); } Thread thread = new Thread ( tt ); thread . setDaemon ( true ); thread . start (); } } catch ( Exception e ) { if (! shutdownServer ) { serverException = e ; logError_ . println ( \"Unexpected Error in TFTP Server - Server shut down! + \" + e ); } } finally { shutdownServer = true ; // set this to true, so the launching thread can check to see if it started. if ( serverTftp_ != null && serverTftp_ . isOpen ()) { serverTftp_ . close (); } } } /** * Stop the tftp server (and any currently running transfers) and release all opened network * resources. */ public void shutdown () { shutdownServer = true ; synchronized ( transfers_ ) { Iterator < TFTPTransfer > it = transfers_ . iterator (); while ( it . hasNext ()) { it . next (). shutdown (); } } try { serverTftp_ . close (); } catch ( RuntimeException e ) { // noop } try { serverThread . join (); } catch ( InterruptedException e ) { // we've done the best we could, return } } /* * An instance of an ongoing transfer. */ private class TFTPTransfer implements Runnable { private final TFTPPacket tftpPacket_ ; private boolean shutdownTransfer = false ; TFTP transferTftp_ = null ; public TFTPTransfer ( TFTPPacket tftpPacket ) { tftpPacket_ = tftpPacket ; } public void shutdown () { shutdownTransfer = true ; try { transferTftp_ . close (); } catch ( RuntimeException e ) { // noop } } @Override public void run () { try { transferTftp_ = newTFTP (); transferTftp_ . beginBufferedOps (); transferTftp_ . setDefaultTimeout ( socketTimeout_ ); transferTftp_ . open (); if ( tftpPacket_ instanceof TFTPReadRequestPacket ) { handleRead ((( TFTPReadRequestPacket ) tftpPacket_ )); } else if ( tftpPacket_ instanceof TFTPWriteRequestPacket ) { handleWrite (( TFTPWriteRequestPacket ) tftpPacket_ ); } else { log_ . println ( \"Unsupported TFTP request (\" + tftpPacket_ + \") - ignored.\" ); } } catch ( Exception e ) { if (! shutdownTransfer ) { logError_ . println ( \"Unexpected Error in during TFTP file transfer. Transfer aborted. \" + e ); } } finally { try { if ( transferTftp_ != null && transferTftp_ . isOpen ()) { transferTftp_ . endBufferedOps (); transferTftp_ . close (); } } catch ( Exception e ) { // noop } synchronized ( transfers_ ) { transfers_ . remove ( this ); } } } /* * Handle a tftp read request. */ private void handleRead ( TFTPReadRequestPacket trrp ) throws IOException , TFTPPacketException { InputStream is = null ; try { if ( mode_ == ServerMode . PUT_ONLY ) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( trrp . getAddress (), trrp . getPort (), TFTPErrorPacket . ILLEGAL_OPERATION , \"Read not allowed by server.\" )); return ; } try { is = new BufferedInputStream ( new FileInputStream ( buildSafeFile ( serverReadDirectory_ , trrp . getFilename (), false ))); } catch ( FileNotFoundException e ) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( trrp . getAddress (), trrp . getPort (), TFTPErrorPacket . FILE_NOT_FOUND , e . getMessage ())); return ; } catch ( Exception e ) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( trrp . getAddress (), trrp . getPort (), TFTPErrorPacket . UNDEFINED , e . getMessage ())); return ; } if ( trrp . getMode () == TFTP . NETASCII_MODE ) { is = new ToNetASCIIInputStream ( is ); } byte [] temp = new byte [ TFTPDataPacket . MAX_DATA_LENGTH ]; TFTPPacket answer ; int block = 1 ; boolean sendNext = true ; int readLength = TFTPDataPacket . MAX_DATA_LENGTH ; TFTPDataPacket lastSentData = null ; // We are reading a file, so when we read less than the // requested bytes, we know that we are at the end of the file. while ( readLength == TFTPDataPacket . MAX_DATA_LENGTH && ! shutdownTransfer ) { if ( sendNext ) { readLength = is . read ( temp ); if ( readLength == - 1 ) { readLength = 0 ; } lastSentData = new TFTPDataPacket ( trrp . getAddress (), trrp . getPort (), block , temp , 0 , readLength ); sendData ( transferTftp_ , lastSentData ); // send the data } answer = null ; int timeoutCount = 0 ; while (! shutdownTransfer && ( answer == null || ! answer . getAddress (). equals ( trrp . getAddress ()) || answer . getPort () != trrp . getPort ())) { // listen for an answer. if ( answer != null ) { // The answer that we got didn't come from the // expected source, fire back an error, and continue // listening. log_ . println ( \"TFTP Server ignoring message from unexpected source.\" ); transferTftp_ . bufferedSend ( new TFTPErrorPacket ( answer . getAddress (), answer . getPort (), TFTPErrorPacket . UNKNOWN_TID , \"Unexpected Host or Port\" )); } try { answer = transferTftp_ . bufferedReceive (); } catch ( SocketTimeoutException e ) { if ( timeoutCount >= maxTimeoutRetries_ ) { throw e ; } // didn't get an ack for this data. need to resend // it. timeoutCount ++; transferTftp_ . bufferedSend ( lastSentData ); continue ; } } if ( answer == null || !( answer instanceof TFTPAckPacket )) { if (! shutdownTransfer ) { logError_ . println ( \"Unexpected response from tftp client during transfer (\" + answer + \"). Transfer aborted.\" ); } break ; } else { // once we get here, we know we have an answer packet // from the correct host. TFTPAckPacket ack = ( TFTPAckPacket ) answer ; if ( ack . getBlockNumber () != block ) { /* * The origional tftp spec would have called on us to resend the * previous data here, however, that causes the SAS Syndrome. * http://www.faqs.org/rfcs/rfc1123.html section 4.2.3.1 The modified * spec says that we ignore a duplicate ack. If the packet was really * lost, we will time out on receive, and resend the previous data at * that point. */ sendNext = false ; } else { // send the next block block ++; if ( block > 65535 ) { // wrap the block number block = 0 ; } sendNext = true ; } } } } finally { try { if ( is != null ) { is . close (); } } catch ( IOException e ) { // noop } } } /* * handle a tftp write request. */ private void handleWrite ( TFTPWriteRequestPacket twrp ) throws IOException , TFTPPacketException { OutputStream bos = null ; try { if ( mode_ == ServerMode . GET_ONLY ) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( twrp . getAddress (), twrp . getPort (), TFTPErrorPacket . ILLEGAL_OPERATION , \"Write not allowed by server.\" )); return ; } int lastBlock = 0 ; String fileName = twrp . getFilename (); try { File temp = buildSafeFile ( serverWriteDirectory_ , fileName , true ); if ( temp . exists ()) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( twrp . getAddress (), twrp . getPort (), TFTPErrorPacket . FILE_EXISTS , \"File already exists\" )); return ; } bos = new BufferedOutputStream ( new FileOutputStream ( temp )); if ( twrp . getMode () == TFTP . NETASCII_MODE ) { bos = new FromNetASCIIOutputStream ( bos ); } } catch ( Exception e ) { transferTftp_ . bufferedSend ( new TFTPErrorPacket ( twrp . getAddress (), twrp . getPort (), TFTPErrorPacket . UNDEFINED , e . getMessage ())); return ; } TFTPAckPacket lastSentAck = new TFTPAckPacket ( twrp . getAddress (), twrp . getPort (), 0 ); sendData ( transferTftp_ , lastSentAck ); // send the data while ( true ) { // get the response - ensure it is from the right place. TFTPPacket dataPacket = null ; int timeoutCount = 0 ; while (! shutdownTransfer && ( dataPacket == null || ! dataPacket . getAddress (). equals ( twrp . getAddress ()) || dataPacket . getPort () != twrp . getPort ())) { // listen for an answer. if ( dataPacket != null ) { // The data that we got didn't come from the // expected source, fire back an error, and continue // listening. log_ . println ( \"TFTP Server ignoring message from unexpected source.\" ); transferTftp_ . bufferedSend ( new TFTPErrorPacket ( dataPacket . getAddress (), dataPacket . getPort (), TFTPErrorPacket . UNKNOWN_TID , \"Unexpected Host or Port\" )); } try { dataPacket = transferTftp_ . bufferedReceive (); } catch ( SocketTimeoutException e ) { if ( timeoutCount >= maxTimeoutRetries_ ) { throw e ; } // It didn't get our ack. Resend it. transferTftp_ . bufferedSend ( lastSentAck ); timeoutCount ++; continue ; } } if ( dataPacket != null && dataPacket instanceof TFTPWriteRequestPacket ) { // it must have missed our initial ack. Send another. lastSentAck = new TFTPAckPacket ( twrp . getAddress (), twrp . getPort (), 0 ); transferTftp_ . bufferedSend ( lastSentAck ); } else if ( dataPacket == null || !( dataPacket instanceof TFTPDataPacket )) { if (! shutdownTransfer ) { logError_ . println ( \"Unexpected response from tftp client during transfer (\" + dataPacket + \"). Transfer aborted.\" ); } break ; } else { int block = (( TFTPDataPacket ) dataPacket ). getBlockNumber (); byte [] data = (( TFTPDataPacket ) dataPacket ). getData (); int dataLength = (( TFTPDataPacket ) dataPacket ). getDataLength (); int dataOffset = (( TFTPDataPacket ) dataPacket ). getDataOffset (); if ( block > lastBlock || ( lastBlock == 65535 && block == 0 )) { // it might resend a data block if it missed our ack // - don't rewrite the block. bos . write ( data , dataOffset , dataLength ); lastBlock = block ; } lastSentAck = new TFTPAckPacket ( twrp . getAddress (), twrp . getPort (), block ); sendData ( transferTftp_ , lastSentAck ); // send the data if ( dataLength < TFTPDataPacket . MAX_DATA_LENGTH ) { // end of stream signal - The tranfer is complete. bos . close (); // But my ack may be lost - so listen to see if I // need to resend the ack. for ( int i = 0 ; i < maxTimeoutRetries_ ; i ++) { try { dataPacket = transferTftp_ . bufferedReceive (); } catch ( SocketTimeoutException e ) { // this is the expected route - the client // shouldn't be sending any more packets. break ; } if ( dataPacket != null && (! dataPacket . getAddress (). equals ( twrp . getAddress ()) || dataPacket . getPort () != twrp . getPort ())) { // make sure it was from the right client... transferTftp_ . bufferedSend ( new TFTPErrorPacket ( dataPacket . getAddress (), dataPacket . getPort (), TFTPErrorPacket . UNKNOWN_TID , \"Unexpected Host or Port\" )); } else { // This means they sent us the last // datapacket again, must have missed our // ack. resend it. transferTftp_ . bufferedSend ( lastSentAck ); } } // all done. break ; } } } } finally { if ( bos != null ) { bos . close (); } } } /* * Utility method to make sure that paths provided by tftp clients do not get outside of the * serverRoot directory. */ private File buildSafeFile ( File serverDirectory , String fileName , boolean createSubDirs ) throws IOException { File temp = new File ( serverDirectory , fileName ); temp = temp . getCanonicalFile (); if (! isSubdirectoryOf ( serverDirectory , temp )) { throw new IOException ( \"Cannot access files outside of tftp server root.\" ); } // ensure directory exists (if requested) if ( createSubDirs ) { createDirectory ( temp . getParentFile ()); } return temp ; } /* * recursively create subdirectories */ private void createDirectory ( File file ) throws IOException { File parent = file . getParentFile (); if ( parent == null ) { throw new IOException ( \"Unexpected error creating requested directory\" ); } if (! parent . exists ()) { // recurse... createDirectory ( parent ); } if ( parent . isDirectory ()) { if ( file . isDirectory ()) { return ; } boolean result = file . mkdir (); if (! result ) { throw new IOException ( \"Couldn't create requested directory\" ); } } else { throw new IOException ( \"Invalid directory path - file in the way of requested folder\" ); } } /* * recursively check to see if one directory is a parent of another. */ private boolean isSubdirectoryOf ( File parent , File child ) { File childsParent = child . getParentFile (); if ( childsParent == null ) { return false ; } if ( childsParent . equals ( parent )) { return true ; } else { return isSubdirectoryOf ( parent , childsParent ); } } } /** * Set the stream object to log debug / informational messages. By default, this is a no-op * * @param log the stream to use for logging */ public void setLog ( PrintStream log ) { this . log_ = log ; } /** * Set the stream object to log error messsages. By default, this is a no-op * * @param logError the stream to use for logging errors */ public void setLogError ( PrintStream logError ) { this . logError_ = logError ; } /* * Allow test code to customise the TFTP instance */ TFTP newTFTP () { return new TFTP (); } /* * Also allow customisation of sending data/ack so can generate errors if needed */ void sendData ( TFTP tftp , TFTPPacket data ) throws IOException { tftp . bufferedSend ( data ); } }","tags":"Backend","url":"https://jiang-hao.com/articles/2019/backend-AJavaTFTPServer.html","loc":"https://jiang-hao.com/articles/2019/backend-AJavaTFTPServer.html"},{"title":"Java代码优化35点总结","text":"前言 代码优化，一个很重要的课题。可能有些人觉得没用，一些细小的地方有什么好修改的，改与不改对于代码的运行效率有什么影响呢？这个问题我是这么考虑的，就像大海里面的鲸鱼一样，它吃一条小虾米有用吗？没用，但是，吃的小虾米一多之后，鲸鱼就被喂饱了。代码优化也是一样，如果项目着眼于尽快无BUG上线，那么此时可以抓大放小，代码的细节可以不精打细磨；但是如果有足够的时间开发、维护代码，这时候就必须考虑每个可以优化的细节了，一个一个细小的优化点累积起来，对于代码的运行效率绝对是有提升的。 代码优化的目标是： 1、减小代码的体积 2、提高代码运行的效率 代码优化细节 1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final，则该类所有的方法都是final的。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。此举能够使性能平均提高50%。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： for ( int i = 0 ; i < list . size (); i ++) {...} 建议替换为： for ( int i = 0 , int length = list . size (); i < length ; i ++) {...} 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： String str = \"aaa\" ; if ( i == 1 ) { list . add ( str ); } 建议替换为： if ( i == 1 ) { String str = \"aaa\" ; list . add ( str ); } 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为fillInStackTrace()的本地同步方法，fillInStackTrace()方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用try...catch...，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet等等，以StringBuilder为例： StringBuilder () // 默认分配16个字符的空间 StringBuilder ( int size ) // 默认分配size个字符的空间 StringBuilder ( String str ) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的StringBuilder）的来设定它的初始化容量，这样可以明显地提升性能。比如StringBuilder吧，length表示当前的StringBuilder能保持的字符数量。因为当StringBuilder达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要StringBuilder达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中----这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： 在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间 把原来的4096个字符拷贝到新的的字符数组中去 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成new HashMap(128)、new HashMap(256)都可以。 10、当复制大量数据时，使用System.arraycopy()命令 11、乘法和除法使用移位操作 例如： for ( val = 0 ; val < 100000 ; val += 5 ) { a = val * 8 ; b = val / 2 ; } 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： for ( val = 0 ; val < 100000 ; val += 5 ) { a = val << 3 ; b = val >> 1 ; } 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 例如： for ( int i = 1 ; i <= count ; i ++) { Object obj = new Object (); } 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： Object obj = null ; for ( int i = 0 ; i <= count ; i ++){ obj = new Object ();} 这样的话，内存中只有一份Object对象引用，每次new Object()的时候，Object对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array 无法确定数组大小时才使用ArrayList 14、尽量使用HashMap、ArrayList、StringBuilder 除非线程安全需要，否则不推荐使用Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为public static final 因为这毫无意义，这样只是定义了引用为static final，数组的内容还是可以随意改变的，将数组声明为public更是一个安全漏洞，这意味着这个数组可以被外部类所改变 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： 控制资源的使用，通过线程同步来控制资源的并发访问 控制实例的产生，以达到节约资源的目的 控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： public class A { private static B b = new B (); } 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用HttpSession的invalidate()方法清除会话。 19、实现RandomAccess接口的集合比如ArrayList，应当使用最普通的for循环而不是foreach循环来遍历 这是JDK推荐给用户的。JDK API对于RandomAccess接口的解释是：实现RandomAccess接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现RandomAccess接口的类实例，假如是随机访问的，使用普通for循环效率将高于使用foreach循环；反过来，如果是顺序访问的，则使用Iterator会效率更高。可以使用类似如下的代码作判断： if ( list instanceof RandomAccess ) { for ( int i = 0 ; i < list . size (); i ++){} } else { Iterator <?> iterator = list . iterable (); while ( iterator . hasNext ()){ iterator . next ()} } foreach循环的底层实现原理就是迭代器Iterator，参见Java语法糖1：可变长度参数以及foreach循环原理。所以后半句\"反过来，如果是顺序访问的，则使用Iterator会效率更高\"的意思就是顺序访问的那些类实例，使用foreach循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的synchronized锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现\"The value of the local variable i is not used\"、\"The import java.util is never used\"，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存----用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList 这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 参数太多势必导致方法调用的出错概率增加 至于这个\"太多\"指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： String str = \"123\" ; if ( str . equals ( \"123\" )){ ... } 建议修改为： String str = \"123\" ; if ( \"123\" . equals ( str )) { ... } 这么做主要是可以避免空指针异常 29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问， if (i == 1) 和 if (1== i) 有没有区别，这就要从C/C++讲起。 在C/C++中， if (i == 1) 判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： int i = 2 ; if ( i == 1 ) { ... } else { ... } C/C++判断\"i==1\"不成立，所以以0表示，即false。但是如果： int i = 2 ; if ( i = 1 ){ ...} else { ...} 万一程序员一个不小心，把\"if (i == 1)\"写成\"if (i = 1)\"，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C/C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： int i = 2 ; if ( 1 == i ){ ...} else { ...} 这样，即使开发者不小心写成了 1 = i ，C/C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i。 但是，在Java中，C/C++这种 if (i = 1) 的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错\"Type mismatch: cannot convert from int to boolean\"。但是，尽管Java的 if (i == 1) 和 if (1 == i) 在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： public static void main ( String [] args ) { int [] is = new int []{ 1 , 2 , 3 }; System . out . println ( is . toString ()); } 结果是： [ I @ 18 a992f 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections 重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： public static void main ( String [] args ) { long l = 12345678901234L ; int i = ( int ) l ; System . out . println ( i ); } 我们可能期望得到其中的某几位，但是结果却是： 1942892530 解释一下。Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是： 0000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010 一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是： 0111 0011 1100 1110 0010 1111 1111 0010 这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论： 整型默认的数据类型是int; long l = 12345678901234L ，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成 float f = 3.5f 接下来再写一句 int ii = l + i; 会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+\"\"最慢 把一个基本数据类型转为一般有三种方式，我有一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+\"\"三种方式，三种方式的效率如何，看一个测试： public static void main ( String [] args ) { int loopTime = 50000 ; Integer i = 0 ; long startTime = System . currentTimeMillis (); for ( int j = 0 ; j < loopTime ; j ++) { String str = String . valueOf ( i ); } System . out . println ( \"String.valueOf()：\" + ( System . currentTimeMillis () - startTime ) + \"ms\" ); startTime = System . currentTimeMillis (); for ( int j = 0 ; j < loopTime ; j ++) { String str = i . toString (); } System . out . println ( \"Integer.toString()：\" + ( System . currentTimeMillis () - startTime ) + \"ms\" ); startTime = System . currentTimeMillis (); for ( int j = 0 ; j < loopTime ; j ++) { String str = i + \"\" ; } System . out . println ( \"i + \"\"：\" + ( System . currentTimeMillis () - startTime ) + \"ms\" ); } 运行结果为： String . valueOf () ： 11 msInteger . toString () ： 5 msi + \"\" ： 25 ms 所以以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 Integer.toString()方法就不说了，直接调用了 i + \"\"底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： public static void main ( String [] args ) { HashMap < String , String > hm = new HashMap < String , String >(); hm . put ( \"111\" , \"222\" ); Set < Map . Entry < String , String >> entrySet = hm . entrySet (); Iterator < Map . Entry < String , String >> iter = entrySet . iterator (); while ( iter . hasNext ()) { Map . Entry < String , String > entry = iter . next (); System . out . println ( entry . getKey () + \" \" + entry . getValue ()); } } 如果你只是想遍历一下这个Map的key值，那用 Set<String> keySet = hm.keySet(); 会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： try{ XXX.close(); YYY.close(); }catch (Exception e) { ... } 建议修改为： try { XXX . close ();} catch ( Exception e ){ ...} try { YYY . close ();} catch ( Exception e ){ ...} 虽然有些麻烦，却能避免资源泄露。我们想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为下面的写法之后，就保证了无论如何XXX和YYY都会被close掉。","tags":"Backend","url":"https://jiang-hao.com/articles/2019/backend-Java代码优化35点总结.html","loc":"https://jiang-hao.com/articles/2019/backend-Java代码优化35点总结.html"},{"title":"人之一生","text":"热爱生命 我不去想， 是否能够成功 ， 既然选择了远方 ， 便只顾风雨兼程。 我不去想， 能否赢得爱情 ， 既然钟情于玫瑰 ， 就勇敢地吐露真诚 。 我不去想， 身后会不会袭来寒风冷雨 ， 既然目标是地平线， 留给世界的只能是背影 。 我不去想， 未来是平坦还是泥泞 ， 只要热爱生命 ， 一切，都在意料之中。 决心 人之一生，总会有许多困惑不已、纠缠不清的琐事，难免受其影响，绝不为其左右。 此时所需的就是断然的取舍与明智的抉择，唯一会限制我们的，是我们自己的决心。 延续 万物善变，人生泛滥的是那些冲动和急躁：即兴的热情、肤浅的思考、仓促的决定、敷衍的搪塞和最终的草草了事； 而珍贵的是生命中那些经久和坚持，一种信仰，一段情感，一个习惯。一句独白自视为约定，一段感情权当作余生。 动力 如果说不清追逐的动力源于何处，就至少要让优秀成为自己的习惯，要乐观阳光，要知道自己身在何处，路在何方。 初心 愿我们在心底埋下的勇气、乐观和向善的种子，最后都成长为自己一直所追寻的最喜欢的样子。","tags":"Notes","url":"https://jiang-hao.com/articles/2019/notes-人之一生.html","loc":"https://jiang-hao.com/articles/2019/notes-人之一生.html"},{"title":"JQCloud: 一个前端生成美化标签云的简单JQuery插件","text":"因为博客需要，发现了一个生成美化简约风格的标签云的JQuery插件。 官网地址： http://mistic100.github.io/jQCloud/index.html 使用方法很简单，可以把JS和CSS文件下载到本地，也可以直接通过Script标签src=\"\"的方法在线引用。 具体的使用方法官网都能查到。 贴出自己微博使用JQCloud的前端代码： < script src = \"{{ SITEURL }}/theme/jqcloud.js\" ></ script > < link href = \"{{ SITEURL }}/theme/jqcloud.css\" rel = \"stylesheet\" > < script > var words = []; { % for tag , articles in tags | sort % } words . push ({ text : \"{{tag}}\" , weight : Math . random (), link : '{{ SITEURL }}/{{ tag.url }}' }); { % endfor % } { % for category , articles in categories % } words . push ({ text : \"{{category}}\" , weight : Math . random (), link : '{{ SITEURL }}/{{ category.url }}' }); { % endfor % } $ ( function () { $ ( \"#tagcloud\" ). jQCloud ( words , { autoResize : true }); }); </ script > < div id = \"tagcloud\" style = \"width: 80%; height: 450px; align-self: center;\" ></ div > 需要注意的是要包含标签云的div模块需要显示指定width和height，否则需要在JavaScript中进行相关设置。 踩坑1：因为要基于JQuery，注意引用的JQuery库可用。由于之前引用的是外网谷歌的库，国内被墙导致标签云一直没有刷出来，后来换成了bootcdn的JQuery库就成功了： < script src = \"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\" ></ script >","tags":"Frontend","url":"https://jiang-hao.com/articles/2018/frontend-JQCloud.html","loc":"https://jiang-hao.com/articles/2018/frontend-JQCloud.html"},{"title":"使用Pelican在Github(国外)和Coding(国内)同步托管博客","text":"介绍： Github Pages 禁用了百度爬虫，因此百度搜索引擎经常抓取不到在Github上托管的博客链接。本文介绍一种可行的解决方法： - 注册Coding用来托管一份和Github上一样的博客仓库专门服务国内的索引 - 配置DNS解析，将国内的线路解析到Coding，国外的线路解析到Github - 配置Pelican，支持一键将同一份本地博客仓库同时发布到Github和Coding ​ 一、 《Pelican＋Github博客搭建详细教程》 按照标题链接给出的教程先搭建出一个基于Github托管的博客系统。接下来将说明如何将博客同步到Coding。 二、在Coding创建一个新的项目 在 Coding首页 进行注册并登陆，创建项目的方法与Github类似，不同之处在于coding新建的公开项目名和用户名相同，而不像Github那样是<用户名>.github.io。创建完成后，生成的新的项目链接应该类似于： https://coding.net/<usrname>/<username>.git 。 将本地SSH公钥拷贝到coding。操作同样与Github类似。由于本地已经为Github生成了一个公钥，这里只用cd进入~/.ssh文件夹查看一个名为 id_rsa.pub 文件的内容，类似于如下。我们只拷贝 邮箱之前 的所有内容到coding的公钥管理页面。 ssh-rsa AAAAfafjIJGOF+FDA。。(省略)。。Ksap Heriam@users.noreply.github.com 三、将仓库拷贝到Coding 进入Pelican的output目录下的本地博客仓库，打开.git/config，修改远程仓库，将 origin 改为 github，并添加 coding： [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \"github\"] url = git@github.com:Heriam/heriam.github.io.git fetch = +refs/heads/*:refs/remotes/github/* [remote \"coding\"] url = git@git.coding.net:Heriam/heriam.git fetch = +refs/heads/*:refs/remotes/coding/* [branch \"master\"] remote = origin merge = refs/heads/master 2. 然后将仓库 push 到 Coding上，在Coding新建一个 coding-pages 分支： git push -u coding master:coding-pages 3. 这时登录Coding就可以看到博客内容已经被拷贝到coding-pages分支。 四、配置域名 登录到网站的域名解析管理页面（我用的是DNSPOD，后来转向Cloudxns），然后添加两条域名解析记录： @ CNAME 国内 coding.me www CNAME 国内 coding.me 在Coding 上\"项目管理\"中找到\"自定义域名／Pages\"，添加要绑定的域名，比如我是 jiang-hao.com 和 www.jiang-hao.com 。注意这些域名也就是我们刚刚在dnspod中设置的解析域名。 五、配置Pelican实现同步提交 设置一键上传：（如有疑问参见 《Pelican＋Github博客搭建详细教程》 第三部分第4点）打开根目录下的Makefile文件，修改以下三个地方： OUTPUTDIR OUTPUTDIR = $( BASEDIR ) /output/<username>.github.io #本地博客仓库路径 - publish publish : $( PELICAN ) $( INPUTDIR ) -o $( OUTPUTDIR ) -s $( CONFFILE ) $( PELICANOPTS ) - github: publish github : publish cd $ ( OUTPUTDIR ) ; git add . ; git commit - am '<添加自己的备注>' ; git push github master : master ; git push coding master : coding - pages 这样 ，通过 make github 命令就能一键发布博客更新到Github和Coding了。","tags":"Frontend","url":"https://jiang-hao.com/articles/2018/frontend-使用Pelican在Github和Coding同步托管博客.html","loc":"https://jiang-hao.com/articles/2018/frontend-使用Pelican在Github和Coding同步托管博客.html"},{"title":"使用Pelican基于Github Pages搭建博客教程","text":"操作系统： Mac OS / Linux 工具集： 1.Pelican——基于Python的静态网页生成器 2.马克飞象——Evernote出的Markdown文本编辑器 3.GoDaddy——域名供应商 4.DNSPod——提供免费域名解析注册服务 5.Github Pages——Github为每个注册用户提供300M的站点空间 6.Python——Pelican工具需要Python运行环境 7.Google Analytics——谷歌站点数据监测分析工具 8.Google Custom Search——谷歌自定义搜索引擎可用作站内搜索工具 9.Google Webmasters——谷歌站长工具 10.Disqus——用来提供博客评论功能 11.Sitemap——站点地图，供谷歌，百度等搜索引擎收录 12.七牛云存储——静态资源管理，上传自动生成网盘直链 最终效果展示： 欢迎访问我的博客： https://jiang-hao.com 一、使用Github Pages创建个人博客页面 Git是一个开源的分布式版本控制系统，用以有效、高速的处理从很小到非常大的项目版本管理。GitHub可以托管各种git库的站点。通过GitHub Pages生成的静态站点，可以免费托管、自定义主题、并且自制网页界面。 ​ 1.首先到Github进行账号注册： https://github.com/ 。 2.注册后登录Github，右上角点击\"Creat a new repo\"，跳转到新页面后填写相关内容，注意版本库名使用'username.github.io'的格式，这里将username替换成自己的用户名即可。 3.设置和选择好页面模板后就可以生成然后发布新网页了。 4.创建SSH密钥并上传到Github。 ​ ​ *以上内容都很简单，有问题可以参照： 关于Github注册登录： 通过GitHub创建个人技术博客图文详解1 关于Github页面生成： 通过GitHub创建个人技术博客图文详解2 关于SSH认证： Windows/Mac下使用SSH密钥连接Github 官方文档： Github官方文档在这里 二、安装Python、Pelican和Markdown Pelican是一套开源的使用Python编写的博客静态生成, 可以添加文章和和创建页面, 可以使用MarkDown reStructuredText 和 AsiiDoc 的格式来抒写, 同时使用 Disqus评论系统, 支持 RSS和Atom输出, 插件, 主题, 代码高亮等功能, 采用Jajin2模板引擎, 可以很容易的更改模板。 ​ 1.安装Python。最新的Mac OS 一般都自带Python环境。在终端输入\"python\"即可确认Python版本。如有需要可以到官网安装：http://www.python.org/。 ​ 2.安装Pelican。可以从github克隆最新的代码安装, 并且建议在virtualenv下使用。首先建立 virtualenv（Python虚拟环境）: virtualenv pelican # 创建 cd pelican sh bin/activate # 激活虚拟环境 从github克隆最新代码安装Pelican： git clone git://github.com/getpelican/pelican.git # 下载代码 cd pelican python setup.py install 3.安装Markdown: pip install markdown 三、创建博客骨架 接下来将通过初始化Pelican设置来生成一个基本的博客框架。 ​ 1.搭建博客目录： mkdir blog cd blog pelican-quickstart 2.根据提示一步步输入相应的配置项，不知道如何设置的接受默认即可，后续可以通过编辑pelicanconf.py文件更改配置。完成后将会在根目录生成以下文件： . |-- content # 所有文章放于此目录 │ └── (pages) # 存放手工创建的静态页面 |-- develop_server.sh # 用于开启测试服务器 |-- Makefile # 方便管理博客的Makefile |-- output # 静态生成文件 |-- pelicanconf.py # 配置文件 |-- publishconf.py # 配置文件 3.进入output文件夹，把自己刚刚建好的username.github.io版本库clone下来，注意这里以及后文中的username要替换成自己的Github用户名： cd output git clone https://github.com/username/username.github.io.git 4.设置一键上传部署到Github。打开根目录下的Makefile文件，修改以下三个地方： OUTPUTDIR = $( BASEDIR ) /output/username.github.io publish: $( PELICAN ) $( INPUTDIR ) -o $( OUTPUTDIR ) -s $( CONFFILE ) $( PELICANOPTS ) github: publish cd OUTPUTDIR ; git add . ; git commit -am 'your comments' ; git push 5.设置完后，以后写完文章就可以通过在blog根目录下执行\"make github\"进行一键部署了。 四、通过Markdown试写博文并上传Github发布 Markdown是当下非常流行的一种文本编辑语法，支持HTML转换，书写博文排版也方便快捷。 ​ 1.写一篇文章：用 马克飞象 编辑器用Markdown语法来写一篇文章保存为.md格式放在content目录下。写完后，执行以下命令，即可在本机http://127.0.0.1:8000看到效果。 make publish make serve 2.创建一个页面：这里以创建 About页面为例。在content目录创建pages子目录： mkdir content/pages *然后创建About.md并填入下面内容： title : About Me date : 2013 - 04 - 18 About me content *注意上面title和date是.md文件的重要参数，需要写在文档开头。比如： Title : Pelican + Github Date : 2014 - 10 - 07 22 : 20 Modified : 2014 - 10 - 07 23 : 04 Tags : python , pelican Slug : build - blog - system - by - pelican Authors : Joey Huang Summary : blablablablablablablabla ... Status : draft 相关介绍请参见官方文档：http://pelican-zh.readthedocs.org/en/latest/zh-cn/ 。完成后同样可以在本机http://127.0.0.1:8000看效果。 ​ 3.创建导航目录项：Menu Item设置。在你的博客中，可设置相应的菜单项，菜单项是通过pelicanconf.py设置的，具体如下所示： MENUITEMS = ((\"ITEM1\",\"http://github.com\"), (\"ITEM2\",URL), ......) 五、安装主题 这里以主题bootstrap2为例，同样还在blog目录下： git clone https://github.com/getpelican/pelican-themes.git cd pelican-themes pelican-themes -i bootstrap2 对应在在pelicanconf.py中添加主题选择条目： THEME = 'bootstrap2' 六、安装第三方评论系统Disqus 在Disqus上申请一个站点，记住shortname。 在pelicanconf.py添加： DISQUS_SITENAME = Shortname 七、添加Google Analytics 去Google Analytics申请账号并通过验证，记下跟踪ID（Track ID）， 在pelicanconf.py添加： GOOGLE_ANALYTICS = '跟踪ID' 八、添加Google Webmasters和百度站长收录 为了让博客被Google更好的收录，比如手动让Googlebot抓取、提交Robots、更新Sitemap等等。 ​ 1.在Google Webmasters上注册并通过验证。 ​ 2.添加sitemap插件。还是到/blog目录下执行： cd ~/blog git clone git://github.com/getpelican/pelican-plugins.git *然后在pelicanconf.py里配置如下： PLUGIN_PATH = u\"pelican-plugins\" PLUGINS = [\"sitemap\"] SITEMAP = { \"format\": \"xml\", \"priorities\": { \"articles\": 0.7, \"indexes\": 0.5, \"pages\": 0.3, }, \"changefreqs\": { \"articles\": \"monthly\", \"indexes\": \"daily\", \"pages\": \"monthly\", } } 3.将make github命令后在output目录下生成的sitemap文件上传到Google Webmasters。 ​ 4.对于百度。它是宣称支持sitemap的，但是网上相关问题一大堆，要么格式不对要么就是抓取失败，要么突然不开放支持。在几次尝试失败以后，我是通过添加JavaScript代码来自动推送网站链接的。具体是在主题模板（base.html）面最后添加代码： <script> (function(){ var bp = document.createElement('script'); bp.src = '//push.zhanzhang.baidu.com/push.js'; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(bp, s); })(); </script> *我还是比较推崇这种方法的，因为比sitemap方法被抓取收录的时间短很多。谷歌的sitemap是xml格式。 九、添加谷歌／百度站内搜索 谷歌站内搜索 1.修改主题。找到这个主题的templates文件夹中的base.html，在 <div class=\"nav-collapse\"> 的最后，添加以下内容： <form class= \"navbar-search pull-right\" action= \"/search.html\" > <input type= \"text\" class= \"search-query\" placeholder= \"Search\" name= \"q\" id= \"s\" > </form> 2.创建search.html。之后，在output目录下，新建一个名为search.html的文件，写入下面的内容，其中需要你自己修改的是google站内搜索的ID号，需要自己在 google站内搜索 的网站上自己申请。 <html lang= \"zh_CN\" > <head> <meta charset= \"utf-8\" > <title> 站内搜索 </title> </head> <body> <style> #search-box { position: relative; width: 50%; margin: 0; padding: 1em; } #search-form { height: 30px; border: 1px solid #999; -webkit-border-radius: 5px; -moz-border-radius: 5px; border-radius: 5px; background-color: #fff; overflow: hidden; } #search-text { font-size: 14px; color: #ddd; border-width: 0; background: transparent; } #search-box input[type=\"text\"] { width: 90%; padding: 4px 0 12px 1em; color: #333; outline: none; } </style> <div id= 'search-box' > <form action= '/search.html' id= 'search-form' method= 'get' target= '_top' > <input id= 'search-text' name= 'q' placeholder= 'Search' type= 'text' /> </form> </div> <div id= \"cse\" style= \"width: 100%;\" > Loading </div> <script src= \"http://www.google.com/jsapi\" type= \"text/javascript\" ></script> <script type= \"text/javascript\" > google.load('search', '1', {language : 'zh-CN', style : google.loader.themes.V2_DEFAULT}); google.setOnLoadCallback(function() { var customSearchOptions = {}; var customSearchControl = new google.search.CustomSearchControl( '012191777864628038963:********** <!写入你申请的google站内搜索的ID号> ）', customSearchOptions); customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET); var options = new google.search.DrawOptions(); options.enableSearchResultsOnly(); customSearchControl.draw('cse', options); function parseParamsFromUrl() { var params = {}; var parts = window.location.search.substr(1).split('\\x26'); for (var i = 0; i < parts.length ; i++) { var keyValuePair = parts[i].split('='); var key = decodeURIComponent(keyValuePair[0]); params[key] = keyValuePair[1] ? decodeURIComponent(keyValuePair[1].replace(/\\+/g, ' ')) : keyValuePair[1]; } return params; } var urlParams = parseParamsFromUrl(); var queryParamName = \"q\" ; if (urlParams[queryParamName]) { customSearchControl.execute(urlParams[queryParamName]); } }, true); </script > </body> </html> 3.将GOOGLE_CUSTOM_SEARCH_SIDEBAR = \"001578481551708017171:axpo6yvtdyg\" 添加到pelicanconf.py文件。注意, 引号里的那一串字符是之前申请的自定义搜索引擎的id。 4.最后发布后就可以看到搜索框了。 ​ 百度站内搜索 1.在百度站长平台中注册一个账号，之后添加网站，按照提示验证网站。之后左侧 其他工具 中找到 站内搜索 ，按照提示填写基本信息，选择搜索框样式，之后点击 查看代码 ，复制其中内容，留用。 2.同样在base.html的这个 <div class=\"nav-collapse\"> 的最后，新建一个 div ，刚才注册最后复制的代码粘贴到这个 div 中： <div class= \"navbar-search pull-right\" > <script> <!略> </script> </div> 3.发布验证。 十、添加Tags侧边栏 在其他一些pelican主题中，看到有标签云，想到Tags的链接可能比Categories的链接更有用，通过更改主题，添加了侧栏中红框内的Tags链接框。 ​ 1.还是找到base.html，找到categories部分： {% if categories %} <div class= \"well\" style= \"padding: 8px 0; background-color: #FBFBFB;\" > <ul class= \"nav nav-list\" > <li class= \"nav-header\" > Categories </li> {% for cat , null in categories %} <li><a href= \" {{ SITEURL }} / {{ cat.url }} \" > {{ cat }} </a></li> {% endfor %} </ul> </div> {% endif %} 2.在这段后面添加： {% if tags %} <div class= \"well\" style= \"padding: 8px 0; background-color: #FBFBFB;\" > <ul class= \"nav nav-list\" > <li class= \"nav-header\" > Tags </li> {% for name , tag in tags %} <li><a href= \" {{ SITEURL }} / {{ name.url }} \" > {{ name }} </a></li> {% endfor %} </ul> </div> {% endif %} 3.保存，重新发布网页验证。 十一、插入视频 其实很简单, 只需要把html代码放进markdown源文件就行了! 而视频的html代码在视频网站上一般都会提供。复制下来放进源文件即可。 十二、拷贝静态文件 如果我们定义静态的文件，该如何将它在每次生成的时候拷贝到 output 目录呢，我们以网站logo图片sitelogo.ico为例，在我们的 content/extra 下放置网站的静态资源文件：sitelogo.ico，在pelicanconf.py更改或添加 FILES_TO_COPY项： FILES_TO_COPY = ( (\"extra/sitelogo.ico\", \"sitelogo.ico\"), ) 这样在每次生成html的时候都会把 content/extra下的 sitelogo.ico 拷贝到 output目录下。 十三、资源目录管理 使用目录名作为文章的分类名 USE_FOLDER_AS_CATEGORY = True 使用文件名作为文章或页面的 slug（url） FILENAME_METADATA = '(?P<slug>.*)' 页面的显示路径和保存路径，推荐下面的方式 ARTICLE_URL = '{category}/{slug}.html' ARTICLE_SAVE_AS = ARTICLE_URL PAGE_URL = '{slug}.html' PAGE_SAVE_AS = PAGE_URL CATEGORY_URL = '{slug}/index.html' CATEGORY_SAVE_AS = CATEGORY_URL TAG_URL = 'tag/{slug}.html' TAG_SAVE_AS = TAG_URL TAGS_SAVE_AS = 'tag/index.html' 十四、指定文章或页面URL 在需要指定URL的文章或者页面中包括两个元数据url与save_as，例如： url : pages /url/ save_as : pages /url/i ndex . html *这个代码指定了本篇文章的url为pages/url/index.html ​ ​ 根据上面很容易推断如何将一篇文章设置为网站的主页，如下代码即可实现将 content/pages/home.md 设为主页： Title: [www.yanyulin.info](http://www.yanyulin.info) Date: 2014-01-08 URL: save_as: index.html *另外还可以通过template:关键字来指定要使用的模板。 十五、独立域名设置 详见：http://www.jianshu.com/p/252b542b1abf Godaddy上购买专属域名，用dnspod进行动态域名解析，步骤如下: 步骤1：修改Godaddy中的NameServers的两个地址为dnspod的DNS地址： f1g1ns1.dnspod.net f1g1ns2.dnspod.net 步骤2：在Dnspod中添加一条A记录，指向Github URL username.github.io 步骤3：在Pelican主目录，即上面创建的blog/output/username.github.io目录，添加CNAME文件，在文件中添加你的独立域名。 ​ ​ *注意这里的CNAME建议放在第十二步提到的content目录下的静态子目录 content/extra 下，并在配置文件中添加相关条目。 十六、相关文章、上下文导航 1.打开pelicanconf.py，定义插件目录和启用插件： #加载plugins PLUGIN_PATH = \"plugins\" PLUGINS = [\"sitemap\",\"neighbors\",\"related_posts\"] #sitemap SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.7, 'indexes': 0.8, 'pages': 0.5 }, 'changefreqs': { 'articles': 'monthly', 'indexes': 'daily', 'pages': 'monthly' } } #相关文章 RELATED_POSTS_MAX = 10 2.邻居导航，在主题模版中调用如下代码，可根据自己的情况修改： <div class= \"pagination\" > <ul> {% if article.prev_article %} <li class= \"prev\" ><a href= \" {{ SITEURL }} / {{ article.prev_article.url }} \" > ← Previous </a></li> {% else %} <li class= \"prev\" ><a href= \"/\" > ← Previous </a></li> {% endif %} <li><a href= \"/archives.html\" > Archive </a></li> {% if article.next_article %} <li class= \"next\" ><a href= \" {{ SITEURL }} / {{ article.next_article.url }} \" > Next → </a></li> {% else %} <li class= \"next\" ><a href= \"/\" > Next → </a></li> {% endif %} </ul> </div> 3.相关文章： {% if article.related_posts %} <h4> Related Articles </h4> <ul> {% for related_post in article.related_posts %} <li><a href= \" {{ SITEURL }} / {{ related_post.url }} \" > {{ related_post.title }} </a></li> {% endfor %} </ul> {% endif %} 十七、最后，一些比较占空间的资源文件（图片、媒体等）可以用七牛来进行存储管理","tags":"Frontend","url":"https://jiang-hao.com/articles/2018/frontend-使用Pelican基于GithubPages搭建博客教程.html","loc":"https://jiang-hao.com/articles/2018/frontend-使用Pelican基于GithubPages搭建博客教程.html"},{"title":"Build and Install OpenDaylight on Ubuntu","text":"Operating System：Linux x64 / Ubuntu 14.04 Prerequisites：Linux system with Java and Maven installed Chinese version is also available at Ubuntu系统下OpenDaylight源码编译安装 STEP 1, Environment Tuning 1. Install Git tool by command line: sudo apt-get install git-core 2. After the installation of Java and Maven, you need to edit a very important file for OpenDaylight which is named \"settings.xml\". It can customize the behavior of Maven at system level. But we often choose to make it in ~/.m2 folder under your home directory to limit it in user scope： (under ~/ directory) mkdir .m2 cp -n ~/.m2/settings.xml{,.orig} ; \\wget -q -O - https://raw.githubusercontent.com/opendaylight/odlparent/master/settings.xml > ~/.m2/settings.xml You can also use this command instead: curl https://raw.githubusercontent.com/opendaylight/odlparent/master/settings.xml --create-dirs -o ~/.m2/settings.xml 3. A new settings.xml file should now appear in the folder you just created, go and check the content, which should be something similar as below： # gedit ~/.m2/settings.xml <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <profiles> <profile> <id> opendaylight-release </id> <repositories> <repository> <id> opendaylight-mirror </id> <name> opendaylight-mirror </name> <url> http://nexus.opendaylight.org/content/repositories/public/ </url> <releases> <enabled> true </enabled> <updatePolicy> never </updatePolicy> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> opendaylight-mirror </id> <name> opendaylight-mirror </name> <url> http://nexus.opendaylight.org/content/repositories/public/ </url> <releases> <enabled> true </enabled> <updatePolicy> never </updatePolicy> </releases> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> opendaylight-snapshots </id> <repositories> <repository> <id> opendaylight-snapshot </id> <name> opendaylight-snapshot </name> <url> http://nexus.opendaylight.org/content/repositories/opendaylight.snapshot/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> opendaylight-snapshot </id> <name> opendaylight-snapshot </name> <url> http://nexus.opendaylight.org/content/repositories/opendaylight.snapshot/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <activeProfiles> <activeProfile> opendaylight-release </activeProfile> <activeProfile> opendaylight-snapshots </activeProfile> </activeProfiles> </settings> STEP 2, Build and Install OpenDaylight Controller Project Please keep in mind that here controller really means core controller without many other additional features, such as WebUI(dlux). As for the development of other features in OpenDaylight and if you need dlux, please refer to next STEP. 1.Create a directory for your project and get the source code of OpenDaylight controller: mkdir openDayLight cd openDayLight git clone https://git.opendaylight.org/gerrit/p/controller.git 2.Specify the version of OpenDaylight you want to build and check: cd controller git checkout stable/lithium //here I specify stable/lithium version git branch 3.Make sure your settings.xml file is right in place. Build the code with Internet connecdtion: mvn clean install -DskipTests 4.Now run your controller: cd controller/karaf/opendaylight-karaf/target/assembly ./bin/karaf 5.And after a while you will enter the OpenDaylight command line mode as shown below: opendaylight-user@root> 6.Some useful command lines to check and install features: feature : list - i //show the features which are already installed feature : list //show all available features (installed ones are marked with \"x\") feature : list | grep < keyword > //show features that contains <keyword> feature : install < feature > //install a <feature> STEP 3, Build and Install OpenDaylight Integration Project Integration Project is more like a framework project which is to integrate all other projects into OpenDaylight. With Integration project you can modify or put your own features under this project directory and test with the controller. 1.Download the Integration source code: git clone https://git.opendaylight.org/gerrit/p/integration.git 2.Get into the integration directory and specify the version you want: cd integration git checkout stable/lithium mvn clean install -DskipTests 3.After previous step is done, you may would like to run the controller: cd integration/distributions/karaf/target/assembly ./bin/karaf 4.Finally you can now begin your development and replace original features with your own ones under this directory, after which you would be able to build and run the controller for testing: username@ubuntu:~/developApps/openDayLight/integration/distributions/karaf/target/assembly/system/org/opendaylight$ ls aaa integration neutron sdninterfaceapp usc bgpcep iotdm nic sfc vpnservice capwap l2switch odlparent snmp vtn controller lacp sxp yangtools coretutorials lispflow mapping tcpmd5 didm mdsal ovsdb topoprocessing dlux nemo packetcable tsdr groupbasedpolicy netconf reservation ttp （END）","tags":"Backend","url":"https://jiang-hao.com/articles/2018/backend-BuildandInstallOpenDaylightonUbuntu.html","loc":"https://jiang-hao.com/articles/2018/backend-BuildandInstallOpenDaylightonUbuntu.html"},{"title":"Windows系统配置WiFi无线热点","text":"Windows 版本： Windows 7/8/10 前提：电脑装配无线网卡 方法：通过Dos命令行创建WiFi热点，然后启用共享 1.首先确定无线网卡是否支持承载网络（Hosted Network）。在管理员模式的CMD窗口输入命令： netsh wlan show drivers 在输出信息中找到 \"支持承载网络：是\" 则继续。 2.确定电脑是否已经配置承载网络。同样在CMD窗口分别输入命令： netsh wlan show hostednetwork netsh wlan show hostednetwork setting=security 如果看到第一条命令输出已经配置好的WiFi热点的SSID，第二条输出WiFi热点的密码，直接跳到第四步。 3.创建WiFi热点。命令行输入： netsh wlan set hostednetwork mode=allow ssid=WiFi名称 key=WiFi密码 完成后提示承载网络设置成功。 4.启用WiFi热点。在命令行输入： netsh wlan start hostednetwork 完成后提示已启动承载网络。 5.启用WiFi热点共享。打开 \"网络和共享中心\"，在窗口左边栏找到 \"更改适配器设置\"，再在窗口右侧找到刚刚创建的WiFi网络，一般显示为 \"本地连接\"，记住。如下所示 在以太网的网络连接图标上点击右键选择属性，在弹出的窗口上方标签栏点击 \"共享\"。然后在下方确认框中启用 \"允许其它网络用户通过此计算机的 Internet连接来连接(N)\"。如下所示，下拉框中选择刚刚创建的WiFi热点，确定，完成。 （完）","tags":"Tools","url":"https://jiang-hao.com/articles/2017/tools-WifiHotSpotOnWindows.html","loc":"https://jiang-hao.com/articles/2017/tools-WifiHotSpotOnWindows.html"},{"title":"Ubuntu系统Apache Maven安装","text":"操作系统：Linux x64 / Ubuntu 14.04 Apache Maven版本：3.3.9 建议预先搭建Java开发环境：详见 《Linux Ubuntu系统下Java开发环境搭建》 1. 前往Apache Maven官网下载最新版本：https://maven.apache.org/download.cgi，本文以apache-maven-3.3.9-bin.tar.gz为例。 2. 在合适的路径下创建文件夹用来存储Maven，本例选择在/opt目录下新建MVN子文件夹。打开Terminal（后文成为T1），输入： cd /opt #进入到opt目录 sudo mkdir mvn #新建一个mvn文件夹 ls #显示成功新建的mvn文件夹 cd mvn #进入mvn文件夹 3.将下载的MVN压缩包拷贝到mvn目录下。新建另一个Terminal窗口（T2）并输入： cd Downloads #进入Downloads文件夹 ls #显示刚刚下载的MVN文件， sudo cp apache-maven-3.3.9-bin.tar.gz /opt/mvn #将文件拷贝到刚刚新建的mvn文件夹中(这里将\"< >\"部分替代为自己对应的MVN文件名，后同) sudo rm apache-maven-3.3.9-bin.tar.gz #删除本目录下的安装包（可选） 4.解压安装MVN，配置环境变量。回到第一个Terminal（T1），输入： ls #显示拷贝过来的MVN安装包 sudo tar -zxvf apache-maven-3.3.9-bin.tar.gz #将安装包解压 ls #显示解压出的MVN文件夹，以及原安装包 sudo rm apache-maven-3.3.9-bin.tar.gz #删除原安装包 sudo gedit /etc/profile #打开etc目录下的profile文件 5.配置全局环境变量。在打开的profile文档末尾添加MVN安装路径（需仔细确认）： #set maven environment export M2_HOME=/opt/mvn/apache-maven-3.3.9 export MAVEN_OPTS=\"-Xmx1024m\" #避免内存溢出错误（可选） export PATH= ${ M2_HOME } /bin: ${ PATH } 6.保存并关闭文档。（注：也可以通过vim 命令编辑etc/profile，打开命令：sudo vim /etc/profile，按 键进入编辑模式， 键退出编辑模式，接着按\":\"再输入\"wq!\"保存并退出；输入\"q!\"不保存退出） 7.启用配置并验证。在Terminal输入： mvn -v 8.显示效果类似如下： Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T08:41:47-08:00) Maven home: /opt/developTools/jvm/apache-maven-3.3.9 Java version: 1.8.0_65, vendor: Oracle Corporation Java home: /opt/developTools/jvm/jdk1.8.0_65/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"3.19.0-25-generic\", arch: \"amd64\", family: \"unix\" （完）","tags":"Backend","url":"https://jiang-hao.com/articles/2017/backend-Ubuntu系统ApacheMaven安装.html","loc":"https://jiang-hao.com/articles/2017/backend-Ubuntu系统ApacheMaven安装.html"},{"title":"Ubuntu系统Java开发环境的搭建","text":"操作系统：Linux x64 / Ubuntu 14.04 Java JDK版本：jdk-8u65-linux-x64.tar.gz 1. 前往ORACLE官网下载最新版本的Java JDK：http://www.oracle.com/technetwork/java/javase/downloads/index.html，默认下载到Downloads文件夹。 2. 在合适的路径下创建文件夹用来存储Java JDK，本例选择在/opt目录下新建JVM子文件夹。打开Terminal（后文成为T1），输入： cd /opt sudo mkdir jvm ls cd jvm 3.将下载的JDK压缩包拷贝到jvm目录下。新建另一个Terminal窗口（T2）并输入： cd Downloads ls sudo cp jdk-8u65-linux-x64.tar.gz /opt/jvm sudo rm jdk-8u65-linux-x64.tar.gz 4.解压安装Java JDK，配置环境变量。回到第一个Terminal（T1），输入： ls sudo tar -zxvf jdk-8u65-linux-x64.tar.gz ls sudo rm jdk-8u65-linux-x64.tar.gz sudo gedit /etc/profile 5.配置全局环境变量。在打开的profile文档末尾添加JDK安装路径（需仔细确认）： #set java environment export JAVA_HOME=/opt/jvm/jdk1.8.0_65 export JRE_HOME= ${ JAVA_HOME } /jre export CLASSPATH=.: $JAVA_HOME /lib: $JRE_HOME /lib: $CLASSPATH export PATH= $JAVA_HOME /bin: $JRE_HOME /bin: $PATH 6.保存并关闭文档。（注：也可以通过vim 命令编辑etc/profile，打开命令：sudo vim /etc/profile，按 键进入编辑模式， 键退出编辑模式，接着按\":\"再输入\"wq!\"保存并退出；输入\"q!\"不保存退出） 7.启用配置并验证。在Terminal输入： java -version 8.显示效果类似如下： java version \"1.8.0_65\" Java(TM) SE Runtime Environment (build 1.8.0_65-b17) Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode) （完）","tags":"Backend","url":"https://jiang-hao.com/articles/2017/backend-Ubuntu系统Java开发环境的搭建.html","loc":"https://jiang-hao.com/articles/2017/backend-Ubuntu系统Java开发环境的搭建.html"},{"title":"Install & Run Gedit from Terminal on Mac","text":"App description: gedit (App: gedit.app) App website: https://wiki.gnome.org/Apps/Gedit Install from Terminal 1.Run: ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" < / dev / null 2 > / dev / null ; brew install caskroom / cask / brew-cask 2 > / dev / null 2.After the command finishes, run: brew cask install gedit Now Gedit is installed. Run Gedit from Terminal Solution 1. Add the following line to ~/.profile or ~/.bash_profile: export PATH=/Users/<Username>/Applications/gedit.app/Contents/MacOS/gedit: $ PATH Solution 2. Create a script named gedit in /usr/local/bin and make it executable: a. Create a new file: sudo vim /usr/local/bin/gedit b. Add content as below, save and quit: 1 2 #!/bin/bash /Users/<Username>/Applications/gedit.app/Contents/MacOS/gedit c. Make it executable: sudo chmod 755 /usr/local/bin/gedit (END)","tags":"Tools","url":"https://jiang-hao.com/articles/2017/tools-InstallAndRunGeditOnMac.html","loc":"https://jiang-hao.com/articles/2017/tools-InstallAndRunGeditOnMac.html"},{"title":"网页 BacktoTop 返回顶部按钮的简单实现","text":"在较长的网页页面中往往需要一个固定漂浮在显示屏右下侧位置的返回顶部按钮。 下面介绍一种简单实现： HTML 部分： < body > < a href = \"javascript:void(0);\" id = \"scroll\" title = \"Scroll to Top\" style = \"display: none;\" > Top < span ></ span ></ a > ... </ body > CSS 部分： # scroll { position : fixed ; right : 10 px ; bottom : 10 px ; cursor : pointer ; width : 50 px ; height : 50 px ; background-color : #2E435E ; text-indent : -9999 px ; display : none ; -webkit- border-radius : 5 px ; -moz- border-radius : 5 px ; border-radius : 5 px ; } # scroll span { position : absolute ; top : 50 % ; left : 50 % ; margin-left : -8 px ; margin-top : -12 px ; height : 0 ; width : 0 ; border : 8 px solid transparent ; border-bottom-color : #ffffff } # scroll : hover { background-color : #3498db ; opacity : 1 ; filter : \"alpha(opacity=100)\" ; -ms- filter : \"alpha(opacity=100)\" ; } JavaScript 部分： $ ( document ). ready ( function (){ $ ( window ). scroll ( function (){ if ( $ ( this ). scrollTop () > 100 ){ $ ( '#scroll' ). fadeIn (); } else { $ ( '#scroll' ). fadeOut (); } }); $ ( '#scroll' ). click ( function (){ $ ( \"html, body\" ). animate ({ scrollTop : 0 }, 600 ); return false ; }); }); 最后的显示效果可以参照本博客：）","tags":"Frontend","url":"https://jiang-hao.com/articles/2017/frontend-网页BackToTop返回顶部按钮的简单实现.html","loc":"https://jiang-hao.com/articles/2017/frontend-网页BackToTop返回顶部按钮的简单实现.html"},{"title":"CSS伪类content: url放图片如何改变图的大小","text":"遇到过一个问题：用before after 之类的伪类的content里放一个图的链接，如何去改图的大小？比如： . nav ul li : after { content : url ( ../img/nav_fg.png ); } 事实上，如果想要设置前端页面通过content：url显示出来的图片大小，那只能修改图片的大小。因为它是直接读取的图片，并不是代入到html中再显示出来。但可以采用background的方式调整图片的大小，比如本博客中学术社交网站ResearchGate的Logo的CSS显示代码如下： . social a [ href *= 'researchgate.net' ] : before { background-image : url ( './images/icons/researchgate.png' ); background-size : 100 % ; display : inline-block ; margin-right : 2 px ; vertical-align : -3 px ; height : 16 px ; width : 16 px ; content : \"\" ; } 其中 content=\"\", height, width, background-size, display, background-image 都必须显示指定。这样，就可以通过 height, width 来分别设置图片的长和宽了。","tags":"Frontend","url":"https://jiang-hao.com/articles/2017/frontend-CSS伪类contenturl放图片如何改变图的大小.html","loc":"https://jiang-hao.com/articles/2017/frontend-CSS伪类contenturl放图片如何改变图的大小.html"},{"title":"Install Wireshark on Ubuntu","text":"1. Download WIreshark source code on website: https://www.wireshark.org/download.html 2. Extract files into targeted folder: tar -xvf wireshark-1.10.7.tar.bz2 3. Install compiling tools and dependencies: sudo apt-get install build-essential sudo apt-get install libgtk2.0-dev libglib2.0-dev sudo apt-get install checkinstall sudo apt-get install flex bison sudo apt-get build-dep wireshark sudo apt-get install qt5-default sudo apt-get install libssl-dev sudo apt-get install libgtk-3-dev 4. Go to www.tcpdump.org and find newest version of libpcap(e.g., libpcap-1.5.3.tar.gz): #tar -xvf libpcap-1.5.3.tar.gz #cd libpcap-1.5.3.tar.gz #./configure #make #make install 5. Go to wireshark folder: ./autogen.sh ./configure --with-ssl --enable-setcap-install make make install","tags":"Tools","url":"https://jiang-hao.com/articles/2016/tools-InstallWiresharkonUbuntu.html","loc":"https://jiang-hao.com/articles/2016/tools-InstallWiresharkonUbuntu.html"},{"title":"法国的一些著名的精油品牌","text":"法国精油品牌： Florial (Florihana) (芙梦丽娜) L'OCCITANE ( 欧舒丹 ) Sanoflore (圣芙兰) CAMENAE (嘉媚乐) NUXE ( 欧树 ) SISLEY ( 希思黎 ) 1. Florial (Florihana) —— F家 品牌： 在欧洲被视为法国第一医疗等级的有机认证植物精油品牌，多年来一直深受着各国芳疗界的喜爱。拥有世界上独一无二的特殊蒸馏技术，采用人工采集和产油率极低的低温、低压、缓慢的蒸馏技术来提炼精油。包装瓶选用一种特殊的绿色玻璃来装载精油，此种瓶子不但可阻挡一般的阳光，还有过滤紫外线和红外线的作用，方便精油更好的保持原型状态。此外，F家的有机精油品种丰富，其中尤其纯露做的非常好，属于世界顶级行列。唯一的一点瑕疵是这家的产地有些并不是非常好（其实也很好理解，算是多品种的机会成本吧）。 公司： J.E Internatioanl (Florihana、Florial商标所有人)公司总部设立在法国的南部，于国家级自然保护区内的科斯高山（Caussols plateau）上。创办人MR. Durante Alain 系工程师出身。自1993年开始从事精油萃取及芳疗推广事业，用近15年的时间，终于创立出Florial和Florihana品牌。 明星产品： 永久花纯露 ( HELICHRYSUM ITALIAN ORGANIC ) 标签： 法国知名精油品牌 2. L'OCCITANE—— 欧舒丹 品牌： 全称L'OCCITANE EN PROVENCE（法语发音：[lɔksiˈtan ɑ ̃pʁɔvɑ̃s]，意即\"普罗旺斯的欧舒丹\")。产品是以普罗旺斯丰富的自然草本植物为原料，用优质的植物由传统方法制造开发而成。每一个产品系列都有其独特传奇：薰衣草、橄榄、马鞭草及蜂蜜节发扬法国普罗旺斯的传统，蜡菊细诉科西嘉岛的故事，乳木果油是非洲的传奇瑰宝。今天， 欧舒丹已成为在全球近60个国家开设逾550家分店的国际知名香氛护理品牌。 公司： 是一家专门制造及售卖个人护理产品及家居产品的国际零售企业，主要生产基地设于法国马诺斯克（Manosque）。 公司成立于1976年， 创办人为奥利维埃·博桑（Olivier Baussan）， 他希望建立一家保留并弘扬家乡普罗旺斯传统的公司。公司名字L'OCCITANE在法语中的意思是\"来自欧西坦尼亚的女人\"。欧舒丹希望\"透过提供独特的个人护理及家居产品，成为提倡地中海式舒适安康感觉的国际模范\"，并以\"舒适愉悦\"、\"真实纯净\"及\"关怀尊重\"等为企业理念。 明星产品： \"Aromachology\" 芳香身心疗法系列：以多种精油混合制成，将\"冥想\"的意境贯穿其中，能帮助身体与大自然沟通。整个系列有沐浴、洗发、润肤、按摩油和蜡烛等产品，其中以沐浴品最为畅销。 标签： 法国知名护理品牌 3. Sanoflore —— 圣芙兰 品牌： 法国芳香界一个很有地位的品牌，所有的原料都采自以有机种植的方式培育出的植物，是全球唯一全系列商品皆获得欧盟Ecocert与Cosmebio有机认证的品牌。SANOFLOR还有很多美容护肤类的产品如：玫瑰面油，按摩油，日霜，晚霜，眼霜等都用的有机植物的精华，而且最值得一提的是：它家产品里凡是需要添加水份的，都是添加的100%有机的花水。SANOFLORE共有6种花水，玫瑰，薰衣草，洋甘菊，橙花，金缕梅，矢车菊。目前，SANOFLORE产品涉及花草类（花草茶，花草药包）、芳香疗法（精油，花水）、有机美容护肤品和天然香料。 公司： 全世界最大有机植物原料供机商，就连法国SISLEY、DARPHIN，甚至是澳洲AESOP的精油成分及原料都来自于它。Sanoflore法文的原意指的是：\"健康的花朵\"。1972年，瑞士籍的地球及自然科学与地质学与商业管理研究所博士RodolpheBALZ，在法国普罗旺斯Drome的核心地带一片零污染的山丘上成立了第一座实验性的花园SanofloreLaboratory，生产出第一瓶花水；1986年，SANOFLORE正式成立了专门研究有机产品的实验室；1998年，Daniel RICHARD家族加入经营，将SANOFLORE介绍给全世界。 明星产品： 花水系列 标签： 法国知名精油品牌，世界最大有机植物原料提供商 4. CAMENAE —— 嘉媚乐 品牌： 源自法国普罗旺斯纯粹的人文精髓的化妆品品牌，应该算是最接中国\"地气\"的法国精油品牌之一了。自2003年进入中国市场以来，在一年的时间不下200余家专卖店和专柜相继开业，至今CAMENAE应该算是国内精油护肤品领域的佼佼者。2007年CAMENAE嘉媚乐成功获得欧盟ECO-CERT国际有机认证。品牌理念：善用精油，关注心灵，回归天然。 公司： CAMENAE，法国知名精油品牌，1970年创立于法国的普罗旺斯。从1970年创立起，至今已发展成为拥有逾3000家门店的超级连锁体系。2003年8月，CAMENAE在北京国贸开张了第一家中国旗舰店，宣告这一欧洲个人护理品牌正式挺进中国，同时启动中国区特许经营项目。 明星产品： \"橄榄精华系列\" 标签： 法国知名护理品牌，中国知名品牌 5. NUXE —— 欧树 品牌： 法国纯植物美容品牌，名字取自NATURE（自然）+LUXE（奢华），意喻该品牌是自然与奢华的完美的结合。NUXE 崇尚自然，创立伊始便坚持独创，注重优选成分，不一味追逐潮流，成为天然美学，回归自然的领导品牌。品牌理念：自然和大胆，效率和真实，性感与奢华，力求将植物的功效发挥到极致，结合\"自然、健康、美丽\"三大护肤要素，共同复兴先锋化妆品理念。在媒体和娱乐圈内对其给予的评价颇高。 公司： 由巴黎的一位药技师兼芳疗及植物疗法专家在1957年所创立，成立以来，研发了许多简单、温和、有效的美容保养品。企业精神：超越流行的限制，而秉持着绝对独立自主。1989 年由崇尚植物与芳香疗法的知名法国女企业家Aliza Jabes收购，经医疗级别的精密制作过程和严密的企业行销与品质塑造，更促使NUXE成为天然美学的教主品牌。 明星产品： 蜂蜜洁面系列，睡莲面霜，植物鲜奶面霜，花水系列等 标签： 法国知名纯植物美容品牌 6. SISLEY —— 希思黎 品牌： 品牌名称Sisley来自於19世纪法国印象派画家Alfred Sisley，因此可以想像这是个流著热血以及多情艺术的品牌。Sisley所有产品均以植物萃取精华与植物香精油作为主要成分，配以其独有配方研制而成，在品质、绝对的安全性和适用性上享有盛誉。产品主要涉及护肤、彩妆、美发和香水（希思黎夜幽情怀女士香水）。理念和定位：\"用科学印证植物美容的传说，以优质尊贵的产品，献给讲究的人士\"（通俗的说就是坚持高品质，然后高价位，护肤品中尊贵与优雅的经典代表）。 公司： 在1976年由法国修伯特．多纳诺伯爵创立，以当时欧洲最新的植物美容学为基础，成功地研创出来的植物性护肤品牌。是现今极少数仍由家族拥有、经营的化妆品品牌。全世界所有的Sisley护肤产品均出自法国巴黎工厂，以确保品牌产品的一致性。产品上市前必先经过皮肤科医生的测试，以确保产品的安全性。Sisley是少数被皮肤医学家认定为最安全的护肤品；即使是过敏性与敏感性肌肤，也可安心使用。 明星产品： Ecological Compound，Botanical D-TOX Detoxifying，Express Flower Gel, etc. 标签： 法国知名植物美容品牌 参考: 1. 十六番：法国购物攻略 2. 各品牌百度百科 3. 各品牌Wikipedia 4. 新欧洲.战斗在法国 5. 各品牌官网","tags":"Notes","url":"https://jiang-hao.com/articles/2016/notes-法国的一些精油品牌.html","loc":"https://jiang-hao.com/articles/2016/notes-法国的一些精油品牌.html"},{"title":"Install VMware Workstation in Ubuntu 14.04","text":"1. Use apt-get update system: linuxidc@localhost:~$ sudo apt-get update linuxidc@localhost:~$ sudo apt-get upgrade 2. Download VMware Workstation bundle. 3. Make bundle executable: linuxidc@localhost:~$ chmod a+x VMware-Workstation-Full-11.0.0-2305329.x86_64.bundle 4. Run the bundle: linuxidc@localhost:~$ sudo ./VMware-Workstation-Full-11.0.0-2305329.x86_64.bundle 5. Installation:","tags":"Tools","url":"https://jiang-hao.com/articles/2016/tools-InstallVMwareWorkstationinUbuntu14.04.html","loc":"https://jiang-hao.com/articles/2016/tools-InstallVMwareWorkstationinUbuntu14.04.html"},{"title":"Apache Shiro Multi-tenancy with JDBC MySQL","text":"0. GOALS - Provide user authentication and authorization for multi-tenant scenarios - Provide easy-to-maintain data storage with only one database - Provide domain-differentiated, credential-based authentication - Provide domain-differentiated, role-based, service-specific authorization; 1. QuickStart with Apache Shiro - English Documentation - Chinese Documentation - Example Applications - Architecture: 2. Environment Prerequisites Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T17:41:47+01:00) Maven home: /Library/Maven Java version: 1.7.0_79, vendor: Oracle Corporation Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"10.11.2\", arch: \"x86_64\", family: \"mac\" 3. Apache Shiro Maven Dependency <dependency> <groupId> org.apache.shiro </groupId> <artifactId> shiro-core </artifactId> <version> 1.1.0 </version> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-simple </artifactId> <version> 1.6.1 </version> <scope> test </scope> </dependency> 4. MySQL QuickStart 1. Install MySQL on Mac 2. 修改Root密码 3. W3Schools SQL Tutorial 4. W3School中文SQL教程 5. RUNOOB MySQL教程 5. Create Tables 1.User Table: 2.Domain_User Table: 3.User_Role Table: 4.Permission_Domain_Role Table: 6. JDBC Setup and Code Customization 1.Maven Dependency: <dependency> <groupId> mysql </groupId> <artifactId> mysql-connector-java </artifactId> <version> 5.1.25 </version> </dependency> <dependency> <groupId> com.alibaba </groupId> <artifactId> druid </artifactId> <version> 0.2.23 </version> </dependency> 2.Rewrite getPermissions method in jdbcRealm protected Set < String > getPermissions ( Connection conn , String username ) throws SQLException { PreparedStatement ps = null ; Set < String > permissions = new LinkedHashSet < String >(); try { ps = conn . prepareStatement ( permissionsQuery ); ps . setString ( 1 , username ); ResultSet rs = null ; try { // Execute query rs = ps . executeQuery (); // Loop over results and add each returned role to a set while ( rs . next ()) { String permissionString = rs . getString ( 1 ); // Add the permission to the set of permissions permissions . add ( permissionString ); } } finally { JdbcUtils . closeResultSet ( rs ); } } finally { JdbcUtils . closeStatement ( ps ); } return permissions ; } 3.Add getUserDomain method in jdbcRealm public Set < String > getUserDomain ( Connection conn , String username ){ PreparedStatement ps = null ; Set < String > domains = new LinkedHashSet <>(); try { ps = conn . prepareStatement ( userDomainQuery ); ps . setString ( 1 , username ); ResultSet rs = null ; try { rs = ps . executeQuery (); while ( rs . next ()) { String domainID = rs . getString ( 1 ); domains . add ( domainID ); } } catch ( SQLException e ) { e . printStackTrace (); } finally { JdbcUtils . closeResultSet ( rs ); } } catch ( SQLException e ) { e . printStackTrace (); } finally { JdbcUtils . closeStatement ( ps ); } return domains ; } 4.Rewrite doGetAuthenticationInfo method in jdbcRealm protected AuthenticationInfo doGetAuthenticationInfo ( AuthenticationToken token ) throws AuthenticationException { VTNAuthNToken upToken = ( VTNAuthNToken ) token ; String username = upToken . getUsername (); String domainID = Integer . toString ( upToken . getDomainId ()); // Null username is invalid if ( username == null ) { throw new AccountException ( \"Null usernames are not allowed by this realm.\" ); } Connection conn = null ; SimpleAuthenticationInfo info = null ; try { conn = dataSource . getConnection (); Set < String > domains = getUserDomain ( conn , username ); if (!( domains . contains ( domainID ))){ throw new AuthenticationException ( \"Domain not found\" ); } String password = null ; String salt = null ; switch ( saltStyle ) { case NO_SALT : password = getPasswordForUser ( conn , username )[ 0 ]; break ; case CRYPT : // TODO: separate password and hash from getPasswordForUser[0] throw new ConfigurationException ( \"Not implemented yet\" ); //break; case COLUMN : String [] queryResults = getPasswordForUser ( conn , username ); password = queryResults [ 0 ]; salt = queryResults [ 1 ]; break ; case EXTERNAL : password = getPasswordForUser ( conn , username )[ 0 ]; salt = getSaltForUser ( username ); } if ( password == null ) { throw new UnknownAccountException ( \"No account found for user [\" + username + \"]\" ); } info = new SimpleAuthenticationInfo ( username , password . toCharArray (), getName ()); if ( salt != null ) { info . setCredentialsSalt ( ByteSource . Util . bytes ( salt )); } } catch ( SQLException e ) { final String message = \"There was a SQL error while authenticating user [\" + username + \"]\" ; if ( log . isErrorEnabled ()) { log . error ( message , e ); } // Rethrow any SQL errors as an authentication exception throw new AuthenticationException ( message , e ); } finally { JdbcUtils . closeConnection ( conn ); } return info ; } 7. Setup Shiro.ini Configuration File [main] #authenticator authenticator = aaa.authn.VTNAuthenticator #(Customized) authenticationStrategy = org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategy authenticator.authenticationStrategy = $authenticationStrategy securityManager.authenticator = $authenticator #authorizer authorizer = aaa.authz.VTNAuthorizer #(Customized) permissionResolver = org.apache.shiro.authz.permission.WildcardPermissionResolver authorizer.permissionResolver = $permissionResolver securityManager.authorizer = $authorizer #Realm jdbcRealm = aaa.realms.MySQLRealm #(Customized) dataSource = com.alibaba.druid.pool.DruidDataSource dataSource.driverClassName = com.mysql.jdbc.Driver dataSource.url = jdbc:mysql://localhost:3306/vtn dataSource.username = root jdbcRealm.dataSource = $dataSource securityManager.realms = $jdbcRealm jdbcRealm.permissionsLookupEnabled = true #SQL Queries jdbcRealm.authenticationQuery = SELECT password FROM user WHERE user_name = ? jdbcRealm.userRolesQuery = SELECT role_id FROM user_role left join user using(user_id) WHERE user_name = ? jdbcRealm.permissionsQuery = SELECT distinct permission_id FROM perm_domain_role left join domain_user using(domain_id) left join user using(user_id) WHERE (domain_id, role_id) IN ( SELECT domain_id, role_id From user left join user_role using(user_id) left join domain_user using(user_id) WHERE user_name = ?) 8. Tips about SQL permissionsQuery in INI file: 1. Query with same parameter at two places: SELECT T . P FROM ( SELECT distinct permission_id as P , role_id AS R FROM perm_domain_role left join domain_user using ( domain_id ) left join user using ( user_id ) WHERE user_name = ? ) AS T WHERE T . R IN ( SELECT role_id FROM user_role left join user using ( user_id ) WHERE user_name = ? ) 2.Optimization with only one parameter: SELECT DISTINCT permission_id FROM perm_domain_role left join domain_user using ( domain_id ) left join user using ( user_id ) WHERE ( domain_id , role_id ) IN ( SELECT domain_id , role_id FROM user left join user_role using ( user_id ) left join domain_user using ( user_id ) WHERE user_name = ? ) 9. Result: Finally, you can easily test that each user has different services authorized by Shiro according to its tenant domain and its role. for ( VTNAuthNToken token : userTokenList ) { Mappable userRequest = new MappableMsg ( null , null , token ); for ( String service : servList ){ userRequest . setServID ( service ); if ( IShiro . getInstance (). isAuthorized ( userRequest )){ String entry = \"Domain \" + token . getDomainId ()+ \": \" + token . getUsername ()+ \": \" + service ; authZResult . add ( entry ); } } } for ( String i : authZResult ){ System . out . println ( i ); } Output: Domain 1: admin: vtn:topo:create Domain 1: admin: vtn:topo:read Domain 1: admin: vtn:topo:update Domain 1: admin: vtn:topo:delete Domain 1: admin: system:vtn:create Domain 1: admin: system:vtn:update Domain 1: admin: system:vtn:delete Domain 1: admin: serv:firewall:create Domain 1: admin: serv:firewall:read Domain 1: admin: serv:firewall:update Domain 1: admin: serv:firewall:delete Domain 1: boss: system:vtn:read Domain 2: tenant1: vtn:topo:create Domain 2: tenant1: vtn:topo:read Domain 2: tenant1: vtn:topo:update Domain 2: tenant1: vtn:topo:delete Domain 2: tenant1: serv:firewall:create Domain 2: tenant1: serv:firewall:read Domain 2: tenant1: serv:firewall:update Domain 2: tenant1: serv:firewall:delete Domain 2: guest1: vtn:topo:read Domain 2: guest1: serv:firewall:read Domain 3: tenant2: vtn:topo:create Domain 3: tenant2: vtn:topo:read Domain 3: tenant2: vtn:topo:update Domain 3: tenant2: vtn:topo:delete Domain 3: guest2: vtn:topo:read","tags":"Backend","url":"https://jiang-hao.com/articles/2016/backend-Shiro.html","loc":"https://jiang-hao.com/articles/2016/backend-Shiro.html"},{"title":"在难搞的日子里他们这样对你说","text":"城市：雷恩 2016年2月1日晚，距国内过新年还有7天。原本只是想要找一封早时候的项目电邮，结果不小心没把持住，泛滥着感慨时光飞逝的小情绪翻阅了一遍以往的邮件，感觉就像是在看旧照片，就像把从大一开始到现在的生活重新走了一遍。至今有很多需要感谢的人，值得纪念的事，我不想在忙碌中就这样让它们随时间流走。因为手里的东西总是忙不完，那些让你成长的却不总有机会不被遗忘。 2012-09-01/2013-08-27 大三 2012年大二暑假，由于本专业大类被限制转专业，我决定通过考取思科认证来自学网络技术。11月，虽然国创(SIETP)项目才刚立项一个月，SRTP项目已经立项半年，但是学习的重心从十月份起已经向网络方向偏移，整天脑子里除了思科的路由交换就是国创的建筑涂料，闲暇之余才能稍微顾及一下专业课程实习和考试，而SRTP项目已经打算彻底水漂过去了。这个过程持续了一年，准备出国又花了一年，期间发生了许多事——得到了许多人的帮助，也犯了不少傻事。 2012-09-Inquiry to Prof. Bai 我希望在本科毕业后往网络工程方向走。我的专业是农业资源与环境，但出于个人兴趣并且经过了慎重考虑后，我想在大学接下来的两年内考取CCIE，可自己目前几乎是零基础，暂时打算到欧朋兰博先报一个CCNA的培训班作为入门。我有一位高中同学（专业与计算机网络无关）在今年暑假拿到了CCIE认证，虽然有了他的建议和引导可以少走些弯路，但他毕竟不在自己身边许多事情还是需要自己去摸索。下面是我想咨询您的问题： 1. 学校有开设相关的培训课程吗？如果没有我又该如何充分利用本校的设备或者资源呢？ 2. 浙大有与CCIE对口的网络工程方面的硕士学位吗？或者其它与CCIE相关的深造项目或计划？ 3. 同学说CCIE的工作经验（项目资历）很重要，花两年时间去读研和工作那种更利于自己今后发展？ 4. 不考虑能力问题，拿到CCIE后出国是否是最佳选择？如果这就是我的规划，自己应该早作哪些准备？ 建议: 我印象中没有这种培训，不过为保险起见你可以打电话问问计算机学院办公室。社会上倒是有，但培训费用有点贵。我记得CCIE认证有面试的，要求当场调试网络的，所以肯定要有设备进行实验，但是学校机房里并没有这些设备的，估计要去问问计算机学院的相关实验室，具体哪里有我也不清楚。CCIE只是一个认证，硕士学位的培养不可能只局限于此的。具体有关计算机学院的硕士专业方向你可以查询计算机学院网站。CCIE认证拿到一般就是去工作赚钱了吧，没有必要再去读什么研了。因为读研主要就是学术方向，CCIE主要还是动手能力。如果出国去工作的话，是可以考虑的，不过去大城市如上海也一样可以赚大钱；如果出国去读研，那CCIE没有什么用。我印象中前几年CCIE很吃香，因为拿到这个认证的人非常少，现在什么行情我就不太清楚了。我觉得你以后的方向不能仅局限于这个CCIE，可以考虑把面放得更广一些，搞计算机主要是时效性太短，学东西花了很多精力很快就淘汰了，又得不断地学新东西，很累的。这一点不像医学、外语，可以吃老本。 2013-05-Inquiry to Prof. Wu 我对网络技术很感兴趣，今后想要成为一名网络工程师。但在就业之前，我渴望自己能够是\"CCIE+网络工程硕士\"。但是我在网络工程领域的硬件储备基本为零，而作为敲门砖的CCIE证书，虽然我在很努力地准备，但也要等到暑假才能考到。我想申请贵院\"本校其他专业免试研究生\"的资格…… 虽然你以前的专业和你想转换的不同，但你对网络研究方向感兴趣的话肯定能有所建树的，我倒很愿意推荐你来我实验室研究、学习。你的英语考级情况如何？保送研究生的资格是否可以争取到？你们学院的老师我倒有些熟悉的，可以寻求一些帮助的。你若情况明朗的话可以来我实验室学习了，了解一下基础研究情况，并推荐你看一些基础性的教程，并与师兄们讨论、求教，相信你会很快有所提高的。我实验室在研的下一代网络前沿技术今后将会非常之普及的，这方面人才的储备的含金量会愈来愈高的。建议可以进一步进行硕博连读的计划。近期有诸多大公司来我们这里合作，其中需要我们这个方向的博士研究生，我这里在学院申请到了3位硕博连读的招生指标，其中1个我想到时给你，这样你后面的研究工作和今后的就业就比较吻合一致的，目标也更加清晰了；另外需要到国外进修的话我这里也是比较便捷的，美国加州大学、西北大学、阿拉巴马大学都有老师在这里合作研究。硕博连读的保研可以申请的吧，目前说要招收导师的确认方可。 2013-05-Kind Remind from Prof. Pan 你花了四年学专业, 却要从零开始学网络, 这种执业证书计算机专业学生不看重, 培训机构兼钱看重，只是浪费了你爸的血汗钱！真正要学网络要学一个计算机学科的知识，改行而出成绩要比別人化多倍的精力和辛勤汗水！行行出状元, 为何要换专业？新材料研究是一很需要方向！ 因此，要三思而行，除非所学专业的确很难找到工作？！ 2013-09/2014-06 大四 2013-09-Inquiry to Dr. Ning 暑假期间我还在备考，所以没来得及联系您，前不久刚刚考完，下一步打算着手准备出国。出国的这个决定下得比较仓促，所以对出国资金，选校定位，文书套磁，奖学金等都认识不深，有关的一些准备工作以及学业规划也并不成熟。因此希望能听取你的指点和引导，从而少走弯路。我以后打算从事IT（网络）方面的工作，所以希望最好能够直接申请到美国CS方面的硕士OFFER，但是由于是跨专业，而且托福和GRE才刚刚开始准备，所以时间非常紧张，并且应该可能会要拖个半年一年，来准备英语成绩，选修计算机专业课，还有积累相关的项目或者实习经历。如果英语成绩能够顺利考出来的话，今年12月底到明年1月份初试申几所院校，如果计划不顺利或者录取结果不理想的话再等明年春季那一批招生，并继续冲刺英语考试，同时可以去积累一下实习经历或者可能的话跟着计院老师获取科研经历。关于费用的问题，我们家里的经济条件并不宽裕，所以我和爸妈非常关心这一点，当然选校的时候也必须要考虑这一点。但是目前为止我对留学美国的花费还没有一个非常清楚的认识和概念，您可以向我具体的介绍一下吗？鉴于经费的问题和选择的空间，听同学说加拿大的花费稍微低一些，我是否有必要也同时申请加拿大的院校呢？另外同时以自己本科的专业去申请几所院校的环境类专业，然后再视情况考虑转到CS是否是一种可能的合理的选择？ 建议： As you understood, your background is inconsistent with what you try to pursue and your family is not able to provide full financial support for your graduate study oversea. In general, the cost for each credit of a graduate course is about $1000 - $2000 for international students and at least 12 credits are required for a full-time graduate student. Lodging and boarding costs additional $10k per year. That's reality. So you may think about it around the way or alternate approach(es). \"以自己本科的专业去申请几所院校的环境类专业，然后再视情况考虑转到CS\" might be the alternate approach, through which you could get a school financial support. I like your attitude for planning your long-run goals. For now, however, you should concentrate on getting as high as possible scores on TOEFL and GRE, which will help you get admitted into a college graduate program and financial support from the college. Once you get high enough scores with them, you can think of what colleges and programs you should apply for. 关于套磁, you should have done certain research on particular professional fields through these you should frequently refer the articles published in professional journals. Then you can naturally get in touch with those authors of the articles to inquire their recent progress, etc. 2013-10-Inquiry to Mr. Li 很抱歉近期没能关注到Autodesk公司在我们学校的校招情况，对贵公司的职位需求不太了解，所以希望先把简历发给您并同您建立联系，望前辈多多批评指正。 建议： ……不过你开发编程方面经历较少。如果我这个理解没错，而且你又希望在IT业方面尝试一下发展的话，看来你比较适合在公司的IT部门工作，进行网络方面管理，不知道你是否同意。我们公司最近招人并不多，每两周会发布一次职位需求列表，最近的一次没有看到IT部门的职位，我会留意邮件，如果有合适的职位一定会帮你推荐。不过现在十月份，距离毕业还早，你大可以多尝试一些其他更加对口的公司，比如思科华为这些。或者准备考研（如果你想的话）。有问题就给我写信，祝好运！ 2013-11-Inquiry to Mr. Zhuliu & Mr. Yue 10月底已经拿到华为的offer了，但是还是非常想出国念IT，所以虽然转专业难度比较大，但还是想考英语再全力去试着申一申，如果实在时间来不及或者结果不合适的话就只能到华为工作几年再出国了。浙大这边有个和KTH的4+2的项目我已经报名了，12月17日对方会来我们学校面试，希望顺利吧!以上是我目前的状态和打算。CV就拜托你了，另外就你对美国大学的了解，能否对我的转专业申请提一些建议，比如说美国学校非常注重什么；我还可以怎样去弥补转专业这种情况带来的劣势；还有申硕士的话套磁对申请结果会有很大的影响吗，（比如要是我的毕设课题是软件定义网络SDN这一块，而某个教授也是研究这一块）。 Zhuliu： 在你面前有很多很好的选择，我相信无论你走哪一条路，都会一如既往的精彩。我自己不是跨专业申请的，只是借原来的专业入了学校的门，然后又背弃师门，转行做码农。所以对于你申请CS专业，我的帮助比较有限。我跟一个CS的同学讨论了你的情况，大家总结了一些想法。 第一，跨专业申请，尤其是跨度较大的申请确实难度很大，因为导师会顾虑学生的专业基本功。但同时，学校和院系也是很欣赏多样性的。我们这里有EE教授研做心理学课题的，语言学教授做自然语言计算处理的。包括我的那位同学，他本科是机械系做机器人和电路的。所以跨专业申请，硬攻CS的话，是要打diversity的牌。我觉得这点你已经做得比较好了，做了CS与你本专业结合的课题。我也相信CS技术可以在你所熟知的专业领域里有很多美妙的应用。（我自己对于GIS也很感兴趣，现在python的兴起正在改变Earth Science整个学科的面貌，很多地理信息的教授都感觉自己落伍了，在学校里找CS相关的学生做研究助理）。跨专业或许是一个短板，但如果你把它与CS结合得好，突出自己跨学科解决问题的能力和学习能力，并且强调自己应用CS的学术深度，就可以扬长避短。兴许有教授恰好有合你胃口的跨专业项目，这样就最好了。 第二，也得考虑\"曲线进入\"的方式。CS核心专业的录取量不大，而且竞争激烈。但以别的专业进入CS强校，不失为一种办法。美国大学有很大的自由度。尤其是对于硕士学生，只要修了符合要求的课程，就能转入CS相关的专业（有时候EE反而比较灵活合包容，现在EE的也都在coding）。 我身边有同学刚进来就修读CS的课程，第二年顺利从civil转到EE。也有同学直接从civil转到了CS。申请的时候违心地\"欺骗\"一下老师的感情也是无妨的。只要不是骗phd funding，套个硕士录取，然后进来就跳，我觉得也是无可非议的。从这个角度考虑，申请你的本专业（与CS结合的更好），或者CS边缘的专业（比如CMU有不少非CS核心的专业可以考虑），或者EE的某些方向（比如网络）可能会增加你进入学校的把握。 综合这两条，你的申请思路可以是两者结合的。很多学校都是以院系为单位招生录取的。所以你可以准备两种倾向性的材料分别投给CS和其他学院。只要有一种方式成功，便是胜利。 对于申请CS专业的CV和PS，你要把自己的主体内容写得跟科班出身的人差不多。你可以简要梳理一下你的专业基础，比如，会哪几门语言，分别什么程度（proficient, experienced)。算法设计的课程或项目。Object Oriented Programming方面的经验。还有就是CS某个子领域的研究。比如你擅长network那块，你可以把相关的课程经历和具体的project都梳理到一起。你能受到华为的青睐，肯定有拿得出手的相关项目，这个子领域的学识至关重要，要突出强调这个，让教授觉得你有干货，并且有志于研究下去（以申请phd的态度申请硕士）。教授在确定了你基础过关的情况下，就是看志趣相投了。然后，还是那一点，科班一样的简历之外，也不可能回避本专业的内容。这个和找工作不一样，我可以只字不提civil专业，说自己是Engineering的。学术录取对于你既往的专业还是会调查的详细清楚的。这就需要像前面所说的那样扬长避短，你要强调你的专业学习中融入的CS是多么的有深度。以你本专业做外壳，大讲你的CS内涵。 而对于申请与你专业比较相近的方向，则还是以本专业为坚实基础，CS作为你的拿手好戏。你取得了本专业的学位，有优秀的成绩，而且做了很多跨学科的课题都非常时髦。是的，现在美国整个学术圈都在追求量化和模拟，你的coding技能可能是你本专业很多老板所垂涎的。我想这种类型的简历，你写起来应该得心应手。再有一点就是投其所好。如果你是把CS作为锦上添花的，那么可以略写专业术语和具体内容，强调实用和宽度（怎么在你的专业里用的好）。比如你的Sql DB的经历对于GIS等信息类但非CS专业，是很加分的。但是对于CS专业，就要浓缩到你的干货上去（你的network方向），因为看简历的对方是行家，他所关心的是子专业的匹配。你最强势的，甚至有可能作为phd课题的CS项目就要详细的写，突出亮点。次要的可以省略和带过。 对于你现有的简历，我觉得这里已经有足够丰富的素材了，充分证明了你的勤奋和才能。接下来的就是为两种战术挑选素材, 量身定做了。至于找CS专业简历的模板，你可以google到很多专业人士的（码农的CV很多），你可以模仿他们的笔调去写自己内容，如果顺带用一下latex写的话，给人专业的感觉（这个不是很重要）。还有一点，除非是对方认识的知名教授，一般都不必列举你的指导老师的名字。这样占篇幅，而且更给对方疏离感，这里你是明星！还有详略的控制。要做的精简的基础上，让对方大概了解到你做了什么工作，用了什么技术。比如你的MySQL项目，你指出的GUI，一看就很明显了，确实不必多解释，但如果你说你用java swing或者php，ruby做web interface那就更令人信服。但main funciton modules就是很含糊的概念，你是负责DB和你们的program交互的module，还是数据与DB交互前的其他处理（如果是，那么可能用到什么核心算法或数据结构，你也可以做文章）。同理，你在implement intellecutal queuing system时候的主要解决的难题是什么，核心算法借鉴了什么。这些都是可以套用术语简要的交代清楚的。另外，你用动词开头的描述很好，确实是这样的规范。可以改进的地方在于把动词替换成高级的同义词，比如把helped换成assisted。 Yue： 华为offer比较不错，实在不行先在国内工作下也还好，国外很多人都是有几年工作经验再读硕士的。我稍微看了下，标注了一些地方可以改下，但还建议你可以自己再仔细修改，并且和更多人一起相互修改，然后找更专业的人士帮忙修改。另外注意写经历时尽量能量化成果，以便更好地differentiate yourself。还可以在网上搜下顶尖美国大学的CV模板，对照着自己的改下。你要转到CS/IT相关方向是吧，建议你在简历里再加上你熟练运用的编程语言、计算机软件吧。我感觉申美国硕士现在不是特别难，美国高校最近招硕士比较多（水）一些，虽然未来扩招的趋势不太可能持续。要转专业的话，相关专业的课程、科研项目、竞赛成果、实习工作经验、CS好教授的推荐信可能会比较有帮助（最好这些都是在国际上认可的），这些估计美国学校都会看重。陶瓷我估计对申硕士帮助不大，如果你想过去跟老师做点课题、争取RA的话，还是可以套的。 2014-04-From Mr. Chisyliu 我个人觉得 学习internet technology本身不错 不过不要只注重网络个体 而要同时学习相关的实现方式 例如 网络应用 java或者数据库 网页开发之类的编程 这样毕业时候自然较好找工作 法国不是非常了解 但学位本身并没有你想象那么重要 主要是你需要找到合适的公司实习或者论文 并且把编程学好 无论你学习什么专业 基本上大学都是教授技术本身 而一般较少教授实现的手段 你所需要的就是找公司实习去强化实现手段 就是所谓的写程序 工程师学位重点并不是学位 而是强制的intern跟法语","tags":"Notes","url":"https://jiang-hao.com/articles/2016/notes-心路.html","loc":"https://jiang-hao.com/articles/2016/notes-心路.html"}]};